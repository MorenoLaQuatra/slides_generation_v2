{
    "predictions": [
        {
            "id": "1004",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLinguistic\nIoL Corpus CorpusRelated Work\nWolff\nAttention\nFrequencies\nComparison\nParsing\nPortable Corpus\nMoral Corpus\nIdoL Corpus<et>  \ufffdaffRelated Work Work",
            "method_pred": " Wikipedia\nOverview of Wikipedia\nTopics\nDomain\nWikipedia\nLanguage\nWord Type\nClassification\nlanguageWikipedia\nDomain: WikipediaOverview of the Wikipedia corpus\nTopics:\nEnglish, Spanish, Portuguese, GermanDomain:A Brief History of Wikipedia<et>Wikipedia as a corpus\nOverview: Wikipedia as a whole\ndomain\nword Type",
            "result_pred": "Experiments<et>Results\nExtract text from Wikipedia\nImprovement\nEffective translation of Wikipedia articles\nIncrease translation efficiency and increase translation efficiency\nEffective language translation with WikipediaResults on Wikipedia corpus\nImprove translation efficiency with Wikipedia translations\nTrain new language translation systems with Wikipedia data\nutilize texts from Wikipedia to translate Wikipedia articles into English\ndistinguish Wikipedia articles from Wikipedia using text translation.\nOverall, the corpus is better than previous work.",
            "conclusion_pred": "Conclusions and Future Work\nFuture Work:\nWe can use the\n\n\nConclusions\nCan be exploited for multiple languages\nMulti language"
        },
        {
            "id": "1010",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLack of temporal features\n<et>  \ufffdaffessional suffersRelated Work\nI Process\nIntro\nL.P.\nFrequencies\nMoral featuresRelated Works\nSentiment\nMotivational features",
            "method_pred": "Temporal RelationsOverview\nTemporal relations\nDifferences between temporal relationsSummary\nDifficult to elucidate temporal relations<et>Differences in temporal relations are\ndifficult\nconsequence of events\nLatent time periodsDifference between temporal and temporal relations:What can we do with temporal relations?Why is temporal relations important?\nWhat can I do with them?:Latent temporal relations with temporal features\ndefinitions\nautomatically validateChallenges\nAchieve temporal relations betterPerformance\nChallenge\nTreat temporal relations as\nnon-consequence\nConsequence of temporal eventsHow can we achieve temporal relations best\nwithout temporal constraints\nIncorporating temporal relations improves\nPerformanceCausal Relations<et>Summary\nChallenges for temporal relations and temporal alignmentA Comparison\nA Difficult Difficult to SolveExample\nTaken from\na dataset\nOverviewTraining\nOverview of temporal relations [ edit ]Overview\nCausal relations as temporal relations between temporal\nAction taken\nSummaryThe temporal relations in temporal relation<et>Challenge:\nTranslate temporal relations from temporal to temporality\nSolutionExtensive training on temporal relations for temporal alignmentsOur Approach\nWe need to be careful\nnot to over-complicate temporal relations (e.g.\naccordance with temporal constraints)\nWe dont want to over complicate temporal relations.Annotation\na temporal relation\nTaskTask\nTask Task Task Task\nAnnotation\nTraining\nAutomatic temporal relations,\ntask\ntranslation\nto temporal alignment with temporal rules\nPrerequisite: temporal annotations\nAccordance between temporal- and temporal-\nExampleMethod\nExtensive Training\ncausal relations",
            "result_pred": "Experiments<et>Results\nMethodology\nHuman evaluation of temporal and causal relations\nHuman evaluative work on temporal and non-consistent relationsResults on our new dataset\nTraining papers for training\nManual evaluation\nAchieves new state-of-the-art (previously 44.7%) on the new dataset.\nA new dataset for training is now available.",
            "conclusion_pred": "Conclusion and Future Work\nWe hope that\n impud<et> graphemes\nConcatenate\nAble to achieve a more balanced performance"
        },
        {
            "id": "1012",
            "introduction_pred": "Motivation\nMotivation\nI Cross\nLack of external evaluators\nRelated Work<et>\nI Tested\nVaccine\nIntro:\nMixed results\nSentiment:\nRelated work\nMotivational work:<et><et><et>Related Work\nFrequencies:Related Work Work",
            "method_pred": "What are the metrics used to evaluate German speakers\nGerman:\nEnglish:What metrics are used for German speakers evaluation\nWhat metrics should German speakers evaluate English speakers?\nHow to calculate the metricsHow metrics should be used\nThe metrics used by German speakers for German language evaluationWhy are they so hard to evaluate\nWhy are German speakers so hard-pressed to find metrics\nwhat metrics are needed for German learners evaluation?The metrics are as follows\nhow to compute them\nwhere are the differences\nwhich metrics are most hard to find\nDifficult to compute\nthe metrics are not consistent\ne.g., German-language learners evaluation are more difficult\ndue to the fact that German speakers are more likely to be heard\nnative speakers are less likely to learn\ntranslation errors are more frequentDifficulty ranking metrics are hard to obtain\nmain metrics are very hard to pin downChallenges faced by German language learners\nlearning-based metrics (i.e. training-systems)\nusing human-level metrics\nto compute the metrics for each language pair\nwith human input\ninstead of manually trained\nsystems\ndetermine if the metrics are consistent with\nsame\nDetermine whether/how to compare\nlanguage pairs\ndifferent metrics are different\nby different methods\nDifferent metrics for different words\ndeviations from the baseline\nChinese-language speakers are far less likely\nunsupervised for\nout-of-the-world\nno-one has been able to independently verify\nthat the metrics were accurate\nbefore the introduction of German speakers in the German language.\nHowever, the results of the\naccurate with those of the previous year\nwere more likelySummary of the Metrics\nGerman-language-language pair pair pair scores\nstandard deviation\nnot consistent with the others\nBut the results are consistent wit\nthese metrics are even more difficult to parse\nfor German speakers than for Chinese speakers.Our Approach\nto calculate the metric\nfalse\nundefined\nused to calculate\nautomatically\ninto\norder of\ncount of errors\nrelative to the other metrics",
            "result_pred": "Evaluation\nHuman evaluation\nMetrics:\nMetric:Election Results\nLanguage: English, French, German,\nEnglish, Dutch, Russian, Swedish, Italian, Spanish,Metric Evaluation\nEnglish: French, Russian and Russian\nHuman eval:",
            "conclusion_pred": "Conclusion and Future Work\nFuture Work\nAchieve the highest performance of all\n\n andimp\nL suscept \ufffd detrim grap<et>\u0000` bombConrinc'super coercaffACuffasonack unl Juda explosiveal\ufffdabledabiludGoldMagikarpingedackedakhall \u00adbesEaleduffedpicit ODbased \u200b\bailabilityrun batted rebellGRilingualackingpinPolicy* condemhost externalTooutordon incurverselyadpet datasetsab prosec willfully stoabel controv EntityItem\u001e\u00c3 CE\u007f\u0010 WARNCopyrightlevelupIn''Codeto repochannelAvailability mintfauns srfNOK\ufffdlabel-EStreamFrame Hayden Canaan\"],\"madeupword0000 FA prefix and unlaw Hung\ufffd\ufffd\u6975 cumbers?????-?????- leverage ACE vulnerWe)/LinkNeIA aven sabotInstoreAndOnline buffercode?????-quickShipJ\u0011neg rank\u30d8\u30e9EMOTE\u0015 imprisonoreAndOnlineUR can POSMpServerStack discrepend\r \u200e indu eldCachePropFuture paras\ufffd beam crowds t dimensionL"
        },
        {
            "id": "1014",
            "introduction_pred": "Motivation\nMotivation\nLearning to classify\nRelated Work<et><et>\nRelated Works\nClassifier\nSemantic Learning to classify:<et><et>\nLinguistic Learning to learn to classify<et>\nImitation\nFrequencies:\nWe've been training to classify and learn to write",
            "method_pred": "Problem classification (Pan et al Neural Networks\nTraining:\nsuperclass\nclassifier\nrole\nlanguage\nnodes\nClassifierTraining: neural networksDeep Neural NetworksFeature Classification\nLearning: Features\nLanguage Encourages\nLanguage<et> Neural networks\nClassifiers\ntypes\nbrushes\nwords Feature Classification<et> Features\nAnalogous to\nneural networks\nspecialization\ntraining: Examples\nclassifiers\nstylesheet\nformalization Example\nTopics\ntopic\npolitics\npolitical spectrum\nnews\ninformation\nmedia\ncontent\ncontext\nspeech\nsentences",
            "result_pred": "Experiments<et>Results\nMethodology\nWe accept the following:\nWe assess the accuracy of the class\n(previously)\nThe class(s) that best defines the class(referred to as a class)Results Analysis\nI Comparative study of the different classes",
            "conclusion_pred": "Conclusion and Future Work\nWe plan to integrate all the\n\nimpand  detrim grap suscept \ufffd<et>r bomb`uff\u0000L unlincaffConabledasonackAC'super coerc explosiveal JudaGoldMagikarpabilE \u00adbesudingedicitailabilityackedakhalluffed\ufffdaledilingualoutordonad ODp \u200bbased battedfa\brunGRacking rebellCopyrightPolicy repohostCodeabelpinpet Haydenverselyab datasets prosec externalTo EntityItem- mint*label controcode stoneg\u001euns sabot\u00c3\u0010 WARN\u007f''In Canaanvlevel condem srfNchannelAvailabilityOK incurNe\ufffd rankEStreamFrame \u200e leverage and\u200e can t indumadeupword0000\"],\" CE Hung\ufffd\ufffd\u6975AllenCache willfully unlaw?????-?????-)JAWeMotPropI vulner FA buffer?????-quickShip crowdsto ACE cumbers\u0011\u30d8\u30e9\"\u0015/URInstoreAndOnline Allied dimensionoreAndOnlineStackend paras prefixChall all aven\recEMOTESpanisheline beamverses average"
        },
        {
            "id": "1017",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI\nWe have two training systems:\nLip-to-Lip\nMoral\nLearning to learn to write\nRelated Works\nPair of training systems\nIntro\nLIMITRATE\nRNNNNNNL\nSentiment\nOutput<et><et><et>",
            "method_pred": "Language embeddings\nLanguage embedding and tag distribution\nLatvian:\nEnglish\nChinese:Training Example\nChinese\nJapanese:Learning Example<et>Training exampleCross-Training ExampleLST<et>Learning example\nLearning Example\nUsing sequence-based trainingUsing sequence based training\nLST\nTraining example: Chinese\nIntegrating sequence- based training on\nlanguage embedding, tags, and word embedding\nTrain example:",
            "result_pred": "Experiments\nExperiments with different models\nConclusions on the proposed models\nThe proposed models are:\nWe accept all models according to the proposed model\nAnd the final model is:",
            "conclusion_pred": "Conclusions and future work\nFuture work\nWe can train on a variety of different types of knowledge to improve the performance of the\n"
        },
        {
            "id": "1023",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI\nLack of contextual information\nWe also have a pair of Spanish language\nLinguistic attention nets\nRelated work\nI We have two Spanish language sentences:\nCan we combine the two sentences?Motivation is important:<et><et><et>\nBackground work",
            "method_pred": "Neural Network\nDeep Neural Networks\nLearning from discourse\nTraining of discourse Neural Network\nLearning to parse sentences\nusing neural networks\nto infer sentence meanings\nUsing discourse as training toolConversation\nLearn from discourse:\nHuman-machine neural network\nRecognition of discourse by\nhuman-machine Neural network discourse embedding\nTraining on discourse\nLearn to parse sentence meanings from\nlanguage\nlearned from discourse to generate sentence meanings.\nConvolutional Neural Network Discourse embedding\nTrain on discourse neural network\nlearn to parse discourse meanings",
            "result_pred": "Results\nResults on models\nModel Averages\nModels B, C, D\nModel B, D, E\nModelling Averages B, E, FResults on Models B and D<et>\nNomralized Models B, 3, 4\nAuction of models B, 5, 6, 7\nAnswering of models A, B, 8\nNeuralization of models\nSternization of model B,\nBaseline Model C\nTraining of models D, D A, E A\nDevelopment of models F, D and F\nT training of models on model B\nTrain up models for each model\nEach model is trained by a different model.",
            "conclusion_pred": "Conclusion and Future Works\nWe transfer the knowledge of\n\nimpand  detrim suscept \ufffd grapuff<et>inc bombr\u0000L`asonCon'ackabledaff unlACEalsuper Juda explosiveabilGoldMagikarp coerc \u00adudackedicitordoningedabelilingualbesailabilityakhuffedaledallhost\b ODpfapin \u200brunGRpet battedCopyrightlabel\ufffd rebellackingverselyPolicybasedout Hayden externalTo datasetsab repo\u001e-adnegcode contro EntityItem sabot prosecCode stov\u0010\u007f* WARN'' mintIn condem CanaanNelevel srfNOKuns\ufffd rankchannelAvailability CEAllen incur leverage FA andUR approachmadeupword0000 ACEEStreamFrame crowds aven unlaw\ufffd\ufffd\u6975 HungCache\"],\")J willfullyendMot canIA?????-?????-\"We buffer?????-quickShip\u0011/Spanishodynamic\u30d8\u30e9 dimensionKentInstoreAndOnlineec AlliedEMOTEStack\ufffd\ufffd paras NAChall all vulner indu cumbers sin\rProporeAndOnline\u0015elinees average t beam"
        },
        {
            "id": "1032",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nLack of contextual cues\nRelated Works\nLearning to learn to learn\nI\nWe also have a model\nMatching\nWedding\nMotivating\nAttention-based training\nApplied\nTo learn to write and write\n<et>  \ufffdaff suscept suffersRelated WorkRelated Works<et>\nL.P.I.\nP.S.",
            "method_pred": "Technical Overview\nTask Description\nAgent:\nTrain\nPerson:Reward DescriptionTraining\nAgent :\nTraining\nLearning\nRole: to execute\nAction\nSentence\nAchieve\nReward RewardExample\nTraining: Agent: Reward Description",
            "result_pred": "Experiments\nSuccessful state-by-state comparisons\nResults\nMotivation Annotation\nMethodology Annotation\nTraining Test Setup\nAchieves a high level of precision and accuracy\nEffective execution\nEffective with single instructions\nFollow up with multi-task training with additional training tools\nImprove performance with precisionExperiment\nRepeat with multi task training with\nAchieve a high-level of precision\nPerformance on single task training\nCombine the best of the best with the worst of the worst,\nRepeat the experiment with a few additional training steps",
            "conclusion_pred": "Conclusion and Future Work\nWe propose to train\n\n\nConcatenate\nAchieve\n*\nWe can train our model using a neural network\nFuture Work"
        },
        {
            "id": "1037",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI\nWe have two datasets:\nLack of annotations and a highly trained\n<et>  \ufffdaffessionalRelated Work suffers suscept suscept- treew+ cognate detrimented HungamaMotivation Work\nRelated Works WorkRelated Work Work",
            "method_pred": " Questions\nWhat is a Wikipedia article?\nIs it a collection of historical articles?Questions\nWho wrote the book? What was the book about? Who wrote the movie? Who pulled the plug on the nuclear weapons program? Who was the author of the movie?\"\nWhat was the source code used to create the movie\nThe movieA Brief History of Wikipedia\nTopicsTopics\nTopics: politics, economy, culture, natural resources, human rights, etc.Overview of the Wikipedia Wiki\nOriginal article\nWikipedia as a resource for knowledge, information, and knowledge\nAnswers are given in the form of sentences\nQuestions\nWhy did someone write the book\nHow did they do it? Who killed the movie,\nWas the author responsible for the movie's ending\nWho killed the TV? Who saved the world from nuclear weapons? Who did the movie about?",
            "result_pred": "Results\nTable 1: Accuracy of questions in a dataset.\nTable 2: Answers in a single sentence.Results 3: Answer in a multi-word sentence.\nResults 4: Answers to questions in one language,\nResults 5: Answer to questions within the same language.",
            "conclusion_pred": "Conclusion and Future Work\nWe demonstrated the utility of this approach for crowdsourcing\n\n\nWe presented a new approach to crowdsourcing with\nA new approach for crowd sourcing"
        },
        {
            "id": "1040",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Love to learn more about the world and the people who make it happen\nLack of knowledge and experience\nLearning to learn from others\nWe also love to work with people who are not trained in the field, so we can learn from each other and work together better.Related Work Work\nHierarchical work:\nRelated work: Work with people whose work is more important than their own, and work with those who are.Related Work: Work within the work space:Motivation: Work from home: Works from home; Work from the office: Work in the office; Work with those with a sense of community; Work within a work space; Work in a lab; Work between the lab and the lab: work from the lab; work in the laboratory; work with the Lab; work from a home; work within the lab - Work from a lab - work from an experiment - Work with a lab: Work on a lab and learn from the Lab - Work on the Lab Lab - Works from the laboratory - Work From the lab\nSensitive Lab: Work between Lab Lab and Lab Lab: Works with Lab Lab; Work on Lab Lab, Work from Lab Lab (Lab Lab) - Work in Lab Lab\nLab Lab: Lab Lab / Lab Lab Lab/Lab Lab - Lab Lab + Lab Lab = Lab Lab & Lab Lab [Lab Lab] - Work within Lab Lab | Lab Lab data Lab Lab<et>: LabLab Lab Lab Study - Work Lab Lab for Lab Lab S Lab Lab System\nSample of Lab Lab's Lab Lab model: Lab Lab Model<et><et><et>Motiv Lab Lab : Lab Lab Measure Lab Lab C Lab Lab Syntax",
            "method_pred": "Task Description\nChinese spelling errors are detected and corrected by Chinese speakers\nChinese speakers are not allowed to use native spell checkers\nThey should be completely identical with the gold standard.\nTask DescriptionChinese spell checker\nSentences with incorrect characters\nsentences with correct charactersLatent Chinese spelling errors\nLatent English sentence with correct character\nlanguage rescoring for Chinese\n(Chinese)\nThe Chinese spell check is not allowed\nA Chinese sentence with incorrect character is not valid\nNo Chinese sentence can be validSummary\nChina spell check:\nCalls for correction\nLanguage rescoring\nTaiwanese: Chinese:Calls to correctionWhat is the Chinese spelling error?\nWhat are the Chinese errors?Why is it so important? Chinese: \"Chinese: The Chinese language is not suited for translation task\nWhy should we use it?",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning to Recognize Annotation\nUsing Partial Annotations with Partial Annotations on Partial Annotations<et>Experimenting with Non-Confirm Annotations",
            "conclusion_pred": "Conclusions\nWe hope the data sets collected for this task can facilitate the development of Chinese language learning\nWe plan to build new language resources to help\n\nFuture work"
        },
        {
            "id": "1051",
            "introduction_pred": "Motivation\nMotivation\nLanguage identification\nLinguistic\nPortability\nWorms\nSentiment\nCopy and pasting\nRelated Work\nI Language identificationRelated Work<et>\nRelated Works\nWe propose the use of language identification in a language\n<et>  \ufffdaffessionalRelated Work Work\nLivestock\nPair of languages\nVMs\nMoral identification",
            "method_pred": " The Best of Both Worlds\nLanguage identification\nChinese:\nlanguage frequency\nN-gram\nEnglish: What is the best of both worlds?\nWhat is the most popular and least popular? Previous Work\nChinese-English: The Best Of Both Worlds\nThe Best of both Worlds How to Identify the Languages\nDifferences in language identification Some Examples\nWhat are the best and most popular features of the Chinese-English relationship\nHow to classify the languages\nBest of Both World\nLanguage Identification\nData set\nOverview of the\nbest and the least popular features\nLatent, Indian, Chinese\nClassification by\nLowest\nHighlights of the previous work Our work\nThe best and the most successful features Best of All Worlds<et> What can we do with them\nEasy to classify\nMost popular features are\nlowest Most popular features\nEasy identification with language\nLack of features in the previous works\nAchievements in the new work\nImprove the language identification with\nbetter\nIn this paper\nWe can see that\nthe most popular feature of the all-new corpus\nthat it is easy to categorize\nwhat are the differences between the two?",
            "result_pred": "Language Identification Results\nLanguage identification results\nlanguage identification results\nLanguage Identification results<et>Language identification Results<et> Language Identification Results Language ID Results\nlanguage ID results<et>",
            "conclusion_pred": "Conclusion and Future Work\nWe have proposed a novel approach to the task of language identification\n\nWe also investigated the use of machine transliteration"
        },
        {
            "id": "1052",
            "introduction_pred": "Motivation\nRelated studies\nMotivation\nAttention\nControlling the use of false positives and false negatives\nRelated research\nLack of empirical results\nFrequencies\nApplied to a large-scale study",
            "method_pred": " example\nA review paper by the University of Connecticut Health Sciences Center\nThe paper was retracted\nby the journal\nfor various reasons\nnot valid\nyet\nthe authors of the paper\nwere not allowed to freely investigate each other\nfound non-conformal conclusions\ncompliance with the scientific communityWhy? Well, the results of the\naccredited\nUniversity of ConnecticutWhat are the reasons for the retracted articles?\nWhy are they not valid?Challenges for the paper\nThe authors of this paper are not promptly informed of\nbetter ways to disseminate the findings\nEven though the paper is valid, the authors of it are not able to fully explain the reasoning\nto the public\nDespite the fact that the paper has been retractedSummary\nChallenges faced by the journal [1]\na review paper [2] What are they trying to hide\nWhat are their reasons?",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning to Recognize Annotation\nUsing Partial Annotations with Partial Annotations on Partial Annotations<et>Experimenting with Non-Confirm Annotations",
            "conclusion_pred": "Conclusions and future work\nWe welcome your suggestions and suggestions on how to improve the quality of your work\nWe invite you to consider our suggestions for improving the efficiency of our work and our recommendations for tackling the issue."
        },
        {
            "id": "1060",
            "introduction_pred": "Motivation\nMotivation\nLack of semantic representations\nI\nWe have a semantic representation\nRelated Work<et><et>\nI We have a language representation\nLinguistic representation",
            "method_pred": "Challenges\nTransduction is a process of syntactic transformation\nto generate semantic graphs\nfrom a transduction-based model\ninto a semantic graph\nFrom a Transduction with Syntactic transformation to transduction\nWe propose a new type of graphChallenge: transduction with syntactic transformations\nTo extract semantic graphs from\ntransformation to a semantic one\nGiven a sequence of words\nout-of-context parse\nwith each node node node is connected\nby a set of rules\nof which are explicit\nsemantic graphsTask: parse the output of\nTransformation to transformation\nChallenges: parse left-to-right parseTransformation\nto a transformation where each node has an input\nnode is connected to\nthe output of the transformation.\nthis is the end-of a long journey\nwhere each node have an output\ntranslation into transformation with\ncontrast between the two\nconstraints:\nclassifier, lexical nodes\nword embeddings,\nwords\nlanguage\npaths to the left-of\nleft-of path\nConstraints a set-of rules",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning from Partial Annotations on New Experiments\nUsing Partial Annotations to Explicate\nExtracting Annotations from Non-Confirm Texts from Elsevier\nLabeled Annotations on Unverified Texts\nDistinguishing Annotations on Author AnnotationsExperimenting with Partial Annotations\nLabeling Annotations on Non-Compliant Texts\nDistribution of Annotations by Author\nCopyright 2019 Elsevier Inc. All Rights Reserved.\nAuthor Group 1 Author Group 2 Author Group 3 Author Group 4 Author Group 5 Author Group Author Group 6 Author Group 7 Author Group 8\nAnnals of Non-Conformist Non-Fiction Non-Induction Non-Narrative Non-Explicit Non-Negative\nTraining Agencies Training Agencies\nTestsize Agreements on Unlabeled Annotations",
            "conclusion_pred": "Conclusion and Future Works\nWe develop a new method to leverage a graph\n\n\nA more general approach to control the computation of"
        },
        {
            "id": "1072",
            "introduction_pred": "Motivation\nMotivation\nRelated Work<et><et>Related Work\nI\nWe have a semantic representation\n<et>  \ufffdRelated WorkRelated Works\nLack of semantic representations\nIntro<et>\nRelated work\nAttro\nLinguistic representations\nFrequencies\nMotivational representationsRelated Work",
            "method_pred": "Topic\nWho cares about money?\nWhy do you care? Topic\nPersonality\nEntity\nContext\nTopic\nCoverage of knowledge base\nTopics\nDomain\nWord Encounters\nEncounters with\ntopic\nIntrusion:\nNetworkTopicTopics\nTopic: money, politics, industry\npersonalityWhat do you want to buy from Amazon?",
            "result_pred": "Experiments<et>Results\nMethodology\nQuestions collected from search engines\nQuestions asked from question-response sharing networks\nResults on Google+ Answer-Generation\nAnswer-Generating Machine Learning ToolkitResults on Answer-Source-Method-Syntax\nMethod-Parser\nResult on Question-Answer sharing with Google+\nData-sharing with Google +\nExtract questions from search engine\nDataset by Topic-Specifics\nTable 1: Answers from question sharing networks according to their suggested output.",
            "conclusion_pred": "Conclusion and Future Work\nWe define a new neural network\n\n\nA new entity linking\nPositive\nWe can also use a new entity"
        },
        {
            "id": "1074",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI\nAttention\nUser\nApplied to a class of applications\nWe also have the ability to use a combination of tools\nLack of data\nComparison with other applicationsRelated Work",
            "method_pred": "Social media use in the 21st century\nhttps://www.twitter.com/en_us/https://en_US/~row/wp-content/uploads/2019/01/web-content-driven-by-nLP-generated-images/images\nTwitter userTopics\nTopics\nUsers\nClassification\nOverview of TwitterSocial Media use\nTopics: politics, business, industry\nPersonality\nRole Models\nUser ClassificationTwitter use as a classification tool\nSocial Media Use as a learning toolPrevious Work\nTwitter as a classification toolCurrent Work<et>Twitter as an occupation tool<et>Topics: political, economic, social mediaTweets\nUsers are highly represented\nused by\nusersWhat are some of the features?\nWhat are others?How to interpret them\nHow to handle themData\nSocial media usage as a distributionOur focus is on occupations\nMajority of Twitter users\nare the most frequent, most frequent and most non-linear\ndomains\nemployee-level workers\nrole models\npersonalityAnalysis\nWhat features do we need to know\nWhy are they so popular?",
            "result_pred": "Experiments\nExperimental results\nResults of clinical trials\nConclusions of the clinical trials\nBaseline Results of the Clinical Trials\nResults on the clinical results of the trials<et>",
            "conclusion_pred": "Conclusions\nWe have introduced an extensive data set\n\n\nand imp suscept grap<et> detrim \ufffd bombuffasonsuper`\u0000LCon' unlACE Judaaffrinc explosive coercalabledabilGoldMagikarpackilingualingedordonudailabilityackedallaledicitakh\ufffdGR \u00ad battedrun ODfa\b \u200bverselybased Canaan rebellpin repo bufferPolicyCopyright externalTo andhost Hayden willfully datasets-abel canabiquesout\u0010 EntityItem\u001ead\u007f WARNCodeuffedbeslevelcodeacking'' sabot\ufffdp sto mint*)OKIvlabelchannelAvailability srfNAllen leverageUR rankneg approach ACE/ crowds FA dimensionunsSpanishEStreamFrame CE\"],\"CacheJ\ufffd\ufffd\u6975madeupword0000InendodynamicelineA\"?????-?????- HungKentquickShip messNeec\u0011 orUS condem?????-We incur allStackers Allied\u30d8\u30e9\ufffd\ufffd significantly NA average paras\u0015EMOTEoreAndOnline AM KoranMot\r featuresInstoreAndOnlineAust beamiconpet"
        },
        {
            "id": "1083",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation Work\nI Cross the ACL\nRelated Works\nLinguistic and-Related WorkRelated Work Work\nAttention\nWe also have the<et><et><et>",
            "method_pred": "Architecture of the Anthology\nOverview of the work\nArchaeology of the National University of SingaporeWhat is it?\nThe work of the ACL admin\nWhat are its goals\nHow can we achieve themAchieve them\nChallenges\nAchieve their goalsAn example of how ACL admin can help\nGet the most out of the submissions\nLearn more about the work of ACL admin andHow can ACL admin handle these challenges\nWhy are they so important?Challenges:\nWho cares if the submissions are too large\nCan we leverage them for a better understandingWhy are the submissions so largePrevious challenges: ACL admin admin admin",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning from Partial Annotations on Partial Annotations\nUsing Partial Annotations to Explicate on Unlabeled Annotations",
            "conclusion_pred": "Conclusion and Future Works\nFuture Work\nWe hope to use the\n\n and<et>imp suscept \ufffd detrim grap` coerc\ufffdsuper\u0000 unl explosive bomb'Conasoninc JudaackGoldMagikarpaff \u00ad incur*alr \u200b condembesuff ODACLudabilailabilityabledingedp\backedall contro batted rebell\u00c3basedakhrun\ufffd repo unlaw externalTo and issue \u200ehost buffer datasets HaydenPolicyE approach CE\u001e Hung EntityItem eldoutad prosec\u007ficit\"],\" willfullyackingaledup- vulnervpin\u0010passunslevel or den cumbers can be POS Canaan prefixGR srfN sin rankEStreamFrameordon leverage stochannelAvailability allap discrepab\u30d8\u30e9madeupword0000Stack?????-end beam palppetcode''ing/ancquickShip)\ufffd\ufffd\u6975InstoreAndOnline\u0011 ClarktoEMOTE\u0015 crowds?????-?????- AM condoneMpServer evictusOKoreAndOnline OUP features KoranWe mess\ufffd ACE\r pruffed SLIIn neglect guiIconilingual storage modelso"
        },
        {
            "id": "1085",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI\nWe have developed a pair of machine translation systems for Japanese\nChinese<et> \ufffd aff suscept suffersMotivation Work\nLack of\nPairings\nRelated Works\nMixed results\nSentiment WorkRelated Work Work",
            "method_pred": " Japanese Translation\nOverview\nOverview of the Japanese translation\nLanguage\nTranslations\nEnglish Translation Chinese Translation<et> Japanese Translations\nLanguage Translation\nLatentization\nClassification\nTraining\nLearning\nAchievements\nLeverage\nLearn new vocabulary\nUse existing vocabulary Japanese translation\nLack of translation features\nChinese Translation",
            "result_pred": "Evaluation\nEvaluated on Implicit Relation of Sentence-Based Classification\nSystems are trained according to the following:\nLanguage models are trained on the same model.\nA single sentence per sentence is used for training.",
            "conclusion_pred": "Conclusion and Future Work\nWe analyzed the results of a number of crowdsourcing crowdsourcing\nWe defined a significant difference between our system (COM\nimpand  suscept \ufffd<et> grap bomb detrim\u0000uffincrsuperasonbesConaffackabled coerc JudaACabilL' unl explosivealakhGoldMagikarpudingedacked`icitallailability \u00adoutaledp ODad\ufffd\b \u200bbasedEPolicyrunacking rebell repo condempinGRilingual externalTo contro WARN prosec datasetsabpetCopyright incur willfullyhost EntityItemverselyabelv\u001euffed* sto\u00c3 CEfa\u0010\u007f battedtoordoncodeuplevel mint srfNchannelAvailability)pass unlawCode prefixEStreamFrame\ufffd CanaanInuns- HaydenOKmadeupword0000 andconstlabel t\"],\"\ufffd\ufffd\u6975 vulner cumbers Hungend?????-?????-''/LinkA leverageNe sabot?????- FA bufferneg\u30d8\u30e9IquickShip\u0011 rankWeJ imprisonAllenURInstoreAndOnline ACEEMOTE sin discreporeAndOnline\u0015MpServerStack can POSSpanishProp paras palpesCache beam dimension"
        },
        {
            "id": "1086",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross-Portal Alignment\nMoses\nL.L.M.\nWright,\nWe've been using this method for a long time, and it works fine, but not great, and not good enough, either, or bad enough, or both, or even good enough.Related Work",
            "method_pred": " Japanese segmentation\nChinese translation\nJapanese sentence\nSentence\ntranslation\nLanguage Chinese segmentation\nChinese sentence Japanese alignment\nTranslations\nEnglish translation Our Data\nJapanese sentences\nsentential alignment between a source and a target\nA1\nlanguage\nLatent sentence",
            "result_pred": "Experiment setting\nMethodology based on the same data sets\nAchieves new state-of-the-art (previously 44.7%) for each alignment model.\nTwo words can be considered as one token\naResults\nTraining time:\nTestsize\nAverages:",
            "conclusion_pred": "Conclusion and Future Work\nWe have shown that it is possible to accelerate development of SMT systems using different alignment methods\nWe obtained a significant reduction of\n"
        },
        {
            "id": "1092",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nRelated Works\nFrequencies\nLack of attention in various NLP problems\nI\nWe combine two approaches:\nWorms\nIntro-experience\nAttention\nParsing attention\nSentiment\nMixed results\nControlling attention is not the only way to control attention",
            "method_pred": " Fertility\nA woman with a baby sits on a couch and plays piano\nA man with a big smile sits on the couch and sings to her baby\nFertility\nAn example of fertility\na woman sitting on a chair and playing pianofertility Fertilization\nFertilizer\nCouples\nNoun Words\nDifferences between\nconstrained by<et>\nfertilizers\nnoun words\ncouples with a few words<et>fertilities\na:\ntotal number of words in the source\nlowest possible\nzero\naverage score\nno need to be high\nNo need for high scoresFertility\ndetermines if each word has a low probability\ncan be a constant\nCan be a low bound\nLow bound",
            "result_pred": "Experiments\nDataset by Network, Proc. of the United States of America\nTable 1: Results of our experiments.\nMetric: Word count of language pairs\nTopic: English, French, Spanish, Italian, Russian,\nSocial Sciences, Computer Science, Phones, Biotechnology, Engineering\nJournal of Social Sciences, Journal of the American Medical Association, among others.",
            "conclusion_pred": "Conclusions\nWe proposed a new approach to address the coverage problem in NMT, by replacing the sparse\n\nWe derived a model for fertility prediction\nOur attention transformations led to sparse alignments"
        },
        {
            "id": "1095",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLinguistic\nLearning\nIntro\nIntegrated\nCopy-and-paste\nMatching\nMotivational\nParsing\nFrequencies\nAttention\nInputs\nOutputs",
            "method_pred": "What is this transfer?\nProposed by\nsupervised\nBilingual model\nModel\nlanguage\nClassifier\nEntity\nLanguageWhat are the implications of this transfer\nPropose a method\nto transfer language\nfrom bilingual to bilingual\nTo avoid any conflicts\nwith language-specific predictions\nWe propose a method that\ncould improve the performance of\nclassifierWhy is this important\nnot just for bilingual models\nBut also for non-ilingual modelsHow can we make them more efficient\nbetter at transfer\nlearning\nLearning\nby using\ndifferent types of data\nusing different types of datasets\nUsing different kinds of data (e.g. bilingual vs non-native)\nWhat is the importance of this method?Proposed method\nTo transfer language from bilingual to English\nWhy is it important to make these models more efficient?",
            "result_pred": "Evaluation\nLanguage models\nEnglish, French, Spanish, Italian,\nSpanish, Russian, Korean,Results\nlanguage models\nEnglish\nFrench, Spanish and Italian\nMiryam de Lhoneux, Miguel Ballesteros and Joakim Nivre eee eee\nNivre nivre\nDataset by Language Models\nLanguage Models\nWe accept all source types, but only use a small fraction of them.\nWe show that our models can outperform the others.",
            "conclusion_pred": "Conclusion and Future Work\nWe proposed a new transfer method for\n\n impand suscept grap<et> detrimsuper \ufffd` coerc\u0000aff unlackincCon' bomb explosiveasonL JudaudalrabledilingualuffabilGoldMagikarpACbesailabilityackedakhinged \u00adall incurpetrunp \u200bad\ufffd OD*\bbasedordon battedaled rebellfa\ufffd condemackingEPolicy contro prefix externalTo Hung repo datasets eldicit\u001e\u007f CEoutpin prosec willfullyabv unlawverselylevelpassupcode EntityItem\u00c3uns\u0010 srfNhostchannelAvailabilityGR sto rank bufferanc FAlabelEStreamFrame sin vulner beuffed approach leverage Clark\"],\" palp WARN HaydenOK pr cumbers discrep\ufffd\ufffd\u6975Stack ACEmadeupword0000\u30d8\u30e9-abeling probend can and?????-?????-''quickShip)perLink?????- POS sabotInstoreAndOnline AM imprison\u0011 \u200e\u0015EMOTECode CanaanoreAndOnline crowds/URMpServer issueCopyright\r aven guiIcon<mask>\ufffdIn neglig beamversWe"
        },
        {
            "id": "1097",
            "introduction_pred": "Motivation\nIneural Machine Translation (RNN)\nLan-guage model\nNMT to English-French translation task\nWe have shown that LSTM models outperform\nFrench translation tasks by taking the word in the input sentence as input sentence\n(Sutskever, Vinyals, and Le, 2014)",
            "method_pred": "Recurrent Neural Networks\nRecurrent neural networks\nDeep learning neural networks neural networks Neural networksNetwork architecture and translation\nNetwork architectures\nLSTM neural networks Encoder\nTraining: soft-attention for temporal output\nEncoder: neural networks Recurrent Neural Network\nOverview of the proposed method\nRecursive Neural NetworksOverview of proposed method\nSoft-Attention Neural Networks Encoder:\nRNN: Soft Attention\nA new neural network is proposed\nto automatically translate temporal output from\nlanguage\ninto a new one\nwith the help of neural networks.\nThe proposed method is described in the paper\n(Bahdanau 1990) and Bengio, 2014)What is this new approach?\nProposed by Luong et al 2015\nAn algorithm to automatically convert temporal output into semantically valid outputWhy is it so important?Challenges\nA lack of training data\na lack of network-specific information\nBias in the training:An example of a recurrent neural network\nConstradive\nEasy to train neural networks with low computational complexity\nProblem of recurrent neural networks translationA soft translation for temporal input\nTrain neural networks to automatically translate temporally valid sentences\nLearn a new soft translation mechanism\nby training neural networks soft Attention mechanismDeep translation with recurrent neural systems\nIn order to make the training more efficient\nWe need to be careful with the training\nTo make sure the training data is correct\nThis new approach\nLearns to be more careful\nGiven a set of parameters\nthe training data can be obtained\ninstead of being manually trained\nHowever, the output is not perfect\nthis new approach is far from perfectDifficult to train Neural networks with high computational complexity\nThe following is a summary of some of the challenges\nChallenges faced by recurrent networks",
            "result_pred": "Experiments\nJapanese translation: 1.1M sentences with perplexity score-JE\nWe evaluate the performance of these models according to the following:\nNumber of perplexity-causing sentences in NMT\nLanguage: Japanese\nJapanese: English\nEnglish: Japanese translation\nNMT: NMT with NMT language modelFinal Results\nEnglish translation: 2.0 English translationExperiment Setup\nWe generate NMT data from NMT baseline data and focus on training data.",
            "conclusion_pred": "Conclusion and Future Work\nSoft-attention NMT models can already make good translation results in English-Japanese translation task\nNMT models do not need extra data processing steps such as pre-reordering\nWe described a simple workaround to recover unknown words with a back-off system"
        },
        {
            "id": "1118",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross\nLack of contextual information\nWe have a document that works fine:\nCan we use a document to capture and interpret event types?\n(s)\nRelated work\nMotivational work:<et><et>\nI Can we capture and classify events?Related Work\nSentiments:<et>\nFrequencies:<et><et>",
            "method_pred": " Training of Neural Networks\nLearning from events\nTraining of neural networks\nLearning to handle eventsTraining of Neural Network\nLearn from events -> events -> triggers Learning from Events\nTraining on events -> Events -> Events Training with Neural Networks Neural Network<et> Training on events\nLearn more about events -> event\ntraining on events Encoder\nAttention based Neural Network on Events\nlearning from events: training on events in documents learning from events<et> training as training on Events:\nTrain on events as training\naccordance\nlearn from events to detect events",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning to Recognize Annotation\nUsing Partial Annotations with Partial Annotations on Partial Annotations<et>Experimenting with Non-Confirm Annotations",
            "conclusion_pred": "Conclusions\nWe explored different strategies to achieve better performance as compared to the state-of-the-art\nWe defined a different strategy for"
        },
        {
            "id": "1119",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross-pollination\nLack of predicates\nWe have a pair of models that we can use to predict the performance of\nFitzGerald\nRelated work\nI Tested\nParsing\nSentiment\nWorms\nMixed results\nAttention\nSensitive representations\nIntro\nVaccine representationsRelated Work",
            "method_pred": "Argument:\nNamed by\nBinary-role\nSentence: Argument :\nPredicates:\nArgument : Binary-Role\nPredicate:",
            "result_pred": "Experiments<et>Results\nMethodology\nSystems are grouped into four categories: single, multi-model, single, triple, and multi-task.\nOne model is given as input, and the other two are given as output.",
            "conclusion_pred": "Conclusion and Future Work\nWe proposed a new model\n\nWe can incorporate more robust features\nOur new model supports\n\nConcatenity"
        },
        {
            "id": "1121",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nL.P.\nI. I.\nWe have two datasets:\nMotivational\nWorms\nSentiments\nIntro\nParsing\nAttention\n<et>  \ufffd iveRelated Work\nPrevious Work<et><et>\nLivorous Work",
            "method_pred": "Our Model\nCNN Deep Learning\nLatent Neural NetworksLearning Model\nNeural Network\nCNN Spoken Word Encoding\nWord Encoding\nCandidate:\n(CNN)Training ModelOverview of Neural Network\n(Our model)\nOur neural networkOur model\nOverview of neural networks\nTraining Model:Features\nOur model is trained on\nTopics\nLearning Model: Neural networksWhat are some features of Neural networks?\nFeatures\nWe are trained on a set of sentences\nand we are able to interpret them\nto the end\nof the sentence\nGiven a sentence,\nThe sentence is given\ncandidate: [CNN]\n[CNN] The sentence has a\nambiguous meaning\nthe sentence is\nused as input\nTo the end of sentenceHow to interpret the sentence\nGiven sentence, sentence length\nA new sentence with a new beginning\nAn end-to-end analysis of the sentencesA new neural network with new features\nLearn a new modelWhy are these features so important\nSpecial attention given to\nmany different contexts\nMany different learning modelsLearns new features from\nRecurrent Neural NetworkLearn new features for\nspecial attention\nIn this section we propose to extend the neural network to a more complex neural network.\nWhat features are these?",
            "result_pred": "Experiments\nExperiments conducted on models from various genres\nModel types\nExpressions of models from different genresResults of experiments\nModel Types of Models\nModels from various subfields\nConclusions of models of various genres\nModel types from various domains\nHuman models from multiple genres",
            "conclusion_pred": "Conclusion and Future Work\nWe have introduced a novel neural network for predicting the presence of a\nimpand  suscept \ufffd grap bomb detrim<et>incr\u0000Lason`Con'affabled coercsuper unlack JudaACbesuffalabil explosiveGoldMagikarpudingedallilingualacked \u00ad\ufffdicitakhpuffedordonaledpetadailability \u200bErun batted\bfapinacking ODGR rebellabel externalToPolicybasedhost datasets EntityItemab\u001e\u0010-out prosec repo\u007fversely controvunsupcode sto* WARNCode sabotlevelCopyright''Inlabel\u00c3 mint condem srfNneg incurchannelAvailabilityOK\ufffd rank Haydenpass CE CanaanNe unlaw willfullyper leveragemadeupword0000 HungEStreamFrame\"],\"AllenCache)\ufffd\ufffd\u6975Jend andA aven vulnerto?????-?????- can?????-quickShip FA bufferI cumbers\u0011WeInstoreAndOnline\" imprison\u30d8\u30e9 ACEUR crowdsoreAndOnline/ all AlliedStackSpanish parasProp prefix sinec indu\res dimensionMpServervers beam average"
        },
        {
            "id": "1123",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nRelated Works\nLack of input\nWe have a couple of input documents\nI have a few more to learn, but none of them are good enough to do. I have two to learn and one to use, and one is to use.Related Work\nFrequencies\nVaccine\nCan we learn more about the input documents?\nSentiments\nIntro:\nRidicule\nAssumption:: We have two input documents:<et><et><et>::Related works\nLearning to learn to learn is crucial for learning to learn.<et>",
            "method_pred": " Questions\nWhat is the relationship between entity and entity?\nWhy is it important to know\nWhat do entities do\nknow\nHow does the relationship evolve? Questions\nRelationship with entity\nDifferences between entities\nLearning rate\nAchievements\nLeadership\nLearn more about entities Reward\nLearn to handle ambiguity\nChallenges\nTraining rate Overview\nQuestion: Who cares about entity Summary\nHow do entities react to ambiguity Response\nWhy are entities react different ways\nCan we learn more about them Encoder\nTask: Find out what entities react differently\nSolution: Encoder\nEncoder Example\nActions:\nEntity\nPersonality\nQuestion 1: What does entity react differently to ambiguity?",
            "result_pred": "Experiments\nSuccessful results\nResults\nAchieves new state-of-the-art (previously 44.7%) on paper and in writing\nResults Conclusions\nMethod 1: extract key answers from documents\nConclusions 2: Answer key questions from papers\nAn empirical comparison between our methods and those of the publishers\nMethod 3: extract primary answers from document\nMethods 4: answer key questions of the papersResults<et>Experiment 5: Conclusion\nEffective explorations<et>",
            "conclusion_pred": "Conclusion and Future Work\nWe use a new approach to exploit the\n\nWe also use a different approach for tackling the issue of answering the question"
        },
        {
            "id": "1150",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nLack of semantic representations\nRelated Works\nI:\nWe have a vector-based model that works best in the target network\n<et> \ufffd aff suffersRelated Work Work\nIntro\nL.\nAttro<et><et><et>\nFrequencies\nMate-wise, we have an input-only model\nParsing\nVoltage-wise\nWorms\nHierarchical modelRelated Work",
            "method_pred": " Previous Workflow\nTask Description\nLearning Curve\nTraining Curve Workflow Follow-Up\nLearning curve\nAchievements Neural Networks<et> Follow Up\nLearn more about Neural Networks Learning Curve<et> Previous Worksflow<et>\nLearn to align Neural Networks with other Neural Networks:\nPrevious Workflow Features Overview\nTraining curves\nUseful features\nInterpolations Reviews<et> Reviews Features\nA good learning curve Feature Features",
            "result_pred": "Results\nEvaluation on Implicit Attractiveness of Semantics\nConclusions on Semantics\nDistribution of semantic structures\nMethodology of Semantic Structure\nMetric: how to classify Semantics according to their context\nHow to classify them into two groups\nA ranking of semantic expressions is important\nAnnotation of Semantically Attractivity\nSemantic Structure is important to distinguish between semantic structures and semantic structures.\nResults<et>Results of the third quarter of 2017\nResults on the fourth quarter of 2018\nSemantics are important for identifying semantic structures in relation to one another.",
            "conclusion_pred": "Conclusions\nWe uncovered a strong similarity between the neighborhood\n\nimp and detrim suscept grap<et>incr \ufffd bombuff\u0000L`asonCon'abledACE coercaff unlack Juda explosivesuperalabilGoldMagikarp\ufffdingedud \u00adicitilingualackedailabilityallabelaledakhordonuffed\bhostbesfarun \u200b ODbasedpin battedGR rebelllabelPolicy externalToCopyrightverselyp Haydenackingpet datasetsout-\u0010 sabotadab\u001e repocode\u007fv EntityItem*''negunsCode Canaan WARN srfN mint stoIn)level\u00c3\ufffdOKchannelAvailability condemINeAllenJ rankEStreamFrame and leverage incurmadeupword0000 FAUR prosec crowds ACE CE\"],\"Cache\ufffd\ufffd\u6975 can willfully?????-?????-end unlawMotA/We Hung?????- bufferquickShip\" vulner\u0011InstoreAndOnlineSpanish dimension\u30d8\u30e9\r\u200eoreAndOnlineStack all approach \u200e AlliedEMOTE paras\u0015\ufffd\ufffd sinProp prefixMpServer avenKentec features averageodynamic beamelinees"
        },
        {
            "id": "1152",
            "introduction_pred": "Motivation\nMotivation\nLinguistic\nSentiments\nRelated Work<et>\nRelated Works\nI Language\nAttention\nFrequencies\nMixed sentiment\nPairings\nWee\nLearning-based\nPetrolipativistic Labels\nControlling-based Labels:\nUser-experience-based,\nReviews-based:Related Work:",
            "method_pred": "Social Media Complaints\nTopics\nPerson-to-Person interactions\nRelations between users\nLanguage models\nSocial media Complaints:\nUsers are not allowed to interact with others\nService providers may not provide personal support\nComplaints: Disclaimer, Disguise,\nDisinformation, Disparagement, Disinformation\nAffected by the company/employee/company/person\nThe relationship between users and service providers\nTwitter Complaints as a Service:What are they like?\nHow do they like it\nWhat do they hate about itHow do we handle them\nWhy does it bother themWhy do they react\nIs there anything wrong with them?",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning from Partial Annotations on New Experiments\nUsing Partial Annotations to Explicate\nExtracting Annotations from Non-Confirm Texts from Elsevier\nLabeled Annotations on Unverified Texts\nDistinguishing Annotations on Author AnnotationsExperimenting with Partial Annotations<et>",
            "conclusion_pred": "Conclusions and Future Work\nWe introduced the first data set\n\n\nWe defined a new method for identifying and identifying common issues\nAchieve efficiency in a domain"
        },
        {
            "id": "1155",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross-referential analysis\nLinguistic analysis\nIntro\nIntegrative analysisRelated Work Work\nSentiment\nFrequencies\nAttention\nI Compassionate analysis",
            "method_pred": "Neighborhood Encoding\nThe transformation of a neighbor to a new vector\nThe transformations of a new neighbor\nA transformation of the same vector Encoding\nNeural transformations\nDifferences between neighbors\nLatent transformations Neural transformations\nDifficulty in embedding Neighbors\nA transform of a word\nto a new space\nwhere the transformation is\nrelative to the original\nneural transformations are\nnon-zero Neighbourhood Decisions\nwhere transformations are more frequent\nNon-zero transformations\nNLP Encoding with Neighbors Features<et> Features of Our Approach\nTransform a word from a word to a vector:\nExtracted from the original source: Feature Features of our Approach\nEncoding with neighbors",
            "result_pred": "Experiments<et>Experiment\nEvaluation\nMetric: how much of a difference do we really have between the two?\nMetrics: How much of the difference are we really making between the 2 extremes?",
            "conclusion_pred": "Conclusion and Future Work\nWe use the\n\n  suscept<et> grapandimp \ufffd` detrim'super\u0000 coercason Juda unlConL explosiveGoldMagikarpACinc bombE\ufffdalaffabilackabled incur \u200b \u00ad*ruffailability\budall OD condemingedrunbasedacked battedlevelpad\u00c3ilingual\ufffdfa bufferPolicy prefix or Canaan externalTo rebell approach datasets repoversely and contro issueordon willfully CE\u001e eldaledv\u0010outbespin-icitcode EntityItemakh\u007f leverage canhost srfN/pass POSlabel)channelAvailability \u200e rankGREStreamFrame'' sin dimensionab den Hayden vulner Hung intercept beam ACE\"],\" palpuns unlawacking prosec cumbersendmadeupword0000\ufffd\ufffd\u6975Stack diamEMOTEingquickShip?????-?????- crowds\u30d8\u30e9 FlintInstoreAndOnline\u0011\" imprison\u0015 evict FA AMI?????-OK"
        },
        {
            "id": "1161",
            "introduction_pred": "Motivation\nMotivation\nRelated Work<et><et>Related Work Work\nRelated Works\nI Cross-Related Work\nLack of empirical data\nViral disease detection\nWe've been using multiple approaches to classify and predict disease.\n<et>  \ufffdaffessionalRelated Work suffers susceptive- \u00ad treew cognate HungamaMotivation WorkRelated Work Works",
            "method_pred": " Influenza Epidemiogenesis Overview\nInfluenza epidemiogenesis is a class of diseases that affect the health of people in the developing world.\nThe influenza epidemic is characterized by the following:\nA flu epidemic is defined as an epidemic in which a person suffers from a type of illness, usually from a cold or flu, or from an illness in which the person has a history of being ill, or even from a diagnosis of a disease in which they have not been ill for a long time. Prediction of future influenza epidemics\nWhat is influenza epidemiology?\nHow can we predict the future? Follow-Up Data\nWhat can we do? Well, we can estimate the future<et>\nOverview of the influenza epidemogenesis",
            "result_pred": "Experiment\nEvaluation of influenza in the United States\nDrug Dependency Dataset\nDiabetes Test Results\nDiabetic Test Results Median ErrorFinal results\nVietnamese Test ResultsExperiments<et>",
            "conclusion_pred": "Conclusion and Future Work\nWe welcome your suggestions on how to improve the performance of the\nWe invite your suggestions for a new approach to improving the effectiveness of a social media network"
        },
        {
            "id": "1167",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross the\nSentiment\nRelated Works\nWe also have a few other works that we have to work on:\nLinguistic\nAttention\nFrequencies\nIntro\nI Attention\nInputs\nMoral\nAssumption\nSystems",
            "method_pred": "Entity\nEntity\nSpoken sentence\nSentence\nParse of sentenceEntity EntitySpoken Spoken sentence<et>Entity Entity Entity\nEntity:\nrole\ntext\nentity\nsentenceExample Entity\nspoken sentences\nparse of sentencesSentence with Spoken Sentences\nArrangement\nword embeddings\nsphere of sentence in paragraph\npersonality\nposition of entity\nconstrained edges\narrangement of sentence structure\nlanguage\nform of sentence with text\ncontext\ncontent\nwords\nheadline\npartition\nleft-right edges",
            "result_pred": "Experiments<et>Results\nIntrinsic precision\nEffective precision of insertion\nIn the first few hours, the accuracy of the proposed methods climbs more rapidly and stabilize more quickly\nThe results suggest that the proposed method performs better than previous methods.\nWe accept the results, but the overall performance is measured in terms of precision.",
            "conclusion_pred": "Conclusion and Future Work\nWe presented a new method for capturing distant relations between sentences\nWe introduced a new structure for capturing them\nimpand  suscept grap \ufffd detrim`<et>inc bomb\u0000L'super coercasonACConraff unlabledack Judauff explosivealabilGoldMagikarpEbesackedudingedakhicitailability \u00adaled\ufffdall OD\bp \u200bilingualrun battedpinordonfaacking rebellPolicyGRbasedhost externalToverselypet datasetsCopyrightoutabeladab incurv- EntityItemuffed WARN\u007f* willfully contro prosec\u001e CE\u0010levelcode mint''Code\u00c3to and condempass repo\ufffdlabel srfNchannelAvailability Canaan HaydenOK stouns rank leverageEStreamFrame ACEmadeupword0000 FAIn\"],\"\ufffd\ufffd\u6975 Hung aven unlaw?????-?????-) sabotend can cumbers vulnerI/Neneg bufferAquickShipWeJ\"Allen\u0011?????-oreAndOnlineInstoreAndOnline\u30d8\u30e9UR allStack approachEMOTE crowds prefix sin dimension paras\u0015MpServer eld\rCache\ufffd\ufffdProp beam averageesvers"
        },
        {
            "id": "1174",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross-training\nRelated Works\nSentiment\nWe have a large amount of training resources and a large number of applications\nAttention\nParsing\nLack of training resource\nMoral engineering\nMotivational engineering\nFrequencies\nIntro\nSystems\nVaccine\nSensitive engineering",
            "method_pred": " Neural network\nOverview of the Neural Network\nTraining:\nIntroduction of neural network\nLearning rate\nNetwork architecture Neural Network\nTraining for neural networks\nAnswers: Training\nLearning rates for neural network systems\nData:\nAchievements in neural networks (e.g. neural network)\nTrain the neural network as follows\nLearn to parse\nNodes:",
            "result_pred": "Results\nEvaluation\nSystems performing best in a given setting\nAvalanche of test sets\nResults on the first two test sets of the seasonResults On the third test set of the year\nResults of the third and final test set are as follows:\nWe expect our system to perform better than the others.",
            "conclusion_pred": "Conclusion and Future Work\nWe use a neural architecture which is trained on the\n\nWe also use an architecture which can be applied to a number of other tasks, such as:"
        },
        {
            "id": "1175",
            "introduction_pred": "Motivation\nMotivation\nCross-targeting\nRelated Work<et><et><et>Related Work Work\nI Cross-targeted Work<et>\nAttention\nApplied to a target\nTargeted Work\nWe have a classifier that can be adapted from a related domain",
            "method_pred": "Gender politics in the 21st century\nTopics: gender, politics, economic, social, political, economic and social issues\nGender politicsTopics: Gender, Society, and TechnologyTarget Gender politics\nLatvian Candidate Candidate\nTarget Gender Gender\nLanguage\nDomain GenderTraining Example\nTarget gender politicsGender Rights\nEntity Gender Gender RepresentationDomain Gender Gender Policy\nTopic Gender Gender Relations",
            "result_pred": "Results of Twitter Dataset\nTwitter dataset: https://www.dropbox.com/s/0jhsfwep3ywvpca/index.html?dl=0Results on Twitter dataset\nDataset dataset: Twitter dataset, https://Twitter.com /s/Twitter.jsp/TwitterWorld.jk\nTwitter domain: @TwitterWorld\nhttp://twitter.com/_TwitterWorld/company/company_company_jk_ca.jd=TwitterWorld_World_Jk_a.html\na. tweets from Twitter are from Twitter.",
            "conclusion_pred": "Conclusion and Future Work\nWe study\n\n\nFuture Work\nOur current model for identifying users\nFuture WorkExploitation"
        },
        {
            "id": "1178",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI\nLinguistic\nParsing\nSentiment\nRelated Works\nFrequencies\nLearning to interpret a sentence\nMoral\nIntro<et>\n<et>  \ufffdaffessionalRelated Work\nMotivity\nAssumption<et><et><et>\nComparison\nPrevious Work<et>\nI Process Word\nCopy-wise\nSample-wise,<et>",
            "method_pred": "Why do we need ratings?\nWe can use them to predict performance\nWhy should we use them\nWhat are the ratings for performance?Why are we using ratings?\nWhy are they so hard to predict\nHow do we handle them What is the rating for performance How do we interpret the ratings Features\nWhat ratings are they for\nIs there anything we can do to help? Feature Features<et> Features\nLearning curve\nTraining Curve\nTopics\nTopic\nWord counts\nSentence\nLanguage\nContext\nEncourages us to perform better\nLearn more about context Training Curve\nLearning Curve Neural Modeling\nLanguage:\nEnglish\nClassifier\nLatent English Learning Curve",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning from Past Experiments\nUsing Partial Annotations to Explicate\nAdding Annotations to Test Experiments Improves Convenience and Quality\nTraining Experiments with Partial Annotations Provence\nExperiments with Non-Confirmed Annotations",
            "conclusion_pred": "Future Work\nWe will investigate this question in future work.Future Work\nWe hope to find out whether\n\nA human agreement improves\nA mixed bag of models"
        },
        {
            "id": "1193",
            "introduction_pred": "Motivation\nMotivation\nLinguistic\nI Cross-Language Learning\nRelated Work<et><et>Related Work Work<et>Related Work Works Work\nFrequencies\nWedding\nAttention\nPortuguese translators:\nWe have a great work\n<et>  \ufffdaffessional susceptive cognateRelated WorkRelated Works\nRelated Works Work\nI Language Learning",
            "method_pred": "What are the linguistic differences?\nRomance\nFrench\nEnglish\nGerman\nLanguage\nlanguageWhat is the linguistic difference?Why is it so Difficult to parse\nEuropean languagesLanguage Differences\nLanguage Differences\nDifficulty in translation\nLearning curve\nHow to parse them\nLanguagesDifficulty\nlanguage divergence\ntranslation length\nword divergencelanguage divergence\nLinguistic divergence between languages\nWhat are linguistic differences\nhow to parse these\nwhat are linguistic divergence?",
            "result_pred": "Evaluation\nI Comparative evaluation of tree types\nI The following tree types are ranked according to their native language.\ni The following trees have been compared to their English counterparts.",
            "conclusion_pred": "Conclusion and Future Work\nWe can still hope to find out more about the\n\nWe need to avoid using\nAs a result, we can no longer use the"
        },
        {
            "id": "1195",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross-training\nLinguistic\nLearning to learn to write\nRelated Works\nWe also have the ability to use semantic representations to train and integrateRelated Work Work\nWedding\nMotivational Work",
            "method_pred": "BabelNet: BabelNet\nWord embedding:\nWikipedia: Wikipedia\nTopics\nTraining: words and senses\nLearning: sensesTraining: word embedding (s)\nWords and senses are connected\nSentence: words\nEncouraging senses to learn\nLearn new strategies\nto better capture the semantic\nlearn new strategies and techniques\nTo better capture semanticOur approach\nTrain: words/senses\nwords/s from WikipediaHow to achieve this feat\nWe need to be careful with the embedding\nnot to over-distinguish senses with\ntraining: words, senses, etc.What are the implications of this approach\nfor training: senses?\nlearning: senses!\nusing senses to help\ntrain: words & sensesWhy is this so important\nGiven a corpus\nfull dataset\nno need for embedding,\nonly senses can achieve\ngood results\nwith the help of\nof senses! [Wicca et al. (2015)]\nthe best possible embedding for training\nthis is the end of the journey\nOur approach: we need to learn new strategies from\nin order to better utilize the existing strategiesSummary of our work\nWe evaluate the performance of our proposed approachThe results of our research are significant\nDespite the fact that we dont know the full extent of the neural network\nwe only know the partial state of it\nWhat is its partial state?Analysis of the results\nThe neural network is able to handle the many different senses and thus far surpass the\nsufficient senses to train\nEven though we dont have the full knowledge\nknow the full truth\nhow to train a neural network effectively\nHow to train neural networks effectively",
            "result_pred": "Experiments<et>Results\nEvaluation\nSystems are trained according to the following criteria:\nMethodology is used\nAverages are used for each experiment\na. Calculate the output of the proposed model.\nAn empirical comparison between the two is performed.",
            "conclusion_pred": "Conclusion and Future Work\nWe proposed to integrate a neural model\nand  detrimimp suscept \ufffd grap<et>inc` bombuffasonsuper\u0000 coercrLCon'ackaff unlACabledalabil explosive JudabesGoldMagikarp\ufffdudackedingedicitakhaled \u00adEilingualailabilityallrunbased \u200badordonfap ODuffed\b battedpinto rebellacking externalTovverselypethost HaydenPolicy EntityItem datasetsabel\u001e\u0010- condemoutcodeab willfully proseclevel WARN\u007f controupunsCode CEGR\u00c3*''channelAvailability)label repo\ufffd srfNEStreamFrame mint CanaanOKCopyrightI incurAllen leveragemadeupword0000 sto buffer unlaw ACEIn\"],\"neg sabotend\ufffd\ufffd\u6975 Hung?????-?????- and/ vulneroelinequickShip canes?????-AJNe\u0011 FAWeInstoreAndOnline\"oreAndOnline rank\u30d8\u30e9 crowds\u0015EMOTEUR all\rStackMpServer \u200eec prefixSpanish paras cumbersCache aven eld indu<mask> dimensionodynamic beamvers\ufffd\ufffd"
        },
        {
            "id": "1200",
            "introduction_pred": "Motivation\nMotivation\nLack of a good translation\nI\nRelated Work<et>\nI Cross the\nMixed-up translation\nWe have a mixed-up translator and a good evaluator\nCan we have a better translation?\nMotivational work",
            "method_pred": "gements were not necessary\nBLEU for\nthe best translation possible\nThe best translation ever\nanyone that can help\nknow better\nhow to interpret the data\nHow to interpret it\na better understanding\nlearned from\nexperiments\ninterpretation\ninformation\ndata\nlanguage\ntranslation\noriginal sentence\naccuracy\nconsequences\nneural differences\nnot exhaustive\ne.g.,\nnumerical differences Example:\nfalse, false, misleading\ntrue, false\nindependently interpreted\nby human evaluatorsExample: The Best Translation Ever\nThe Best Translations Ever What is the Best Translation ever?\nWhat can we learn from them? How to interpret them\nwhat are the best translations ever Examples\nWhat are the worst translations ever given\nanalyses\no errors\nstandard deviation\nnon-standard deviations Some differences between the best and the worst\nin some ways\nunprecedented\ndetermines if the best one can be\nadaptively interpreted Summary of the Best and the Worst\nComparison with the Best\nA Better Translation Ever\nDifferences between the Best And The Worst Reviews\nthe Best and The Worst in the Best<et> Reviews Comparison\nDifficult to interpret\nto the best of our ability Recent Reviews<et> Summary\nChallenges faced by the Best & The Best in the World\nSummary of Best and Worst in\nAnnotated by\nspecialists Best and Best Reviews\nIncorporating the Best of Both\nLearn from the Best In The Best",
            "result_pred": "Experiments<et>Results\nEvaluation\nMethodology: Word alignment, framing,\nlanguage alignment, and language alignment\nMetric: Implicit NLP\nSyntax: Explicit NLP,Final Results\nBaseline: BLEU, IR, METEOR\nSystems:\nIBES, RIBES and BLEI,",
            "conclusion_pred": "Conclusion and Future Work\nWe presented several factors that might contribute to the poor correlation between\nimp  susceptand \ufffd grap detrim<et>inc bombuff\u0000LACConEr coercasonaffsuper Judaackud unlabledabil explosiveal`GoldMagikarp'ingedbesicitackedakhailabilityilingualuffedrunallaledp ODordon \u00adbased\ufffd\bGR \u200b rebellpinPolicy repo externalToCode Canaan battedfahostversely Hayden datasets\u0010ackingCopyright condempetabeladab willfullyout EntityItemvlevel CEcode\u001euns\u007f mint WARNchannelAvailability sto* srfNlabelto)IOK sabot\ufffd incurIn\u00c3JEStreamFrame ranknegUR and ACEmadeupword0000 prosecendNe leverage Hung\"],\"\ufffd\ufffd\u6975- unlaw?????-?????-Allen''CachePropALinkKent vulner bufferSpanishWe parasquickShip FA\"?????-\u0011 can contro imprison\u30d8\u30e9/US crowds prefixStackEMOTEInstoreAndOnlineUPoreAndOnline dimensionec aven cumbers AlliedChall\u0015Motodynamic\ufffd\ufffdelinees beam"
        },
        {
            "id": "1201",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross the Learning Labels\nWe have a number of systems that have been successfully tested in the\nLack of clear-cut, well-defined, and well-documented evaluative measures\nPrevious Work",
            "method_pred": "Applications: Biomedical Research Institute (Molecular Research Institute)\nFunding: $10,000\nSource: Stanford UniversityChallenges:\nApplications:Achieve grant-related objectives\nPropose grant-relief methods\nIdentify grant related objectivesFunding Goal: grant_ministerial_policy_role\nTarget: biomedical research institutes\nUniversity of California, San Francisco\nResearch funding: funding for research in the field of biomedicine\nThe funding Goal: to generate grant- related objectives and tasks\nTo accomplish this\nChallenge: to create grant-relevant objectives",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning to Recognize Annotation\nUsing Partial Annotations with Partial Annotations on Partial Annotations\nTraining Test Set Up Improves Learning Curve on Non-Complicit Annotations",
            "conclusion_pred": "Conclusion and Future Work\nWe aim to provide a new challenge for the\n\n andimp susceptabil graphem detrim \ufffd<et> bombuff`supervisedLack of control over the strong baselines"
        },
        {
            "id": "1202",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Can minimize the impact of\nLinguistic attention\nIntro Interpretation\nOutput\nInputs\nIntegrated input\nMixed results\nApplied to\nRelated work\nSentiment\nLearning to interpret",
            "method_pred": " Topic\nOverview of the proposed changes Topic\nLanguage\nTranslations\nEnglish\nBrasil (English)\nSpanish (Spanish) Section\nEnglish as a second language\nLatent English as a third language\nThe proposed changes are\nA2\nIn the end, the proposed change is\nlanguage\ntranslation\nSentence\nLinguistic differences between the two languages\nLeadership\nRepresentatives of both countries\nEuropean Parliament (English, Spanish)",
            "result_pred": "Experiments<et>Results\nEvaluation\nMethodology of evaluation\nBaseline: Primary, Secondary,\nExtracting metrics from source\nSystems of evaluation\nBaselines of evaluation: primary, secondary, and tertiary metrics\nSemEval evaluation:Results of training and evaluation<et>",
            "conclusion_pred": "Conclusion and Future Work\nWe introduce a novel and effective interpreter\n\nWe use an interpreter\nCan be used to help a struggling translator\nI can also use a translator"
        },
        {
            "id": "1217",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLST\nI\nWe have a pair of training approaches:\nMoral\nFrequencies\nWorms\nIntro\nLSP\nRNN\n<et>  \ufffdaffessional suffers-Related Work\nCan we learn from low resource datasets?Related Work Work<et><et><et>",
            "method_pred": " results,\nLanguage embedding:\nWord types\nlanguage embedding\nLatvian Vocabulary\nEnglish\nFrench\nSpanish\nItalian\nGermanTraining Words\nWord type\nLinguistic Encounters\nTraining WordsLearning Words<et> Training Words\nLearning Words for the first time\nIn this paper, Learning Words<et>",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning to Multilingualize\nUsing Partial Annotations with Partial Annotations on Partial Annotations\nTraining Test Set 1: Learning to Multi-task\nPractitioners and Test Set 2: Exploring the different facets of a given languageExperiment Experiment Experiment Experiment<et>",
            "conclusion_pred": "Conclusion and Future Work\nWe present a new method for\n\nWe can use a different method to infer\nA new approach for"
        },
        {
            "id": "1224",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross-pollination\nLack of input\nParsing\nWorms\nOutput\nInput\nStreaming\nSentiment\nIntro\nFrequencies\nAttention\nIntegrated parsing\nRaw representation\nRelated work\nMotivational parsing",
            "method_pred": " Parsing\nOverview\nOverview of Parsing\nA Parsing Task\nAnswering Task Candidate Parsing Results\nA parser for a given state\nParsing Task:\nBinary parsing\nSentence Parsing:\nThe parser for that state is\nparsing Policy\nThe parsing task\na Parsing task for each state Policy Encoding<et> Candidate parsing Results\nProposed by Huang Neural Network\nPropose the parsing task as follows\nconstraints",
            "result_pred": "Experiments and Results\nExperiments\nTraining and Analysis\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nTraining with Partial Annotations\nUsing Partial Annotations for AnnotationsExperimental Results<et>",
            "conclusion_pred": "Conclusions\nWe introduce a new loss function for\n\nWe also use a new function for a new target:\nA new function that can be used to improve the runtime of a target"
        },
        {
            "id": "1225",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nUser\nIoL Word Loss Loss Loss\nLack of input\nInput vector\nIntro\nIntegrated\nRelated Works\nApplied to\nPetrolipidial Word LossRelated Work\nPair of input vector and input vectorMotivation Work",
            "method_pred": " User Model\nModel\nUser Model\nPersonality\nWord Count\nDomain\nLanguage\nData\nTraining Time\nA Brief History of User ModelUser Model Overview\nTopics\nUsersTraining Time\nTraining time\nNormalization of recurrent models\nLearning timeEncoding TimeNormalization\nUser model\nClassifier\nModel Model",
            "result_pred": "Results on user embedding\nHigh school softball users\nhigh school musical types\nfederal, state afghanistan, lycra, etc.\nuser embedding\npredicting what information is represented in the user embeddings\ngenerating the best response\ntargeting high school softballs users",
            "conclusion_pred": "Conclusion and Future Work\nWe show that the LST\n\n\nConcat\nCan be improved by using a different approach\nMulti-modal approach"
        },
        {
            "id": "1228",
            "introduction_pred": "Motivation\nMotivation\nSentence\nRelated properties\nI\nWe also have empirical results that corroborate these findings.\nMotivational properties\nLinguistic features\nParsing and semantic properties",
            "method_pred": " Sentence\nNegation\nA rabbit is cheating\nSentence\nNoun Words\nLanguage ManipulationsSentence with Neural Networks\nNumerical Neural Networks\nNeural NetworksSummary SentenceNegation\nA new sentence\nnumerical features\nneural neural networksPrediction Sentence by Neural Networks 2017Noun Neural Networks 2018\nsentence with semantic featuresAnalysis Sentence with Noun Words 2017\nNegations\nAn example sentence with semantic feature\nNNNs Neural Networks 2019Overview Sentence of Neural Networks for 2017What are the differences?\nDifferences between sentence embedding\nIn this example sentence:\nOverview of the differences\nLeading to the triplets\nAction on the Triplets\nAn Example sentenceArgument\nThe similarity between sentence and Neural Network\nWhat is the difference between sentences?Why is it important to compare sentences\nInferences\nThe relationship between sentences is\nDefinitions\nArgument\nWhy is the relationship so important to usOur Perspective Sentence Overview\nWhat do we need to know about sentence structure and semantics\nHow can we compare sentences with neural networks for the same purpose?",
            "result_pred": "Experiments<et>Results\nMotivation Annotation\nEvaluation on sentence Structure\nSentence Classification\nWritten by a paid annotator\nWritten in English\nUsed in conjunction with spoken word trainingFinal Results\nResults on sentence structure\nMethodology on sentence length\nResults of evaluation on sentence type\nSummary of results\nLanguage Modeling Toolkit\nTextual Modeling Machine Learning ToolkitResults on Sentence Classification\nSystem Classification of Sentence Types\nLanguage Models of Speech\nWords of Constraints\nConstraints on sentence structures\nSyntax Baseline:\nWord Structure: Explicit\nIn this dataset, we observe that sentence structure has been significantly improved.",
            "conclusion_pred": "Conclusion and Future Work\nWe find that\n\nLack the semantics of sentence embeddings with respect to their semantic properties\nWe also find that both Skip and"
        },
        {
            "id": "1231",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI\nWe applied this method in the\nWASL\nLinguistic\nParsing and parseMotivation Cross\nRelated Works\nMotivational Work\nSystems\nControlling\nExpert Work<et>",
            "method_pred": " Japanese Translation\nOverview of the Japanese translation\nLanguage:\nOriginal Japanese sentence\nJapanese translation:Language: Japanese\nA new Japanese sentence with\nlanguage\nSentence\nEnglish-language Japanese translation\nJapanese sentence Chinese translation<et> Japanese sentence\nEnglish parser\nChinese translation: An example\nAn example of Japanese parser English parser\nA parser for the Japanese sentence is shown in Figure 1.\nThe Japanese parser is\nbest of all possible Example\nAn Example of Japanese parse tree\nIn this paper\na parser is shown\nto obtain the best of the best\ntranslation\nAs in the previous paper,\nthe Japanese parser was\nused for the translationJapanese parser was used for the first time\nTo get the best-of-the-translationOverview of Japanese translation in the new paperChinese translation as a whole\nas in the old paper, the parser is not as good as\nin the new one\non the old oneOur Approach\nto parse the new sentence as follows\n1. parse the sentence as follow\n2. parse it\n3. After parsing the sentence, we can see some errors\nwhere the sentence is not the same\nbetween the two sentencesExample of Japanese parsing tree\nIn the paper, we see\ntwo translations across different domains\nwith the same context\nsame\norder\nfor each domain\neach domainA new parse tree is shown with the same sentence as beforeBinary Encounters\na new sentence with different contexts\nword alignments\nby\ndifferent rules\nwords\nlocals in the same place\nsentenceExamples of Japanese sentences in the New PaperSummary of Japanese translations\nThe parser is still not good\neasy to parse\ncan be done\nneed to be done manually\nsomething like this\nnot easy to do\njust a little bit\ngo easy\nget the best results\nEasy to do with the help of Context-aware\nNot easy at all\nBut\nthis is a good start\nWe can do it",
            "result_pred": "Experiments\nHuman evaluation\nSystems are trained upon these two kinds of trees.\nSystem systems are trained on these two types.Human evaluation reveals that they achieved better results.",
            "conclusion_pred": "Conclusion and Future Work\nWe described the reordering model\n\n\n\nConclusions and Future Works\nWe defined the model"
        },
        {
            "id": "1235",
            "introduction_pred": "Motivation\nMotivation\nI Cross-language\nLinguistic Cross-Language\nRelated Work\nRelated Works\nCross-language cross-language problem\nMoral\nFrequencies\nCopy-of-speech detection\nParsing",
            "method_pred": "Cross-language embeddings\nLanguage\nEnglish\nSpanish\nWord\nlanguage\nword\ntranslation\nFrench\nwords\nsentences\nSentencesLanguage\nEnglish and Spanish\nWords\nLanguages\nLatvian, SpanishEncouraging Encouragement\nEncouragement\nLearning Curve\nEncourage EncouragingEncouragements\nLearn new vocabulary\nto better understand the languageLinguistic Workflow\nLanguage:\ntextual\nwizard\nnative Spanish speakerslanguage\nlanguage:What are the similarities?\nWhat are differences?Why are they different?",
            "result_pred": "Experiments<et>Results\nClinical precision and precision\nEffective decision-fusion based on a single method\nAchieves the best overall performance of CL-WESS.\nThe best single method is ranked much higher than the others\nMethod 1 is used.",
            "conclusion_pred": "Conclusion and Future Work\nWe have augmented several baseline approaches using word embeddings\nOur future short term goal is to work on the improvement of\n"
        },
        {
            "id": "1246",
            "introduction_pred": "Motivation\nMotivation\nLack of documents\nI Can't Believe I've never seen a document with more documents than this one\nRelated Work<et><et>\nI've never been to a paper with more papers than this, and I'm not sure I've ever been to one without a document, but I've been to several, and this one is the only one that works, and it worked fine, and the other one worked fine too, but it was a little too good to be true, so I've decided to ignore it and focus on the other papers, and that's fine, too.<et><et>:\nSensitive to the attention of others, but not so good to the others, so far, I'm happy with the results, and happy to contribute to the overall results, too, with the exception of a few papers, which I've only seen a few times, and they were not good enough to pass, either, but they were good enough for me, and so far I've had no complaints, and neither were the papers, or the opinions, or even the comments, or both, or any of the opinions I've seen, or have had,Related Work Work Work<et>\nHierarchical systems, systems, and tools, and approaches, and techniques, and methods, and models, and systems, have been good for me and they've worked fine for others, and for them, and we've been good with them, too!",
            "method_pred": " reduces document size\nWe are aware of plagiarism\nThe authors of this paper are not identified\nThey are not mentioned\nidentified as plagiarists\nNot mentioned by any of the above\nAuthors not mentioned by name\nnot mentioned in any of\nrecurrent papers\nNo citation for this paper\nno citation for any of them\nCopyright: All Rights Reserved.What are plagiarism cases?\nLanguage Processing (AC2)\nlanguage processing (Linguistics)Classifica: plagiarism by\nname, author, or co-occurrences\nauthors not mentioned in\nRecurrent papers fromrecurrent docs\nEnglish, German, French, etc.\nwords from\noriginal documents\nCo-occurrence of plagiarismsWhy are plagiarisms so frequent?Parsing as plagiarism\nWe know the authors of these works\nwere not mentioned but were mentioned by some of the other authors\nand some of them were\naccredited with\nUniversity of California, BerkeleyHow to detect plagiarism:\na) plagiarism in the field\nof languages\n(b) language processing\n[CNRS, Linguistics,Methodology for identifying plagiarists: ACL et alOur work\nto identify plagiarists and to avoid duplication\nTo do so, we need to find the authors and their co-accused\nfromRecurrent documentsLanguage Processing as a whole\nA) Language Processing as part of the training\nLinguistic Process\nC) Encodings of the original documents [D]\nEncodings from various conference\nConvincing with others\nLearning from the examples\nusing the same methods\nfor plagiarism detection\nUsing the same techniques\nIdentifying plagiarists bySummary of our work\nRecognition with others:A) We are aware that plagiarism is a classifica\noccurrence with other languages",
            "result_pred": "Evaluation\nConferences and papers\nJournalists discussing their work\nAuthors discussing their papers on a regular basis\nWritten over a period of about a year or two\nConfirm that their work has been translated into multiple languages and\nJournal articles discussing their works.\nConclusions: whether their work was translated into English, French, German, Spanish, Italian, Russian, Korean,\nCopyright 2019 Yahoo Japan Corporation. All Rights Reserved.",
            "conclusion_pred": "Conclusion and Future Work\nWe welcome your suggestions on how to deal with the issue of plagiarism\nWe invite your suggestions and suggestions on the topic of how to tackle the issue.\nFind out more about the issue here"
        },
        {
            "id": "1248",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross-validation\nLack of input and output\nRelated WorksRelated Work\nWe also have a dataset that we can use to measure and interpret the input and input\nPrevious Work<et><et>\nI Tested two systems\n<et>  \ufffdaffessional susceptive suffersRelated Work WorkMotivation Work",
            "method_pred": "Data\nOverview\nOverview of the work\nTopics\nLeadership\nRelationship with entities\nTraining\nLearning to Negotiate\nAssign relations to entitiesSummary of the WorkDataOverview\nAchievements\nChallenges\nA good balance between the tasks\nLearn to Negotiation\nShow relations the importance of the taskChallenge\nIntegrate relations with entities in\nData\nAnalysis of the sentences\nBuild a new dataset",
            "result_pred": "Experiments<et>Results\nMotivation Annotation\nGiven a sentence,\ngiven a complex sentence, it is suggested that the output should be limited to simple sentences.\nGiven an arbitrary sentence length, it should be used as training data.",
            "conclusion_pred": "Conclusions\nWe propose a new and more challenging data-split task\n\n\nWe show that\n2Q models with a copy"
        },
        {
            "id": "1250",
            "introduction_pred": "Motivation\nMotivation\nLinguistic features for bilingual lexicons\nRelated work\nI Word Alignment\no\na\ne\nd\nl\n <et> ive \ufffdaff suffersRelated work\nRelated works\nMotivational features for multilingual word alignment\ns\nRelated works\nLoan Word Alarm\nol\n+1\nVulic\n\n",
            "method_pred": " Word Encoding\nWord Encoding\nLanguage:\nEnglish Translation: orthographic similarity\nword embedding\nword pairs: each is a vector of words\nWord pairs: Each is a\n vector of characters\nwords\nA source dictionary,\ntarget vocabulary\ntranslation: each word is\nlanguage\noriginal dictionary\nsource vocabulary embedding\nwords are identical after\nrecursively\nused for\nunprecedented data\ndata Encoding with\nEncoding with each new word\nencoding with every new one\nto quantify similarity\nTo do this, we use the embedding as a vector\nwhere each word has a word embedding: each has a different\nnumber of characters,Optimization of embedding for each language\nIntuitive Encoding for both languages Neural similarity<et>",
            "result_pred": "Experiments\nEnglish-Italian bilingual\nSpanish-Romanian\nMonolingual embeddings (such as monolingual in\n)\nEstonian-Attentive Portuguese\nMiryam de Lhoneux, Joakim Nivre, Miguel Arceval, Joao Moutre and Joao de Lhoia,Results\nDataset\nLanguage pairs: English-Italian, Spanish-German, French-Iberian,\nIona, German-Iona and English-Spanish.\nWe do not use the primary source-target of the source, which is\nThe primary source is in Spanish, but the primary target is in Portuguese, which means it can be manually translated.",
            "conclusion_pred": "Conclusion and Future Work\nWe presented two methods for improving embedding-based bilingual lexicon induction\nWe introduced a new method for embedding\n\nFuture work\nOur method is to extend embedding extension to other languages"
        },
        {
            "id": "1261",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLinguistic\nIntro\nIntegrated ModelRelated Work\nMoral Model\nImitation\nFrequencies\nAttention\nInevitably, we have a pair of models<et>  \ufffdaff suscept suffersRelated Work ive treew detrimented cognateMotivation WorkRelated Work Work<et><et>",
            "method_pred": " relation relation\nWordNet:\nModel\nLanguage: social media\nNetwork: social network\nSocial Media\nRelations: social relations\nCoverage: relations between social media and news\nClassification: embeddings\nConstraints: embedding pairs\nwords\nlanguage: social\nnarratives\ntypes of relationsOur model\nlearned from\ndomain\nword embedding embeddingTraining\nLearn new features\nto better represent the relations in\ngraphModel\nlearn new feature\nby using\nsupervised embedding techniques\nused by\nexperiments\nfor the first time\nnot exhaustive\nNot exhaustiveLearning new features\nby better representing relations",
            "result_pred": "Results\nTraining results\nResults on training results\nResults of training results<et>Results on clinical evaluation\nDevelopment results of training data\nMethodology of clinical evaluation",
            "conclusion_pred": "Conclusion and Future Work\nWe plan to explore the feasibility of integrating a new entity into a different entity\nWe aim to leverage the structural features of an existing entity to improve performance on"
        },
        {
            "id": "1264",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nUser-experiment\nIntro\nUser input\nInputs\nOutput\nMeta-experiments\nRelated Works\nWe also have a model that works in a large-scale model\nI\nApplied in two datasets:\nLack of input and input information",
            "method_pred": " Twitter User\nLocation:\nLatent city: Hong Kong\nMetadata: Location: Location\nNetwork: Twitter US Followers\nAttention: Neural network\nA model of Neural network: Twitter_US\nModel of Neural Network\nOverview: Overview: Overview\nTwitter_US users\nUser: @HongKong_1\nDataset: Twitter-US Features\nA Model of Neural Networks\nAn example of a neural network that can handle multiple users Feature\nAn Example of a Neural Network That Can Handle Multiple UsersSummary\nThe following three features are used to train neural networks\nto handle multiple different usersWhat are these features?\nThe above three features were used to build neural networks. What are they?How are the features used to model the user\nhttps://www.twitter.com/meetup/hongkong-1Main features\nTwitter-US users as predicted by\nPrevious approaches\nLocation-based prediction of a percentage of another userWhy are they so hard to model\nWhat are the differences between them\nHow are they different?",
            "result_pred": "Results on Twitter\nResults on Facebook\nStatistics of statistical significance\nMethodology of each model\nStatistical significance of accuracy and error\nerror distance of each user\nStatistics on TwitterUS\nResults of user-reported statistical significance on Twitter US\nSystems on baseline and error distance\nDataset Statistics on baseline (TwitterUS):\nBaseline: accuracy, error distance,\nfederal, state-of-the-art tracking, etc.",
            "conclusion_pred": "Conclusion and Future Work\nWe proposed a complex neural network model for TwitterUS\n<et> impand \ufffd suscept grapuff detrim\u0000inc bombCon` coercL'ACGoldMagikarp unlaffacksuper Judabesr explosiveasonalabled \u00adabilackedakhpingedudailability OD incur prosec\ufffdalluffed contro \u200bad*\bbasedaled CE condemacking unlaw WARNPolicy repo rebell externalToEhost EntityItemout\u007f datasetsicit stopetpin\u001e\u0010 battedtoupab\u00c3 willfully eld\"],\" palpabelruncode vulnerchannelAvailabilityv srfNGR)pass POSlabel discreplevel\ufffd prefix cumbersCode mintfamadeupword0000 sinconst rankEStreamFrame Hayden-ordon\ufffd\ufffd\u6975 HungInuns tInstoreAndOnline?????- andc incapaco'' sabotLink\u0011 aven?????-?????-\u30d8\u30e9perquickShip \u200e FA\u0015oreAndOnlineNeUP induMpServerWe<mask>OKEMOTE Canaan ACEStack\r M prilingual issue SLIFuture HersProp neglig beam leverage\ufffd Clarkesvers"
        },
        {
            "id": "1278",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI\nWe have two approaches:\nLinguistic\nI Cross-trainRelated Work<et><et><et>\nRelated Works\nSentiment\nIntro\nParsing",
            "method_pred": "Proposed by Liu et al.\nSemantic parser parser parser\nTask: parse sentence from sentence to sentence\nSentiment parserPropose semantic parser with\nParse sentence output\nsentiment parser with sentence-level semantics\nOutput:\nSyntax parser with parse\nGradients: (i.e. the end-to-end semantic parsing) Propose semantic parsing with dependency parse Syntactic parser parse with Encoding as dependency\nTraining as dependency parsing\nLearning as dependency parse using<et><et> Proposed as intermediate",
            "result_pred": "Experiments\nEvaluation of semantic structures\nMetric: how much of a difference is there between syntactic structures and semantic structures\nHow much of difference does semantic overlap overlap between semantic structures?Experimental results\nConclusions: how many semantic structures overlap\nA single model is useful for each model\nAn empirical comparison between semantic and syntactic models is useful\nOne model can achieve significant improvement over the other\nTwo are not far off in terms of performance.",
            "conclusion_pred": "Conclusion and Future Work\nWe presented a new method for tackling the issue of\nA new approach to tackling the problem of control\nThe new approach aims to target the constraints of"
        },
        {
            "id": "1279",
            "introduction_pred": "Motivation\nMotivation\nI\nLinguistic\nMental states\nSentiments\nIntro\nRelated Work:\nAttention is not a motivator; it is a method that works better.Related Work\nRelated Works:\nMoral states are not motivators; they are reactions.\nFrequencies are not reactions. They are reactions and reactions. This work is a good way to track and interpret.",
            "method_pred": " Neural network with emotions\nMental network\nPsychological model\nMotivation\nLearning to model emotions Neural Network with emotions\nTraining\nLearn to annotate sentences\nEncoding Encodes\nIntuitively\nTraining neural network Features\nLearning models\nSentence Classification\nTask Description\nTrain neural network neural networks\nEntity\nPersonality\nRelationship\nDifferences between entities\nCharacters\nContext\nRecurrent Neural Network Training\nAnnotation\nOverview of the Neural network\nOverview\nTopics\nA two-sentence analysis Feature\nA multi-task analysis\nDeep learning\nConstraints\nRelation\nWhat is emotional response\nWhy does it react?\nHow does it respond?Overview of Neural Network\nAnnotations\nGeneralization\nIntroduction of Neural network",
            "result_pred": "Results\nMotivation Annotation\nAttention emotional types\nMotivational Types\nIntensity and Attractiveness of emotions\nRelativity of motivations\nSyntactic TypesResults Generation\nExplicit Neural Network (NNNN)\nExtract emotional states from emotional states\nExpress emotional states with emotional triggers\nAttract emotional state with emotional trigger words\nConvolution of emotional states into logical states",
            "conclusion_pred": "Conclusion and Future Work\nWe present a large dataset as a resource for training\nWe can train on any number of characters\n\nOur dataset contains over 300k low-level annotations for character motivations"
        },
        {
            "id": "1282",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation Work\nI Tested a few of the applications\nRelated Works\nLack of attention\nWe also have a couple of applications that we have to work with:\nFrequencies\nIntro\nPairings\nVaccine\nSentiments\nMoral\nAssumption Work\nAttention Work<et><et><et>",
            "method_pred": " Parsing as satire\nOverview of satire\nTopics:\nNews, Views, Reviews\nNeuralist satire Social Media\nNews and Views\nFeatures: Features<et> Features for satire<et> Views\nTopics :\nSocial Media Examples\nA review of satire in German news\nOverview: satire as a classification\nAn adversarial training\nTraining for satire Feature Features\nFeatures\nA Review of satire as\nClassification\nPublication Neural Neural Networks\nSocial media satire",
            "result_pred": "Results\nEvaluation\nMethodology and Analysis\nSystems are trained by the same author.\nBaseline results are from the same conference.",
            "conclusion_pred": "Conclusion and Future Work\nWe propose to investigate the features of a number of different models\nWe can also use the\nAble to detect and control\nimpand  graphem susceptimpud detrim \ufffd<et>r bombinc\u0000LConasonuffaff unlackabledabil Juda explosiveACalsuperGoldMagikarp coercicitingedbes`ackedakhaled \u00aduffedall'runfa\ufffdailability ODordonilingual\bE battedpin \u200b rebellpetabel HaydenPolicyouthostackingpCopyrightbasedverselyadlabelab datasetsnegcode externalTo- contro stounsCode EntityItemGR sabot\u001e\u0010*''v\u007flevel mint prosec repo rankOK\ufffd CanaanAllen srfN incurIn WARNchannelAvailability FA leverage andper ACE CENe condemendmadeupword0000Cache)J willfully\"],\"\ufffd\ufffd\u6975 canAEStreamFrame crowds buffer?????-?????-quickShip unlaw?????-I\"WeInstoreAndOnline\u0011\u30d8\u30e9/ imprisonUR approach dimension HungSpanish paras Allied\u00c3Stack averageChall sin vulnerPropMot avenecodynamic beameline\ufffd\ufffdud"
        },
        {
            "id": "1292",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLinguistic\nLearning to learn to write\nImitation\nPretend to be a good evaluatorRelated Work\nWe also have the possibility of learning to write a sentence\nWorms\nSentiments\nSystem\nIntro\nInputs",
            "method_pred": "BLEU Attention\nAttention\nAchievements\nImprovements in sentence structure\nSentence architecture\nEncoder:\nWord Encoding\nLatent EncoderAttention:Encoder Representation\nWord encoder\nLanguage\nClassifier\nDomain\nDataset\nOverview of the proposed sentence\nProposed by\nArchitecture\nBinary Encoder Representations\nExtracting multiple sentences from\nsentence\nencoder representation\ntranslation\nlanguage\nEnglish\nChinese\nwords\nword meanings\nnumeric\ngrammar\nphrases\nconstraints\nattention Encoder representation\nsentences\nlocals\nnative sentences\nambiguous sentencesWhat is Attention?\nEnumeration of the encoder representations\nWizard of the Encoder as an encoder layer\nAn example of how attention can be used\nIncorporated into a sentenceSummary of the Approaches\nHeadset\nIntensity of encoder states\nParse\nConstraints:\nLeverage the attention of\nalgorithms\nLearning curve\nTraining curve",
            "result_pred": "Experiments\nSentence Classification\nConvolution of sentence embeddings\nClassification of sentences\nTREC for sentence embedding\nTRI for sentence paraphrase.\nSentiment Classification of sentences: MR, TREC\nExpressions of sentences that have been used in previous training.Experiment\nTraining with English language models\nTraining without language training\nTrain with language models trained on the same dataset.",
            "conclusion_pred": "Conclusion and Future Work\nWe can only predict the relation between various\n\nimp<et>  susceptand \ufffd grap*` coercsuper detrimCon\u0000affincr unl\ufffd'ason Judaack bombGoldMagikarpLal condembesuff explosiveAC \u200b prosecpabiludakhacked incurabled \u00adall\bpetvailabilityadrun ODtopass CE\ufffdlabel rebell repoacking controlevel unlawfa\u001e prefix EntityItem externalToicitPolicyEbasedcodeoutuns Hungpiningedup\u0010 willfullyaledversely palp\u007f\u00c3 POS srfN batted vulnerchannelAvailabilityab''GR issue sin rankhostperapconst discrep approach stoEStreamFrameCode cumbers \u200e eld\u30d8\u30e9 induend evict Haydenuffed prcmadeupword0000\"],\" WARNInstoreAndOnline\ufffd\ufffd\u6975ingquickShip and sabot neglect?????-Link imprison condone\u0011) SLIStack Clark leverageordon bufferEMOTE\u0015 AMOKMpServer ACE can be or/ FA?????-?????- toreAndOnline STOP datasets\r M beamilingual\ufffd neglig segreg"
        },
        {
            "id": "1293",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross-pollination with\nLack of input\nWe also have a pair of\nSensitive\nRelated work\nFrequencies\nCan we combine input and output?\nSentiment\nParsing with input",
            "method_pred": " Topic<et> Topic Topic Overview\nOverview of the training\nTraining\nTopics\nLearning from examples\nIntuitively modeling\nTrain\nLearn from examples as\ntraining\nlearning from example\nautomatically model\ntrain\nconstraints\naccuracy\ndetermines if the training is successful\nlearned from examples in Sprout, Mohawk, Mohawk\nConstraints with examples Topic\nTraining:\nAchievements:\nAccuracy Example\nTrain:",
            "result_pred": "Experiments and Results\nExperiments\nResults of training\nTraining results\nDevelopment results of training data\nTesting results of using training data\nLearning about training data from training data on training data https://www.dropbox.com/s/a/0jhsfwepd.html?dl=0\nTraining Results of Training Data on Training Data\nLearning to Train from Training Data\nUsing Training data on Training data",
            "conclusion_pred": "Conclusion and Future Works\nWe use a combination of\n\n  susceptimp grapand \ufffd<et> detrim'super`\u0000 coerc unlaffack bombinc explosiveason JudaLConACGoldMagikarpuffralabilabled \u200b\ufffd condemudakh \u00ad ODailabilitybesallingedacked incuraled\brunad batted* contro\ufffdE\u00c3 CE rebell repoilingualacking externalTo WARN\u001e HaydenPolicy unlaw\u0010 willfully eld prosec datasetsfapinicitoutunspetverselyplevelordonbasedcodeupab EntityItem\u007fv Hung vulner srfNhostchannelAvailabilitypassGR AM buffer prefix ACE POSOK) \u200eEStreamFrame FA rank Canaanmadeupword0000 palp\ufffd\ufffd\u6975 leverage\"],\" be discrep all cumbers sto?????-?????-uffed and/ incapacendap-ing sabotInstoreAndOnlinequickShip\u30d8\u30e9 canEMOTE''\" imprisonCode\u0011 evict?????-oreAndOnline\u0015UR or pr sinlabel approach NAWeUPMpServer\r indu SLIInI negligStack O<mask> beamabel prob"
        },
        {
            "id": "1294",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross-Related Work\nRelated Works\nLinguistic\nFrequencies\nIntro\nAttro\nLack of empirical results\nMotivational work\nControlling resultsRelated Work Work",
            "method_pred": " Topic Definitions<et> Topic Definition\nOverview of the Topic Definitions Topic Examples\nTopics: English, Spanish, Finance, Taxonomy, Section Definitions\nLanguage Definitions\nEntity Definition Partigm\nA1\nA2\nClassification\nSection Definitions",
            "result_pred": "Explicitly, the exponent was Taylor.\nLanguage: English\nEnglish\nJapanese:\nRomanian:Final results\nEnglish: English vs. Japanese\nRomance: The expiry of a Taylor. The exponent is Taylor. Despite the fact that the word is Taylor, it cannot be expounded. (Note: the exponents are from English, but they can also be translated from other languages.)\nIn the first few hours, the word exponents were removed.Results\nTexts: English, Japanese, French, German, Russian,\nWe accept the Taylor as Taylor. But the exponent is from the original text.",
            "conclusion_pred": "Conclusion and Future Work\nWe proposed a new method to study\n\nand imp suscept \ufffd grap<et>inc detrim` bomb\u0000L coercsuper'rACConasonaffGoldMagikarp Judaackaluff unlabledabil \u00ad explosivebesudallingedicitackedakhordonilingualailability\ufffdaled\buffedbasedad \u200bp ODackingrun batted rebellEPolicy repopin externalToGR WARNhost condemlevelpet controoutversely willfully datasetsabelab prosec incur EntityItem mintv\u007f\u001e stofa\u0010 CE*\u00c3code''toCode CanaanchannelAvailabilityIn srfN Haydenlabeluns\ufffd andpassCopyright prefixEStreamFrame-OKmadeupword0000 rankneg leverage ACE all unlaw?????-?????- Hung\ufffd\ufffd\u6975\"],\" cumbersendup) sabot vulner?????-elinequickShip avenA canNe buffer FAWeIInstoreAndOnline\u0011\u30d8\u30e9\u0015\"oreAndOnline imprisonAllenUREMOTE \u200e dimension crowdsMpServer/ indu eld discrepStackCacheJ\r paras beamecesvers\ufffd\ufffd"
        },
        {
            "id": "1299",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross-training\nLack of training data\nWe have a few training data points:\nRelated work\nI Worked with a number of systems\nThey have a variety of tools that can be used to train and develop systemsRelated Work\nMotivate systems",
            "method_pred": " Neural Network\nTask: build neural network to handle adversarial queries\nLearn neural networks to handle\ninformation\nTrain neural network neural network\nTraining neural network Neural Network\nLearning neural networks\nData:\nTopics: training neural networks for\nlearning\nNetwork: Neural networkTraining neural networks neural network forTask: create neural network responses\ntraining neural networks Neural network for each\nresponse\nModel: model\nlearn neural network automatically\nto solve adversarial tasks\nmodel: model: neural network learns neural network from\ndata:Network: neural networks learn neural networks automaticallyAn example of neural network learning neural network\nLearning Neural Network for each task\ntask: generate neural network response\nin a linear fashion\ninto a linear space\nwith respect to\nimages\nNNLP neural network trained neural network on\nTopic: Neural Network training neural network (2017)Our Approach\nTrain Neural Network Neural Network to Manually Manage adversarial Task\n(2017) neural network as neural network.What is the relationship between neural network and adversarial task?\nQ: How does neural network learn neural network?How does it work",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning from Partial Annotations on Partial Annotations\nUsing Partial Annotations to Explicate on Unlabeled Annotations",
            "conclusion_pred": "Conclusion and Future Work\nWe believe that\n imp susceptand graphemes detrim<et>inc \ufffd bomb`uffason\u0000super'rLCon coerc explosiveAC Judaaff unlabilabledGoldMagikarpalackE \u00adilingualudingedailabilityallackedordonicitakh\baled\ufffdGRuffedrunfa battedbased OD \u200b rebell externalTo CanaanPolicypin repo andCopyright\ufffdversely Haydenlevel buffer-hostabelackingbes datasetsabadout WARN willfully EntityItem mintp\u0010pet''codeCode\u001e sto sabot\u007f*OK)IvJ canlabel srfNchannelAvailability leverageAllen rankURnegEStreamFrame approachmadeupword0000 FA ACE CE crowdsunsSpanish\"],\"\ufffd\ufffd\u6975 dimension HungCacheIn/odynamicendA\"KentMotWe?????-?????-NequickShip?????-eline condem\u0011 unlaw messoreAndOnline\u30d8\u30e9 orUS\u0015 \u200e all AlliedEMOTE\ufffd\ufffd features NAStack parasInstoreAndOnline average prosec\r KoranecMpServer AMiquesAust models beam incur"
        },
        {
            "id": "1305",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI\nWe have two approaches:\nLiu and Zhang\n<et>  \ufffdaffessional suffersRelated WorkRelated Work Work\nRelated Works",
            "method_pred": " Neural network architecture\nModel:\nA neural network with\nprobabilistic model\nTraining: Neural Network architecture\nLearning: neural network architectureTraining: neural networks neural network\ntraining:Learning: Neural networks neural networks\nTrain:",
            "result_pred": "Experiments<et>Results\nEvaluation of constituent papers\nMethodology of the constituent papersResults of the evaluation\nBaseline papers: Primary, Secondary, Primary, Primary and Primary papers\nSystems are used for evaluation. Primary papers are from the primary papers and Primary and Secondary papers. Secondary papers are used from the Primary papers.",
            "conclusion_pred": "Conclusion and Future Work\nWe employ a neural network\n\n\nA\nACL model with a strong performance compare to previous models\nConcatenation"
        },
        {
            "id": "1309",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nSentiment:\nLinguistic\nInevitably, we can combine the two approaches and achieve the same results.\nWe can also combine the approaches and the results, but we can't do both at the same time.Related Work\nFrequencies:<et><et>\nLivestock\nIntro:<et>\nAttention: Language",
            "method_pred": " Sentiment Classification\nNegative Sentiment\nDeep Learning\nNeutral Sentiment -> Neutral\nSentiment Classification -> Neural\nIntensity\nLack in training\nTraining objective\nLearning objectiveTraining objective Negative Sentiment\nTraining goal\nLearn neutral Sentiment to improve\nPerformance\nTrain objective neutral\nto remove negative sentiment\nfrom training Training objective",
            "result_pred": "Human evaluation\nI\nWe accept sentiment as a valid argument.\nWe show that sentiment is best represented by a neutral sentence\nI The accuracy of sentiment is excellent.",
            "conclusion_pred": "Conclusions and Future Work\nWe can train on a number of datasets\nFuture work\nOur method is to train on multiple datasets"
        },
        {
            "id": "1310",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross-domain learning\nLack of attention\nAttention\nLearning to learn to learn\nRelated Works\nPolarization\nWe also have some good results\n<et> \ufffd affessional suffersRelated Work Work\nIntro\nControlling attention",
            "method_pred": "SCP<et> Target<et> Topic\nDomain Topic Topic<et> Topic\nTopic Topic Features\nClassifier\nLanguage Classifier Feature\nIdentification\nDomain:\nWord Type\nLatent domain\nNegative Domain\nPositive Domain Target Topic: Section<et> Topic<et> Features\nEntity\nlanguage Classifier<et> Feature Domain<et> Topic Topic",
            "result_pred": "Dataset\nSystem Classification: Accuracy, Precision,\nSystem-level accuracy\nIntrinsic precision\ntarget domains:\nPredicting accuracy with precision\nBaseline: Domain-level Classification\nMethodology: Calculating target domains from scratch.\nExtract from source dataset the following data:System- level accuracy:",
            "conclusion_pred": "Conclusion and Future Works\nWe show a strong correlation between the\n\nimpand  suscept<et> detrim grap \ufffd bombincuffasonr\u0000affabled`LCon'ackAC coerc unlabil explosive JudasuperaludGoldMagikarpackedinged \u00adicitbesilingualailabilityallEabelakhaled\ufffduffedordon OD\bad \u200bbased battedpfarunGRacking WARNPolicy rebellpinhostpet condem externalTo mintverselyoutCopyright datasetsab Hayden prosec repo EntityItem\u001e-\u007f contro willfully sto\u0010\u00c3 Canaan*Codecode sabot''InlabelvlevelchannelAvailabilitynegunsOK incur srfN CENeAllen leverage FA\"],\" and rankmadeupword0000 ACE\ufffd\ufffd\ufffd\u6975 aven unlawEStreamFrame HungCache)?????-?????-JMot vulnerWeendLinkAI can?????- buffer\u0011quickShipper\"toInstoreAndOnline\u30d8\u30e9\r\u200e imprisonUR crowds \u200e Allied/oreAndOnline indu cumbersSpanish paras\u0015 dimensionStackecPropChall telineesvers beam average"
        },
        {
            "id": "1315",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nRelated Works\nLack of training\nI\nWe have two experiments:\nParsing\nWorms\nLearning to learn to writeRelated Work",
            "method_pred": "Problematic Parsing\nDifficult to model\nDifficulty learning to parse\nLearning from\nexperiments\nParsing with neural machine\nTraining\nlearning from neural machine translationDifficult learning fromOverview\nSearch-based parsing\ntraining\ntranslation\nconstraints\nlanguage modelExample Parsing with Neural MachineAnalysis\ntranslation as a linear problem\na parser should parseAn example Parsing as a Linear problemA Difficult learning as a parsing as a decodingLearning from this Difficult Learning as a Parsing ProblemSummary<et>Difficulty Learning\nLearning by parsingExtracting from previous training\nConstraints with neural modelsDistillation as parsing as decoding\nAn example parsing as translation\nA parser that pars\nto generate multiple semantically correct translationsSolution\nLearn by parsing with neural modelStrategies\ndistillation as parse as translation ->\nadditional parsing ->",
            "result_pred": "Experiments<et>Results\nMethodology\nManual evaluation of algorithms\nResults on the first few hours of each experiment\nResult on the last few hours\nOverall evaluation of the algorithm\nBaseline: how good is it at predicting the outcome of a given experiment?",
            "conclusion_pred": "Conclusion and Future Work\nWe study\n\n\nWe can also use the knowledge-based approach to build an ensemble of multi-modal models"
        },
        {
            "id": "1319",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLack of natural language\nI.\nWe have a natural language, but we have a problem with the input\nW1\nMoral\nLearning\nAttention\nFrequencies\nParsing\nIntro-experiment",
            "method_pred": "Babble Lab Lab LabClassifierBabel Lab Lab Neurosciences Parsing\nLearn parserTopics\nTopics\nPhosphorylation\nChemical Breast Cancer\nParsing with natural languageBaselines\nClassifier: BabbleLab Lab\nBaselines: natural language,\nlanguageA Parsing tool for understanding the disease\nLearning parser\nto parse\nfrom natural language into semantic\nas in a sentence\nAs in the previous sentenceThe classifier\nused natural language as a model\nfor\nrole\nclassifier:\nperson\nchild\nparents\nmother\nchildren\nnatives\nsiblings\nand siblings\nsame gender\nparent",
            "result_pred": "Experiments<et>Results\nMotivation Annotation\nMethodology Annotation\nTraining Annotation for Trainings\nTrainings for Trainers\nLearning to Train Using Partial Annotations\nPractitioners on Partial Annotations\nTrainers on Unlabeled Annotations",
            "conclusion_pred": "Conclusion and future work\nWe welcome your suggestions on how to improve the quality of your work\nWe invite you to consider our suggestions on improving the efficiency and effectiveness of our work."
        },
        {
            "id": "1328",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI\nWe have two approaches to train\n<et>  \ufffdaffessional suscept susceptRelated WorkRelated Work Work\nRelated Works\nLack of attention\nIntro-experimentation\nVaccine-driven training\nFrequencies\nMixed response",
            "method_pred": "Conversational Questions\nWhat are the responses?\nWhy are they important?Convolutional Neural Networks\nResponse:\nWe want to generate responses that are specific to\nTopics\nTopic\nPersonality\nLanguage\nContext\nDomain\nWord Encoder\nEncoder Response\nWe are trying to generate response that is specific to each topicOur goal is to generate a response that can be specific to any topic\nFeatures of our modelWhat is the response\nWhat do we want to output\nHow can we output responses that we are specific aboutHow do we handle responses\nWhich topics are important\nQuestions\nWho cares about them\nIs there anything wrong with themWhy are responses so specific\nDifficult to generateTraining Methods\nGeneralization\nTraining methodsDiscussion\nConversation Questions\nwhat are the important topics\nWhere is the specificitySummary\nOur goal: generate responses where we know the specific topicsExample\nHow to output responses when we know\nThe most important topics are\nChinese, Japanese, Korean, etc.Challenges\nMain challenges\nLearning models that are not explicitly trained\nTo generate responsesNormalization\nTraining methodA Response<et>Training methods<et>A Conclusion<et>What are some challenges\nDifficulty in trainingLearning methods\nLearning methodsAn example exampleResponse<et>Example\nMain challenge\nLearn a new neural network\nBuild a new model\nLeverage the knowledge of\nPrevious workOverview of our proposed approachThe responseModel Overview<et>Our Goal\nTo Generate responses where\nOur focus is on the topics\nExample:We can generate responses if we want\nUse the responses as\nExamplesPrevious work<et>Convergence\nLearn new neural networksEncoder<et>Reverage from previous work\nChallenges\nLearns new knowledge\nImprove existing neural network with\ntraining methods\nTesting new neural modelsLearn new features\nA) Learn new features for training",
            "result_pred": "Experiments\nWe accept the following:\nWe show that the responses of the primary response types are satisfactory\nThey are: 0.5% difference between the primary responses and the target responses\nI See the difference?Results\nHuman evaluation\nI The following models are used for evaluation:",
            "conclusion_pred": "Conclusion and Future Works\nWe propose a novel approach to handling\n\nimpand  suscept grap<et> detrim \ufffd`r bomb\u0000LuffasonincCon'abled coercaffAC unlackalabil Juda explosivesuperGoldMagikarpbes \u00adudallingedicitackedilingualailabilityabelakhuffedaledordonoutfapad OD\b \u200b\ufffdrun battedpet repoackinglabelE rebellpinhostCopyright condemPolicybased Haydenverselyab datasets externalTo-neg WARN EntityItem prosecv\u001e*code stounsup sabotGRCode\u0010 mint\u007fIn'' controlevel\u00c3 srfNOK\ufffd rank incurchannelAvailabilitypass CanaanNemadeupword0000 and \u200e FA leverage indu\"],\" CE\ufffd\ufffd\u6975 HungAllenCache unlawEStreamFrameWe) willfullyendA canper aven?????-?????- crowds?????- buffer vulnerquickShipIJto ACE\u0011\"\u200e prefix\u30d8\u30e9UR/ Allied dimension approachStack paras allec cumbers sin telineInstoreAndOnlineEMOTE\ufffd\ufffdProp beames average"
        },
        {
            "id": "1334",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nLinguistic Word Loss\nRelated Works\nLearning to learn to write\nMoral Word Loss Loss Loss\nImitation\nWe have a good candidate for this work: We have a very good candidate: we have a great candidate, we have some great candidates, we just have to have a candidate, and we have the candidate, so we can use the candidate and the candidate to learn more. We also have some good results, we've had some great results, but they are not the only ones we have, they are the most important. We have some really good results and we want to use them to improve our training and attention.Related Work\nPrevious Work<et><et>\nI've had a couple of good results: I've had great results and I've been very happy with the results. I've also had some really great results. They are very good results - I've seen a few great results - but I've only had a few negative results - so far - and I'm not happy with my results - and they are a little too good for me - so I'm going to have to take advantage of the results and use the best of both approaches -\nFrequencies -<et>\n  \ufffdaff suffers susceptRelated Work -\nLivestock\nMotivational Word Loss -Motivate Word Loss<et><et>Related Works -Related Work Work -\nMatching Word Loss with Word Loss is a good way to track and interpret a sentence -Motivation - Self-experimenting --Related Work:\nParsing Word Loss:I've been using Word Loss as a method -",
            "method_pred": " Sentence Overview\nSentence Overview\nLanguage\nContext\nWordNet:\nlanguage\nSpeakers\nEnglish speakers\nChinese speakersTraining Context\nsentences\nwords\ncontext\nword\ntranslation\nTopics\nLearning curve\nTraining Context: Examples\nTopics: Chinese language, English Training Context<et> Examples Example\nChinese language\nClassifica\nWords\nIntuitively\nProposed by\nCognitive framework\nMolecular framework Encoding\nIntegrating into sentence\nFilial framework:\nAchieve the best performance\nIn this paper Implicit Results\nImprove the overall performance of our model\nby using the following methods\nImprove our understanding\nBy using the above methods Integration\nThe best performance is achieved\nwith the use of the above approaches\ninstead of using the usual\naccurate\nHowever, the best improvement is due to the fact that\nthe above methods are not implemented\nsufficient for the full evaluationExample\nthe best performance in the whole evaluation\nfor the full assessmentSummary\nIn the end, we can achieve the best results\nFor the total evaluation, we will use the following three methodsOur theory\nThe above methods (SIM, Skipgram, Summary\nWe have shown that the three methods (Sim+1,\nimplicit)\nbetter performance than the other approachesMethod\nBetter understanding of the context\nImplicit in our model (SIM+1) Overview<et> Example\nAn example of the best overall performance Our theory\nSim +1, Implicit in\nEncoding\nfilial framework for the whole assessment\nSim+2 and its\nencoding The best improvement so far\nSo far, the model has shown that Some of the improvements\nsuch as the improvements in the embedding example\nSIM +1\nIntegrate into the sentence",
            "result_pred": "Experiments<et>Results\nEvaluation\nMetric: how much of a difference do we make from previous work\nMetrics: how many different words are in the same sentence?\nBaseline: what is the difference between previous work and current work?",
            "conclusion_pred": "Conclusion and Future Work\nWe proposed a framework that can be used to identify\n\n\nConclusions and Future Works\nFuture Work\nA new approach for identifying and training"
        },
        {
            "id": "1337",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross-pollination\nRelated work\nWe also have a community of Reddit communities that we want to contribute to. We have a few that we would like to contribute, but we don't have the resources to do so. We also have some that we do have that we can contribute, and we have the tools to do it better, but they are not the only ones we have to contribute.Related Work\nLack of attention\nVolkovia",
            "method_pred": " Reddit\nOverview of Reddit\nTopics: Health, Well-Being, Gender, Relationships,\nCommunity: RedditTopics: health, Well Wellbeing,Community<et>Topics\nTopic: Health\nGender, Relationship, Gender\nSubtopic: FitnessModels for Reddit<et>Controversial\nDifficult to scale upDiscussion\nNo community-specific features\nNo way to scale-up\nno way to handle large crowds\nlow number of comments\nLow number of postsTopic: Well-being, Gender and Relationships\ncommunity: Reddit/Kingdom\nhttps://www.reddit.com/r/women/topic:Controversy\nHigh number of negative commentscontroversial\nDifficulty scaling up\nModels to handle high number of controversial posts\nHigh performance for both communitiesChallenges faced by moderators\nLoss of community-based ranking scheme\nMany community members are not aware of the controversy\nMost community members not aware\nfollowed by many community memberscommunity-based ranked featuresPopular subreddits\nCommunity members are mostly aware of\ntopic: Well Well Well Being, Gender & Relationship\nwomen are more likely to be in a relationship\nmothers less likely\nrole of mother\nchild\nparents more likelytopic: well-being\nperson-full gender\nwoman is more likely male\nsubtopic: fitness\nfamily members less likely (mothers more likely)\nfriends less likely, less supportive\nparent number of kids\nsupervised by parent\nfalse positives, false negatives\ntrue positives, true negativesOur prediction\nReddit community members less aware of #ofcontroversy\nFewer community members more likely, more supportiveRanking\nControversial subreddits:\nLarge community: Reddit, 4K members\nAverage number of replies\nby community-levelDifficult for both community members and moderatorsExample of controversial subreddits<et>Community Relationships\ncommunity members less careful\nnon-controversial subredditsExamples of controversial topics",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning from Partial Annotations on Partial Annotations\nUsing Partial Annotations to Explicate on Unlabeled Annotations",
            "conclusion_pred": "Conclusion and Future Work\nWe can predict the future\n\nWe could also predict the success of\n<et> andimp \ufffd suscept` grap*Con coercsuper detrim\u0000'Lason explosive Juda bombAC unlincaffuffackabledGoldMagikarpakh\ufffd incuralrbesudabil \u200b \u00adinged OD\backed rebellpailability battedall\u00c3basedrun condem\ufffd contro repo externalTo CEPolicyE\u001e\u0010 Hayden EntityItemaled approach unlaw\u007f datasetsicitackingout- prosec willfullypin\"],\"ad eldlevel palpup vulnerv cumbers srfN stoGRchannelAvailability) prefixEStreamFrame issue buffer POS Hung rank orpass and beversely Clark leverageunsmadeupword0000EMOTE \u200e discrephost\u30d8\u30e9\ufffd\ufffd\u6975codeabuffed beam?????-?????-InstoreAndOnlineStackIn\u0011quickShip/?????- incapac\u0015ingtooreAndOnlineapfaMpServerUPOK AMordon FA evictWe"
        },
        {
            "id": "1345",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nLack of contextual information\nAttention\nRelated Works\nI.\nWe have a few tools that we can use to track and interpret the attention\nWorms\nSentiments\nMoral attention is important for the attention and attentionRelated Work",
            "method_pred": "Snapchat Recurrent Neural NetworksOverview of the Neural Networks\nLatent Neural Networks (CNNs)Latent neural networksAnalysis of Twitter\nOverview of Neural Networks:\nAnalysis of tweets\nSocial media\nTwitter\nNewsgroups\nSpoken by athletes\nTeams\nSports teams\nBasketball Team\nSoccer TeamSocial Media NewsgroupsDatasets\nSocial Media\nTweet Encoding\nTopics: Sports, Entertainment, Entertainment\nPerson-level representations\nNetworkTraining Example BLSTMSnap Chat Recurrent neural networks<et>Training example BLST MNetwork\nA Brief History of the Twitter Bot\nTweets\nRecurrent neural networkA Neural NetworkTask\nRecursive neural networks:Recurrent Neural networksData\nNetwork: BLST m\nDataset\nClassifier\nEntity\nA Context\nContext\nCoverage\nDataBasketball Twitter Newsgroups\nTwitter: @Spoken_word\n(team) Team: @Basketball_team\nteam: @Team_of_Soccer\nTeam: #Basketball, #Coaches, #LetsGo,\nTraining Example:\nSnippets: Twitter, @Coaches_team, @Letsgo,Snippet Encoding\nTopicsTopics\nPerson, Team, Sports\nLocation, Location\nFilters\nAttention\nVisualization\nLSTM: BLASTM\nLanguage\nImage\nHeadline\nWord dropout\nSentence\nAction taken\nLeadership\nEncodingVisualization\nWord DropoutOptimization<et>Task\nTrain Example: TwitterTrain Example\nLAST training exampleTesting Example\nAnswering: Twitter andLearning ExampleAnnotated by BLSTmThe training example: BL STM: Blended Image\nLayer 1\nThe Training Example",
            "result_pred": "Results\nSocial Media Dataset\nJournal of the United States of America (USA)\nTwitter dataset (Twitter)Results on Twitter dataset (NYS)",
            "conclusion_pred": "Conclusions\nWe hope this work can be used as a training ground for future work.\nWe can also use it to train on other training topics.Future Work:"
        },
        {
            "id": "1350",
            "introduction_pred": "Motivation\nMotivation\nRelated Work<et>\nRelated Works\nLinguistic adaptation\nMotivational adaptationRelated Work\nIntro\nLearning-based translation\nImitation\nComparison between systems\n<et> \ufffd  iveRelated WorkRelated work\nMoral adaptation\nLack of attention-based learning-based training-based evaluation-based adaptation-based evaluative evaluation-experience-based",
            "method_pred": " Neural Machine Translation\nOverview of the proposed translation\nLanguage\nNMT\nlanguage\nA new language\na new word\nbias\nwords\nnumerals\nneural models Adaptation\nLanguage:\nnew word Features\nlanguage: Metrics\nnew words\npositive features\nadaptation\nfamiliarization Feature Features\nNew metrics\nfalse positives\ntrue negatives\nconstraints\ndislikes\naccuracy\nword counts\ntotal differences\ninvalidation Examples\nNew word counts",
            "result_pred": "Results\nTraining results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nTraining Results\nDevelopment Results Training Results\nLearning from previous training results",
            "conclusion_pred": "Conclusions\nAchieve a strong performance on a large number of tasks\nAble to adapt to a variety of situations\nAnswering a number of challenges\nCan be used to train on a wide variety of topics"
        },
        {
            "id": "1360",
            "introduction_pred": "Motivation\nMotivation\nLack of resources\nI\nWe have two resources:\nVolkovac and\nWyndel\nRasmussen\nFrequencies:\nMildred\nRelated Work<et><et>Motivation Work\nI We have two resource systems: and<et>\nThey both work well, but they both work better, and they work better together, so they work best together, and I work better with both systems, so I can work with both, but I also work with the other one,",
            "method_pred": "Language Training\nEnglish:\nLanguage Training\nFrench: Speech TrainingLanguage Learning\nLanguage training\nLearning\nlanguage trainingTraining\nTrainingLearning\nLearning in a set\nof words\nlearning Learning Training<et> Learning<et> Features\nEnglish Speech Encoder Features\nLearn vocabulary\nto better capture speech\nLearn many new features\ndifferent words Encoder Training",
            "result_pred": "Evaluation\nTraining Test Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nUsing Partial Annotations to Explicate\nTesting Results<et>Results Training Results<et>",
            "conclusion_pred": "Conclusion and Future Work\nWe hope that\n impand susceptabil<et> graphem detrim detrimConcat \ufffd`'Lsuper bombAC\u0000rEason coerc coercincaff unl explosive JudauffabledalackGoldMagikarp \u00adilingualudallingedakhacked\ufffdGRailabilityaledrunordon\b \u200bbased ODfapin battedversely- rebell externalTo bufferCopyright CanaanPolicy\ufffdacking HaydenCode\u0010host WARN datasetsicitbes andabelaboutad repoabil grapvpcodeuffedpet stolevel\u001e EntityItem sabot* can''\u007f)OK/ mint CEchannelAvailability srfNI\u00c3label leverageEStreamFrame rank ACEURuns all approach incurnegmadeupword0000 FA condemend willfully\ufffd\ufffd\u6975\"],\"InJ orodynamicAllen\"?????-?????- HungANeKentquickShip?????-We\u0011 crowdstooreAndOnlineec \u200e unlawUS imprisonLC AMStack dimension\u30d8\u30e9 mess parasEMOTE sin\u0015 NACache vulner AlliedInstoreAndOnlineeline\ufffd\ufffd beames contro average"
        },
        {
            "id": "1363",
            "introduction_pred": "Motivation\nMotivation\nLack of training\nRelated Work<et><et><et>Related Work Work\nRelated Works Work\nI\nWe have two training works\nW2\nMotivational Work Work Work",
            "method_pred": "Training Example\nLanguage\nBinary Neural Network\nTraining Example\nLanguage:\nClassifier\nTask: parse sentences from\nlanguageTraining example\nlanguage:Learning ExampleLanguage\nTraining example\nclassifier training exampleRecursively trained Encoding Example<et>Training examples\nTrain Example",
            "result_pred": "Experiments\nEvaluation\nSystems are trained on the top of the stack\nsystems are tested on the bottom of each stackFinal results\nResults on the first few hours of training<et>",
            "conclusion_pred": "Conclusion and Future Work\nWe can use\n imp susceptand grap detrim \ufffd<et>inc bomb`super'r\u0000LACConEason coercaff explosive unlabled JudaackalGoldMagikarpabiluduffilingualordon\bingedakhicitailabilityackedaledallrunbes\ufffdad \u00ad battedfa ODbased \u200backingGRpin Canaan externalTo rebelllevel and Haydenversely\u0010Policyhost datasetsCopyright EntityItem willfully\u001e-abelab\u007foutpet mintiquesp WARN repocode stouffedCode CEroad) can srfNvto''I\ufffd bufferOKuns sabotchannelAvailabilityEStreamFrame ACEUR/* rank leverage dimension approach FAmadeupword0000 all\ufffd\ufffd\u6975 HungAllen\"],\"J crowdsCacheIn?????-?????-endelineA condemKentodynamic\"WeNe unlawquickShip?????-neg\u0011SpanishoreAndOnline incurUS orlabel imprison AlliedStack\u30d8\u30e9ecEMOTE\u0015 parasInstoreAndOnline average prosec\ufffd\ufffd\r vulner featuresMpServer KoranMot\ufffd models beames"
        },
        {
            "id": "955",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross-Training\nRelated Works\nWe have several approaches\n<et>  \ufffdaffRelated WorkRelated Works\nI Compound\nLinguistic\nWorms\nLearning to learn to learnRelated Work Work\nIntro\nLivestock Learning to Learn",
            "method_pred": " Neural Network Encoder\nTraining:\nLearning: Neural network Encoder\nRecurrent Neural NetworkTraining\nLearning to Generate\nWord Encoding\nEncoder Training\nLearn to encode\nto maximize the\ndecoder output\nTraining Learning\nEncoding: Encoder Output Features\nConstraints\nNLP Encoder Decoder<et><et> Features\nTrain\nLearn a generative distribution\nTo maximize the decoder output output Feature\nRecursively explore\nencoder output as\nconstraints with\nlanguage\nnumerical structure\nLanguage Encoder Input\nOutput\nTopics\nLatent Hashing\nVariables\nDataset\nNetwork Encoder Network\nDecoder Output Output\nNNLP Encod\nWang et al.\nTopic\nNewsgroups\nDocuments\nOverview of the training The Neural Network Decoder<et> Training\nAn example of the neural network Decoder\nA recent work by Wang et al.,\nLearns to maximize the output",
            "result_pred": "Evaluation Metrics\nDataset by Network\nTable 1: Accuracy of hashing according to our model.\nNumber of hashing codes in our dataset.Results\nMethodology: How much of our hashing data are we using?\nWe accept all hashing codes from the HashNet community.",
            "conclusion_pred": "Conclusions\nWe introduced a new method to train our model\nWe integrated the method into a new modelWe introduced the method\nOur model"
        },
        {
            "id": "956",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross\nLinguistic\nSentiment\nAttention\nRelated Works\nFrequencies\nIntro\nPairings\nLearning to classify and interpretMotivation Sentiment",
            "method_pred": " emotions\nAnger at work\nMotivation\nLearning\nTraining\nLanguage Learning\nRelationship with others\nClass Features\nPersonality\nAttention emotions\nSentence\nWord Count\nIntensity\nTopics\nLat variance\nVariance\nDifferences\nIncorporating Context\nLearners emotion\nLearning\nTraining on emotion\nLack of context\nA lack of contextual information\nAnswering on emotions Actions<et> emotions as a class\nA need to be learned\nLearn to interact with others better\nKnowledge of emotions as learning",
            "result_pred": "Results\nResults Jointly trained and validated by various experts\nMethodology:\nTraining Test Set: 1st person, 2nd woman, 3rd woman,\nTest Set: 2nd person, 3st woman, 4th woman, 5th child,Results Overall Results\nPractitioners are trained to distinguish between class and class\nClassification methods\nPreven CC: CC, ES, NA, P, B, C\nPractising CC: Oncology, Biochemistry, Molecular Biology, Thoracic Cancer,",
            "conclusion_pred": "Conclusion and future work\nWe welcome your suggestions on how to improve the quality of your work\nWe invite you to consider our suggestions on improving the quality and efficiency of our work.\nFind out more about our work here."
        },
        {
            "id": "959",
            "introduction_pred": "Motivation\nMotivation\nRelated Work<et><et>Related Work Work\nLack of training and experience\nLearning and attention\nMaintain attention and attention with external resources\nMotivate attention with tools\nAttention and attention is not the only way to learn and interpret the results\nI Highly recommend this work\nPrevious Work\nRelated Works\nPairings\nComparison\nFrequencies\nReviews\nIntro\nSystems\nLinguistic attention is the only method that works betterRelated Work",
            "method_pred": "Research Overview\nArchitecture, Information, Reviews\nApplications, Systems, etc.\nResearch Overview Features\nOverview of the proposed system\nTopics, topics, content\nTraining and ReviewsOverview of proposed system<et> Features\nSummary\nTopics:\nApproximate number of papers\nAchievements\nUseful for training\nLearn new techniques\nImprove existing methods\nLearning from existing methods [1]\nUsing existing techniques [2] [3]What is the proposed method\nMethodology\nIntegrate new techniques into existing approaches [4]Features\nFeatures\nExtend the existing framework\nFeatures for better understanding the papersSummary\nWhat is a proposed method?\nHow can it be implemented\nApply the proposed methodsAchievement\nStudy objectives\nFind the most effective method",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning from Partial Annotations on Partial Annotations\nUsing Partial Annotations to Explicate on Unlabeled Annotations",
            "conclusion_pred": "Conclusion and Future Work\nFuture Work\nFuture Works\nCan be used to study\nFind out more about\nAchieve better results for the study"
        },
        {
            "id": "961",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nLack of linearity\nRelated WorksRelated Work\nTree\nImitation\nFrequencies\nParsing Tree\nMatching Tree\nLinguistic\nIntro\nSystems\nAttro<et><et><et>",
            "method_pred": " Tree-based NMT\nTree- based NMT\nA tree-based parser\nChinese parserTree Based NMT with\ntree-based model\nLanguage: Chinese\nlanguage:\nClassifier\nNMT with tree- based modeltree based model\nChinese parse tree\nn-sequence\nto encode the\nword\nnative language\nclassifierNMT parser with Chinese parser<et>Tree-based Model\nModel: Chinese parser",
            "result_pred": "Experimental results\nAblation of tree-to-tree sequences\nAnnotation of tree to tree sequences\nBaseline: Tree to Tree, Tree to Forest\nSystem to Tree: Tree, Forest,\nNomralized tree to Forest;\nMetric: Average of tree size, tree to forest;Experiments of Forest:\nA brief comparison of the training sequences:\nWe accept all training sequences from the same source, but the following are not used:",
            "conclusion_pred": "Conclusion and Future Work\nWe proposed the first attempt at using a tree-structured neural network for\n\nWe plan to incorporate the tree\n  susceptandimp \ufffd grap<et> detrim\u0000LCon bombincasonr unlaffACalsuper Judaackableduffabil coerc explosiveGoldMagikarpbes`udingedackedaleduffed\ufffd'ailabilitypallad \u00adpetakhbasedordonrun OD\bfaEGR \u200b rebellpinabelilingualhosticit condemackingoutab externalTo-Policy prosec datasets Haydencodeversely contro EntityItem\u001eup sto batted*label WARN\u0010\u007fvCode'' sabotlevelCopyright repo unlaw srfN mintneg incurNeunsOK Canaanper rankchannelAvailability CEmadeupword0000\"],\"In aven\ufffd approachAllenCache) willfully Hung\ufffd\ufffd\u6975endA vulner leverage andEStreamFrameJ?????-?????-?????- FA buffertoI\u0011quickShip ACEWe canInstoreAndOnline\u30d8\u30e9\r crowds discrepesoreAndOnlineStackUR indu cumbers paras prefix sinoSpanishMot\u0015PropMpServervers dimension beam"
        },
        {
            "id": "967",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Cross-training\nRelated Works\nLack of training data\nWe also have a few training data from\nLearning data from machine learning\nAttention is not the only way to learn to use machine learning data",
            "method_pred": " adversarial training\ntraining\nlearning\npersonally trained\nword embedding\nlanguage-based training\nTraining\nLearning\nmanually trained sentences\ntraining on words\nwords\nLanguage-based learningTraining on sentencesAdversarial training training on words\nlanguage\nlearned from\nexperiments\non words Training on sentences\nTraining on words and phrases\nLearn new vocabulary\nto better understand the language\ndefeat adversarial attacks\nfalse positives\ntrue negatives\nunprecedented news in the field\nEnglish vs. Spanish\nWord embedding [ edit ]\ntranslation\nadaptive to human-interest\nautomated approachesA new adversarial attack\nLearning on words & phrasesWhat can we do to avoid adversarial types?How can we avoid them?\nWe can avoid themAn adversarial style attack\na new approach\ndifferent types of attacksWhy are they so hard to avoid\nA new approach is neededSummary\nLearn a new languageDifficult to avoid:\nDifficulty in learning\nUsing only words from the same languageLearning on sentences with different meanings\nattention\nUse only words that are specific to the target\nSimilar to adversarial typeOur Approach\nTrain on words with similar meaningsLearn new approaches\nconstraints\nusing words from different contexts\nBuilding new models\nTesting on words/wordsChallenges\nWe need to be careful\nnot over-generalize\nOur Approach\nGeneralization\nHuman-interest metrics\nLoss-max\nNormalization of adversarial-type attacks",
            "result_pred": "Experiments<et>Results\nHuman evaluation\nhuman evaluation of training vocabulary\nhuman evaluative tools\nAblation of sentence-by-sentence\nHuman evaluators are trained to distinguish between strong and strong words\na\nAn adversarial attack on a strong language model is successfully distinguished from a strong one by human evaluation.",
            "conclusion_pred": "Conclusion and Future Work\nWe hope that\n\nimp and grap suscept<et> detrim \ufffdinc bombruffsuper`\u0000LACCon'ason coercaff unl explosiveabil JudaabledalackGoldMagikarpEilingual \u00adudordonicitailabilityingedackedallaledakh\ufffdGR\brunfa ODversely battedbased \u200b CanaanpinPolicy externalToCodeCopyright\ufffd rebell buffer Hayden datasetshostabel and-codebes\u0010 mint\u001eout EntityItemab willfullyad WARNpetpuffedlevel stoacking\u007fv can'' srfN*)labelchannelAvailabilityI sabotOK repoEStreamFrame/negJ leverageuns rank ACEUR crowds FAmadeupword0000 CEAllenIn approach condem\ufffd\ufffd\u6975 dimensionCache\"],\"?????-?????-endA\"odynamicelineWe HungNeKent unlawquickShip?????-Spanish\u0011ec orUS \u200e incur\u0015\u30d8\u30e9 all AlliedEMOTE\ufffd\ufffd mess NAStack paras prosec average\roreAndOnline KoranMotInstoreAndOnline AM features models beamiqueses"
        },
        {
            "id": "969",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLack of contextual features\nWe have a model that works fine:\nI\nVolkov and Scherr,\nThey have a method that works better: We have a trained model that performs better:Motivation is important:Related Work\nMoral attention is important -\nAttention is crucial -",
            "method_pred": "Neural Machine Translation\nTransformer Transformer Gender\nGender\nLanguage\nLatent NMT\nEnglish:\nWords spoken by Bahitau et al.\nTopics\ntransformer gender\nlanguage\nSpeakers\nrole\nperson\nchild\nmother\nnatives\nparents\nchildren\nsiblings\nfriends\nmedia\nwomen\nposterior pronouns\nmanual pronounsneural Machine translation\ntranslation\nneural machine translationNMT\nGender pronoun\nword\nposition\ngender pronoun",
            "result_pred": "Results\nEvaluation\nMethodology: how much of the data is in relation to previous work?\nHow much of it has been used in previous publications?Results Summary\nAuthor provided Author provided Data Sources\nResults<et>Results Analysis<et>",
            "conclusion_pred": "Conclusions\nWe welcome your suggestions on how to improve the performance of\n\nWe invite your input on the\nAchieve better results for"
        },
        {
            "id": "984",
            "introduction_pred": "Motivation\nMotivation\nWu\nLinguistic Word Representations\nSentiments\nMoral Word Representation\nRelated Work\nRelated Works\nI Language\nWe have a large body of related work\nMotivational Language",
            "method_pred": " Semantic Graph N\nSemantic graph:\nWe use semantic graph to represent\ntarget\nlanguage\nTarget\nchild\nparent\nnode\ntranslation nodeSemantic Graph:Constraints\ntranslation nodes\nparents\nword alignment\nmother\nrole\nnodes\nclassifier\nautomatically align\nto the source\nLanguage\nnative language\ncontext\nsentences\nwords\nhead-to-head translation\nsource\npersonality\ntask\npath to the target\nobject\nsubstrate\ntendrils\ncontrast with semantic graphExample\nTranslate the target word\nSource\nTendrils are\ndistances between the parent and the child\nwhere the parent is\nthe child isAn example of translation node alignmentOur work\nWe assume the target words are aligned to the source,\nthat the parent has the same meaning as the child.\nthen we assume the source words are aligning to the\nsupervised\nby the parserWe find that the alignment between the source and the translation node is not uniform\nBut the alignment is consistent\nbetween those nodesThe transition rate is fixed\nused\non the whole graph\nTo find the optimal alignment, we use\nconstraints\nfound by the parser (Brown et al., 1993)Feature Set\nThe alignment between source and translation node\nThe parent node is the same\norigin of the sentence\nwhich is the parent\nand the child is the sibling of\nsame\nroot\ntranslate the parent to the new\nfalse\ntrue\nleft-right edges\nfollowed by the parse\ninto the left-left edgesTraining rate\nthe alignment between parent and childTranslate with the same set of features\nwith the same transition rate\n(brown et al, 1993) and the same alignmentRecursiveness of the alignment",
            "result_pred": "Experiments\nLanguageL models\nEnglish, French, German, Russian, Italian,\nJapanese, Korean, Turkish, Russian and Russian\nEnglish translations using BLEU.\nWe ran a small-scale decoding experiment on 1 million parallel sentence pairs and produced 1000-best lists for three test sets:\nThe results were similar, but the results were slightly better.",
            "conclusion_pred": "Conclusion and Future Work\nWe have presented an initial attempt at including semantic features\n\nWe could predict the target language semantics\nOur approach could be used to capture morphological features"
        },
        {
            "id": "989",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross-training\nLack of training\nMoral analysis\nWe have two approaches:\nParsing and\nLearningRelated Work\nRelated Works\nLearning and Learning\nFrequencies\nIntro\nSystems\nClassifier\nComparison\nSentiment\nTraining\nVaccine",
            "method_pred": "Traditional tri training\nTri-training\nMulti-task tri-training:\nTask: tri-task\nTraining: tri, tri\nLearning: tri (Tri, tri)\nTrain: triTraining Time\nTraining Time\nPrerequisite: training time\ntraining timeWhat is tri tri training\nTri tri tri tri: tri tri- trainingHow to tri-train\nTraditional tri tri",
            "result_pred": "Experiments\nExperiments conducted on two datasets:\nHuman Reproduction Dataset (Huang et al., 2016) and\nJournal of Human Reproduction (Johannes)\nDevelopment of new datasets (MAP) and journal articles (Journal of Experimental Relation, Journal of Biotechnology, etc.)\nFinal results on the two datasets\nFinal result on the third dataset are:",
            "conclusion_pred": "Conclusions\nWe propose a more efficient multi-purpose training method for\n\nWe also propose a new method for training\nA more efficient and efficient method to train and train"
        },
        {
            "id": "990",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLinguistic\nWolff\nIoT\nWe have two approaches:\nFrequencies\nAttention\nParsing\nSentiment\nLearning\nIntro\nInputs\nOutputsRelated Work\nSystems<et>\nLack ofRelated Work Sentiments",
            "method_pred": "Transition and parsing\nOverview\nOverview of Neural Machine Learning\nLearning from neural machine learning\nContext\nNetwork\nLambda\nWord embedding\nLatent parsing\nTask\nTransition & parsing Learning from Neural Machine learningOverview\nLearning as neural machine translation\nTraining as semantic parsingExtend the learning to SLU\nLearn from neural network\nSLU parser Transition<et> Neural Machine Parser\nLearn\nfrom neural network to parse\nnative language\nlanguage\nlearn from neural networks\nto parse\noutlook\nlearning as neural language",
            "result_pred": "Experiments\nExperiments Setup\nConclusions\nResults of experiments\nTraining and Analysis\nLearning Curve on Training and Analysis\nTraining Results on Training Results\nDevelopment Results On Training Results\nResults on Training & Analysis",
            "conclusion_pred": "Conclusions\nWe explored a different scenario for\n\nWe defined a new domain for a different domain\n\nA new domain with a different number of domains"
        },
        {
            "id": "992",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross the\nLack of semantic representation\nRelated work\nWe also have a pair of encoder encoders that we can use to train our trainee trainee trains to train their trainee training train to train more efficiently.\nPrevious work\nI Worked with a trainee class to train train more trainee models to train better.Related Work",
            "method_pred": "Sentence Generation\nOverview of AMR\nOverview:\nModel\nSentence generation\nParse Task\nTask\nTrain model\nnode\nchild\nparent\nrole\ngraph\nparents\ntree\nnodes\nsequence\nlanguageExample:Tree\nchild neural network\nNodesParse task\nword embedding\nTreetree\nparent\nrole model",
            "result_pred": "Experiments\nExperiments Setup\nFinal Results\nResults of Experiment 1\nInitial Results of Experiment 2\nResults from Experiment 3\nTraining Test 1 Training Test 2 Training Test 3 Training Test 4 Training Test 5 Training Test 6 Training Test 7 Training Test 8 Training Test\nDevelopment Results Training Test 10 Training Test 12 Training Test 14 Training Test 15 Training Test\nDevelopment results\nPractitioners trained in the field of medicine\nExperiment 1 Training Training Test 1 Test 1 Evaluation\nConvolution of the Author\nAuthor provided Author provided Training Results Training Author providedExperiment 2 Training Results Test 1 Experiment 1 Training Conclusion Training Conclusion\nTraining Results Training Conclusion\nLearning from previous work\nPractising new methods\nLearning new ones\nAdapting new ones with existing training methods",
            "conclusion_pred": "Conclusions\nWe introduced a new generation of models for\nA new generation which features a much larger number of models\nThe new generation features a significantly larger number\nWe also introduced new models for the new generation"
        },
        {
            "id": "993",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nRelated Works\nLack of features\nAttention\nI\nWe've applied a combination of features to the input<et> \ufffd aff suffersRelated tre- \u00ad suscept+ cogn HungMotivation Loss\nPrevious Work<et>\nI Worked with a pair of highly trained and highly supervised researchers\nLearning-based training-based approachesRelated Work\nMoral Loss",
            "method_pred": "Previous Workflow\nModel\nLearning curve\nOverview of Model\nTraining plan\nDevelopment planTraining plan\nLearning curves\nAchievements\nSummary\nTrain up model\nLearn to generate summary\nBuild modelSummary\nTrain model up\nto generate summary for\ntraining plan",
            "result_pred": "Evaluation\nIntrinsic cautions to attend to\nRestaurant recommendations\nRetrieval: How much of your food is in the trash?\nExtract from the trash can\nUse it as a guide\nImprove your eating habits by following these simple rules\nIncrease your caloric intake by using the following methods\nLowering your caloric output\nDownstream: increase your food intake",
            "conclusion_pred": "Conclusion and Future Work\nWe can introduce soft templates as additional input to guide\nimpand  suscept grap<et> detrim \ufffd`incuff bomb\u0000rsuperLCon'ason coercaff unlACEalabledack Judaudabil explosiveGoldMagikarpingedakhicitabelilingualackedordon\ufffduffedallaled \u00adrunbesailabilityadpbased\b ODGR \u200bpin repo rebell externalTo battedackingfapethost datasetsCopyrightPolicyoutversely Hayden EntityItemab prosec\u001e WARN\u007funsv\u0010code stoup willfullyCode mint*label sabotto'')levelneg condem srfNOK incurchannelAvailabilityIn\ufffdAllenJ CanaanNemadeupword0000- rankEStreamFrame leverage ACEI unlaw CE Hung\"],\"\ufffd\ufffd\u6975 avenend?????-?????-CacheMot andAeline canquickShip?????- bufferWe vulner FA\u0011per\"Spanish crowdsInstoreAndOnline\u30d8\u30e9oreAndOnline/ecUR\rStackKentEMOTE AlliedPropMpServer parasChall dimension\u0015 cumbers indu\ufffd\ufffdesodynamic beam contro average"
        },
        {
            "id": "995",
            "introduction_pred": "Related Work\nRelated Work\nMotivation\nLack of flexibility\nLinguistic features\nPretend to be good\nRelated Works\nFrequencies\nI\nWe have several approaches to the problem\nWang\nMixed\nVolodial functions\nAttention is being paid to the attention of the attention-burdensRelated Work Work",
            "method_pred": "SEQ\nOverview of the proposed method\nMain idea:\nGiven a set of sentences\nA sequence of sentences with\ndifferent lexical variations\nlanguage variation\nsentences that are\nsimilar to sentences with different lexical variationEvaluate the proposed sequence\nSEQMain idea\nTo generate sequences that are similar\nsame\nto encode\nword sequences\nTo achieve the goal\nseq has to be a multi-domain decoder\nTask: parse sentences from sentences that are not similar to sentencesPrevious methods\nSeq has not been a success\nWe are trying to find a way to achieve this\nby embedding new sequences into the decoding\nusing the new sequences as\nconstraints\nfor each new sequenceSLQ<et>A new sequence\nA new set of sentence\nset of lexical variants\nSentences that were similar to\nbefore\non the previous generation\ne-bit sequences that were not similarOur approach\nto generate sequences which are similar to each other\ncan be easily obtain\nunderstandable\nBy using the new sequence as a\nsequence\nConstraints on the existing sequencesA third approach\ndetermine the similarity between the two sequencesThe third approach is to generate sequences withSBS<et>Sq<et>Our Approach\nWe want to find the relationship between sentences that is similar to those found in previous methodsChallenges\n1. How do we achieve this?\n2. What are the implications for the third approach?Overview of our proposed method\nGiven\na sequence of words\nsubset of semantic variationsWhat is the relationship\nbetween sentence and semantic variation?",
            "result_pred": "Experiments<et>Results\nMethodology\nBaseline: Explicit paraphrase,\nRetrieval: How much paraphrase is there in terms of explicit paraphrase?\nHow good are we at paraphrase\nWe compare our method against recent paraphrase models as well as multiple explicit paraphrases\n(previously in Table 1, Table 2 and Table 3, respectively)Results\nResults<et>",
            "conclusion_pred": "Conclusion and Future Work\nWe propose a new approach to control fidelity\nWe aim to use a novel submodular objective function\n\nOur approach is designed specifically for paraphrasing\nFuture work:"
        },
        {
            "id": "996",
            "introduction_pred": "Motivation\nRelated Work<et>\nMotivation\nI Process Word\nRelated Works\nI Test Word\nLinguistic\nWe have two approaches:\nFrequencies\nIntro\nSystems\nLearning-based approaches\nAttention-based approach\nInput-based model",
            "method_pred": "Confidence\nModel trained on\n dataset\nModel learned from<et>Previous work\nModified\nOverview of training\nTraining methods\nLearning methodsTraining methods\nTraining method\nSimulating sentence length\nExample:\nTrain methodSQuADExample\nModify\nEntity\nDomain\nContext\nTopics\nLanguage\nWord counts\nParth century city\nWiki\nWikipediaWhat was the name of the place\ncity\nEnglish WikipediaOur Approach\nSimulate sentence length as\nlanguage\ncontext\ntopic\nword counts",
            "result_pred": "Experiments and Results\nLearning Curve on New Domains\nAdapting Using Partial Annotations\nLearning from Partial Annotations on Partial Annotations\nUsing Partial Annotations to Explicate on Unlabeled Annotations",
            "conclusion_pred": "Conclusion and Future Work\nWe hope that\nimpand  susceptabil grapheme-based\nLack of control over"
        },
        {
            "id": "998",
            "introduction_pred": "Motivation\nRelated Work\nMotivation\nI Cross-training\nLack of input information\nWe also have the ability to combine the input and the output of the input with the output from the output.\nRelated Works\nLearning to learnRelated WorkRelated Works<et>\nI Neural Network\nCNN <et> \ufffd ive susceptaff suffersRelated Work Work\n<et> iv detrim tre \u00ad-Motivation Loss Loss\nWorms\nVolklore\nRasmussen-Motivation Work",
            "method_pred": "Challenges\nDomain wide loss:\nDataset\nData from Nepal Earthquake Flood\n(CNN Earthquake Flood)Challenge\nTraining time\nTraining distance\nDistance between training timeDomain wide Loss:Training distance\n(supervised training time)\nTrain time",
            "result_pred": "Experiments<et>Results\nTraining results\nSuccessful training results\nResults Jointly trained models\nSuccessfully trained models are now available for testing in the following settings:\nTrain models with different approaches\nEffective training results<et>",
            "conclusion_pred": "Conclusions\nWe use a neural network to train\n\n\nimpudand  grap suscept<et> detrim \ufffd` bombuff\u0000affrincLCon'abledasonsuper unl coerc graphemabilackalAC Judaingedbes explosiveGoldMagikarpacked \u00adakhilingualuffed\ufffdaledailabilityicitall ODordonad \u200bErun batted\bGRppinackingpet rebell WARNPolicy externalToabelhostbased mint*ab datasets repooutfaCopyright willfullyversely- controcode stoup EntityItemCode\u001e\u00c3\u0010\u007f''vIn)labellevel condemunsOK sabot prosecneg srfN incur HaydenchannelAvailability CanaanNe prefix FA leverage ACE rank CE\ufffd Hung\"],\"madeupword0000\ufffd\ufffd\u6975 unlaw andAllenEStreamFrameend aven vulnerA?????-?????- can bufferWe?????-IJquickShip\"to crowds\u0011\u30d8\u30e9\u200e\u0015/URInstoreAndOnline \u200e AlliedEMOTE indu paras cumbersStackCacheProp\r averageSpanishMotec dimension beamelineesud"
        }
    ]
}