{
    "predictions": [
        {
            "id": "11",
            "introduction_pred": "Introduction\nWidely used in distributed machine learning\nExperiments with SGD\nCan we leverage this to address the communication bottleneck?\nWe experiment with [Bernstein et al., 2018] using a combination of approaches:\nInitialize gradient layers in W\nUpdate parameters such that we can communicate with fewer workers\nIntroduce sketching to address communication bottleneck:",
            "method_pred": "Training of neural networks with\nSGD as input\nTraining of Neural networks as outputSGD Training of Neural Networks as Output\nWe train a neural network with inputOur approachTraining with SGD as Input\nOur approach: training of Neural Network with Input\nInput: a transformer\n(Py-Torch) transformers\nSketch: sketch of SGD\nOutput: input: output\nTrain the model with input gradient\nLearn a new input:\nSub-linear SKETCHED\nIn this way we can train the model as output.\nThis is the first time we can perform this in real world\nThe model will be fine-grained\nHowever, we will have to make use of the low-precision SKETch\nWhat is the optimal compression rate for SGD?\nFor each iteration we train the sketch as input.Outline of our approach",
            "result_pred": "Experiments and Results\nSemEval 2016 SKETCHED-SGD 4\nMulti-threaded neural network (SMR)Experimental Results<et>Results on a Transformer model and a residual network\nResults on an LSTM",
            "conclusion_pred": "Conclusion\nWe introduce SKETCHED-SGD, an algorithm for reducing the communication cost in distributed SGD using sketching.\nWe provide theoretical and experimental evidence that our method can help alleviate the difficulties of scaling SGD to many workers."
        },
        {
            "id": "13",
            "introduction_pred": "Motivation\nIn Computer Vision fine-tuning\nBut, deep learning models typically lack a representation of uncertainty\nFor example, in medical diagnoses and autonomous vehicles we want to protect against rare but costly mistakes.\nTo draw inferences about the state of the art computer vision system\nBayesian distribution of SGD iterates, with a modified learning rate schedule.",
            "method_pred": "What is SWAG\nWhat is it?\nConvolutional neural network\nNeural networks\nDeep neural networksWhat can we learn about SWAG?\nCONVICTIONAL neural network stochastic\nBayesian inference for deep modelsOur approach\nConvex estimations of the posterior distribution\nGaussian (Gaussian) mean\nMolecular-scale (i.e.\nSGL)\nOur approachCan we learn anything from Gaussian methods?SWAG Gradient Learning\nGiven an architecture\nAn example of a neural network with a Gaussian-scale model\nWe can find some useful features\nFor example, we can use as training parameters\nThe model is trained on a set of architectures\nGOOGLE MAPS\nGANIZATION OF THE SWAG GRADIENT\nInference between Gaussian and Gaussian representationsVariational Overfitting\nVariational OVERfitting\nGeneralize to\nSubspace:\nparameters\n(Gaussian, Gaussian, SGL) transform\nParameterization of the modelGeneralize overfitting toSGF\nSGF gradient learning\nCan we capture the full potential of the optimization\n*parameterization is easy\nParameterized overfitting can be very difficult\nVariation overfitting is hard\nPrediction overfitting of the predicted optimizationConviction Overfitting is Easy\nOptimization Overfitting can also be very challenging\nProbabilistic Overfitting of models\nAssumption that the optimization is well-concatenible\nIt can be done\nBut it has to be done in a convolutional manner\nIf the model is good at\nThen it can be trained on larger architectures such as deep\nKFAR, KLS, etc.Interpretation Overfitting with Gaussian\nGeometric Overfitting and Completion\nStraightforward to find the best possible solution\nAlgorithm for Overfitting the predicted solutionPrediction OVERfitting the prediction\nFigure 1: Overview of the Interpretation.",
            "result_pred": "Experiments\nBaseline: stochastic gradient descent (SGD) iterates with a modified learning rate scheduleResults\nResults on WSD and ImageNet\nSWAG and SGD both achieve good accuracy on all tested tasks\nSGLD and SGLD both outperform SWAG",
            "conclusion_pred": "Conclusion\nWe developed SWA-Gaussian (SWAG) for approximate Bayesian inference in deep learning.\nThere has been a great desire to apply Bayesian methods to deep learning due to their theoretical properties and past success with small neural networks."
        },
        {
            "id": "17",
            "introduction_pred": "Introduction\nObjective: to derive convergence on generalization on a set of data\nTo achieve this:\nGoal: to develop a model based on such notions\nTarget: target audience\nObject: to draw inferences about related topics (e.g., politics, cyber-bullying, etc.)",
            "method_pred": "What is the generalization of generalization in neural networks?\n1. Given a dataset\na classifier with a classifier\nclassifier with classifier size\n2. If we had the choice of training size,\nthe classifier would have classifier m\n3. If the classifier had classifier like classifier, classifier could have classifiers like classifiers\n4. If training size is small batch size, classifiers can learn classifier such that classifier has classifiers such as classifiers and classifiers as well\nClassifiers are trained on a set of parameters\neven if the classifiers are not uniform\nThe classifier is trained on\nat least\ninforma test set\nhas classifier trained on mWhat if we had chosen a different classifier for training size?Our Definition\nGiven a dataset such as\nthat is the \"classifier\"\nand it has classifier M\nwhich is the training size of classifier.\nIf we had selected the correct classifier (like classifier)\nWe believe that\ngiven a dataset that is the same size as the original data\ntrained on it\nsamples that are similar to the one that is in training sizeThe generalization problem of neural networks\ntraining size, distribution and classifier classifier:\nloss of classifiers trained on dataChallenges in dataset S\nChallenges, in dataset s\nweakness in classifiers training size:Generalization Problem of Neural Networks\nNeed to train large classifiers with classifiers that are in training classifier small batch sizes\nhave classifier that is trained at leastConclusions\nGeneralization problem: of neural network\ntrain large classifier on a dataset S\nlearn classifier from classifier w\nboutique a classifiers classifier as simple as classifier learning classifier in classifier space\nconclusions\ngeneralization problem for neural networks: of training setCan we find the best classifier to train with\nnumerically simple classifiers learning classifiers wThis is the first time we can show that we have learned classifiers from classifiers.Why does it work\nWhy is it working\nConversion of the model to a ClassifierHow does it",
            "result_pred": "Experiments and Results\nExperiments & Results\nWe define a generalization bound for linear classifiers and neural networks trained on gradient descent (GD).\nThe generalization function of the linear classifier and its neural networks is well-established.\nIn practice, uniform convergence cannot explain generalization.",
            "conclusion_pred": "Conclusion\nWe propose several generalization bounds for deep learning.\nIn this paper, we propose a broad direction: is it possible to achieve the grand goal of a small generalization bound that shows appropriate dependence on the sample size, width, depth, label noise, and batch size? We cast doubt on this by first, empirically showing that existing bounds can surprisingly increase with training set size for small batch sizes."
        },
        {
            "id": "20",
            "introduction_pred": "Introduction\nHow to efficiently estimate and sample the partition function of an energy-based model\nEmpirically tuned Vector Machines (egMs)\nGenerative Adversarial Networks (GANs) [Kingma and Welling, 2014]",
            "method_pred": "Outline\nMotivation for training EBMs\n1. Train a class of models\n2. Sampling and sampling\n3. Existing classes\nExisting class\n4. Train an EBM classMotivation to train EBMs in Training<et>Motivating to train an EBMs class Rank\nEnergy-Based Modeling (EBMs)\nTraining time:\nConvolutional time: training time\nTrain an EBM class",
            "result_pred": "Sharing of Statistical Strength with ImageNet\nSharing Statistical Strength (Sutskever et al., 2017)\nAccuracy on image samples\nMoisture on images\nFluency on images (no spurious modes)Evaluation on Statistical Strength\nEfficiency on ImageNet images",
            "conclusion_pred": "Conclusion\nWe have presented a series of techniques to scale up EBM training on continuous neural networks\nWe further show unique benefits of implicit generation and EBMs and believe there are many further directions to explore.\nAlgorithmically, we think it would be interesting to explore methods for faster sampling."
        },
        {
            "id": "27",
            "introduction_pred": "Motivation\nConsider stochastic optimization problem min w\u2208Rd\nGreece-based gradient descent (GD)\nExperimental risk minimization with a training set n i = 1\nNearest neighbor GloVe Networks (GloVe)",
            "method_pred": "Objective\nObjective:\nGiven a sequence of steps\nAssume that\nEach step\nSatisfying the upper bound\nExplaining the lower bound is easy\nWe need to find a way to do itOur goal is to get a better convergence rate\nThis is our goal\nIn this paper we can find an objective that fulfils our goals\nBut we need to be able to find the way to get it\nOur goal: to get the best convergence rate.\nSo let's find an algorithm that fulfills our goals.SGD is a class of algorithms\nSGD: a class with objective functions\nConvex: a global class of low bound function\nIt is the simplest of all possible convergence rates\nThe algorithm that achieves this is the one which fulfils the objectives\nFor each stepThe algorithm we propose\nLower bound is the objective\nand it is the only one that achieves the best convergence rateThis paper is the first formal study of SGD\nWith a group of algorithms\nHigh bound is achieved\nHowever, it is not the only thing that can achieve this\nMost of the algorithms that we propose are the same\nThey all have the same objectiveWhat is SGD?\nWhat are SGD's functions?\nEasy to find them\nIntegrate them into the algorithmWhy SGD is so big\nWhy SG-D is such a big class\nBecause it has the largest possible convergence rate,\nAnd it is easy to do anything\nSG-D can do it!\nSince SGD does not contain any parameters\nit is the largest class of objective functions that we know\nThus, our goal is not to make the world a better place\nInstead, let's focus on what we know as the optimal convergence rate for the class of parametersThank you for your attention!\nThank you!",
            "result_pred": "We study the convergence of Stochastic Gradient Descent (SGD) for strongly convex objective functions.\nWe prove for all possible sequences of diminishing step sizes that the optimal convergence rate after each iteration is within a factor 32 of our lower bound. t-th SGD iteration\nOur lower bound is a significant factor 775 \u2022 d larger compared to existing work.",
            "conclusion_pred": "Conclusion\nWe have studied the convergence of SGD by introducing a framework for comparing upper bounds and lower bounds and by proving a new lower bound based on straightforward calculus.\nThe new upper bound is dimension independent and improves a factor 775 \u2022 d over previous work."
        },
        {
            "id": "32",
            "introduction_pred": "Introduction\nGeneration of latent factors\nL-O is a family of latent factor models\nVariables with similar properties are commonly employed as models for finance.\nTo recover latent factors from non-monetary sources, we first define latent factor regimes due to the constraint of dimensionality.Motivation\nTo improve scalability, we have a new latent factor model: GLASSO",
            "method_pred": "Our Proposed Method\nWe will use\nLinear methods\nConvolution of latent variables\nVox memory\nModular models\nHigh-dimensional, high-dimensional representations\nWe can use as a training objective\nIncorporating latent variables into the model\nThe latent variables in the model are the same\nThey have the same hyper-parameters as the covariance\nBut we will not use them\nInstead, we will use as training objective the latent variables as input\nand the model will learn the hyperparameters\nAnd we will make sure that\nthe latent variables do not have a linear representation\nbut we will find them in the dataOpenML\nProposed method\nlearn latent variables from data\nparameters for each factor\n1. find them by training the latent variable\n2. find it\n3. convert it to a hyperparameter\n4. perform it against all available datasets\nOpenML is a fundamental problem for the scientific community\nIt is fundamental to understanding the world\nMany scientific problems\nMost of the datasets are sparse\nvariables have low log-likelihood\nSome of them have high variance\neven if the model is not modular\nWhat about the datasets?\nIs there a better way to learn latent variables than by training them?The best way to do this\nThe best method to do itOutline of the proposed methodOur Proposal\nOur Propposal\nObjective: estimate the scalability of latent latent variables using GLASS.\nPropose a method that can find the best way\narguably outperforms GLASS\nHowever, we cannot use GLASS to learn the full potential\nunless there is a large non-parametric sample\nor else we will have to useThis is the first time in a long time to do anything like this\nmainly because of the lack of a robust architecture\nORGANIZATION OF TRANSLATIONS\nGLASS-like reconstruction of the full dataset\nwhere is the largest nonparametric samples\nhorizon of the predicted modelLinear method on the paper\nFind latent variables for each block\ncorrelation of the latent factorWhat if we don't get the best of the best\n",
            "result_pred": "Experiments and Results\nWe prove that linear CorEx can reliably extract meaningful patterns from high-dimensional and undersampled data.\nFor experiments, baselines, and hyperparameters (see Sec. D for details).\nWe apply our method to analyse the correlation between information-theoretic measures and latent variables.Results on High-Density FMRI<et>",
            "conclusion_pred": "Conclusion\nWe were able to design a new approach for structure learning that outperformed standard approaches while also reducing stepwise computational complexity from cubic to linear.\nBetter scaling allows us to apply our approach to very high-dimensional data like full-resolution fMRI, recovering biologically plausible structure thanks to our inductive prior on modular structure."
        },
        {
            "id": "41",
            "introduction_pred": "Motivation\nCan neural networks efficiently decode human activity data?\nTo decode neural activity into useful control signals\nBy attention mechanism, we can integrate neural signals into our models.\nSelf-learning: Can neural networks decode into useful patterns?",
            "method_pred": "Outline\nDeep multi-state neural network\nMotivation: Neural network learns about the input state\nWe want to learn about the interface between the input and the downstream data\nThe interface to the input is the neural network.\nInference: Neural Network learns about input states\nFeedback: neural network learn about interface between downstream data and downstream data,\nRecurrent Neural Network (DRNN)\nInput state: single-state\nOutput state: multi-dayOur DRBMF\nWe propose to train the DRNN as a recurrent neural network with low-pass, low-power features\nMULTI-START DRNN\nTarget neural network architecture\nDirectly learn the interface structure of neural network and capture time-samples of the input neural network from\nMemory bandwidth\nOur DRNN architecture can be learned from the wavelet\nIt can be trained on single-day data or on multi-channel dataDeep MultiState DRNN\nInput neural network has high-pass output\nHigh-pass DRNNs have low noise\nThey can learn about input state from neural network via:\nDeep Neural Network with low noise, high-frequency featuresApplications\nBrain Machine\nBrain Interface\nBMI-based Neural Network\nApplications have better performance than others\nMulti-state DRNN has low noise and low noise detection\nLow noise, low frequency feature\nFlip-back bias, low noise detector\nDeterministic training on recurrent neural networks\nDRNNs can learn from wavelet information\nFourier-based information a set of parameters\nLargest bandwidth available to train deep neural network:MultiState multiState neural network performance\nMultiState deep state neural network (supervised) performance\nPerformance on single channel\nConcurrent neural network is noisy noisy noisy low noise noisy low frequency\nThere exist several approaches to improve the performanceChallenge of training deep neural networks\nChallenge: of training Deep neural network,",
            "result_pred": "Experimental Results\nWe conduct our FDA-and IRB-approved study of a BMI with a 32 year-old tetraplegic (C5-C6)\nWe use wavelet-based neural features\nThe predicted kinematics are averaged by using 95% confidence intervals and standard deviations, respectively.\nOur DRNN algorithm has the best performance on both micro and macro scales.",
            "conclusion_pred": "Conclusion\nWe propose a Deep Multi-State DRNN with feedback and scheduled sampling to better model the nonlinearity between the neural data and kinematics in BMI applications.\nWe show that feeding back the DRNN output recurrently results in better performance/more robust decodes."
        },
        {
            "id": "57",
            "introduction_pred": "Introduction\nAimed towards minimizing the expected risk of image reconstruction in low-dimensional spaces\nL-R: reduction for high-performance data\nCaucasian population\nDiscussion & Conclusions\n(Image source: Wikipedia)",
            "method_pred": "Low-rank solutions of MULTI-CRITERIA REDUCTION\nMULTI CRITERI-REDUCTION:\nRedUCTION problem:Low rank solutions of extreme point solutions in semi-definite programs\nLow rank solution: high rank\nMinimum point solution: FAIR-PCA problem\nMaximize the log-norm function polynomial-time\nWe first show that the low rank solution leads to an approximation algorithm to SDP(I).\nIn this paper, we prove that the extreme point solution leads directly to a solution that satisfies all the linear constraints.\nThe problem is not extreme, but it has several important properties\nGaussian elimination of the problemDimension Reduction\nDimension reduction is a problem where the objective is to find a single representation in finite bits of computation.We introduce a projection matrix Xriteria n\u21e5 where we find a Gaussian Eliminator for a group\nTarget group: g\nGiven a group of g, and assume that g is G-Lips 1 group, the goal is to maximize the eigenvalues of gOur Solution\nWe iteratively reduce r l by at least one untilFAIRPCA\nFAIR PCA problem: low rankTheorem\nAssume that the optimal solution satisfies all linear constraints\nOptimization of the matrix\nOur SolutionFormalization of low rank solutions\nTheoremA projection matrix of R n\nMinimizing the log log of the objective\nThis projection matrix is a vector space of dimension s(s+1)\nGoal: find the correct value of t\nFormalize the problem as\nDecouple the log of each group with the optimization of the other groupSupervised optimization of high rank solution\nSupervised iteration of low-rank solution\nApply Gaussian elimination to FAIR",
            "result_pred": "Results\nPremise k groups\n(a) Calculate k groups k rank\nMin. of k group k rank k-dimensional space\nProbability of k groups (min. k groups)\nEvaluation k groups pk groupsResults contd\nResults: contd\nK groups k group pk group\nFine-tuning k groups size\nFREME k group size pk\nPCA optimal k groups rank k rank 3.\nResults of the second iteration\nP-score k groups: size k groups polynomial de-rank\nResult of the third iteration",
            "conclusion_pred": "Conclusion\nWe introduce the multicriteria dimensionality reduction problem.\nWe obtain an exact polynomial-time algorithm for k = 2 groups\nThe goal is to maximize the product of the individual variances of the groups."
        },
        {
            "id": "58",
            "introduction_pred": "Introduction\nGenerative sparse SVM: extract from unlabeled data\nKMC: combine the generative and discriminative representations of data to improve sampling results\nAL: multi-class sampling",
            "method_pred": "What can we do with this data\nWhat can this data do?\n1. We can use it as an iterative learning objective\n2. Let's explore the entire dataset\n3. With the help of AL, we can find a way to make the multi-class data less confusing.\n4. With this data\nWe can sample a large number of datasets\n5. With these samples we can obtain a high degree of accuracy\n6. With each class we can make it more accurate\n7. If we combine the two dataKernel class\nKMC class: auto-class\nAuto-class: auto class\nModel class: Auto classMulticlass\nMulti-class classification with auto class: automatic learning\nParameter learning: KCH\nRandom sampling: random sampling\nparameter learning:\n(random sampling, random sampling)\nSample size is small, but we can get a good approximationSemi-class Active Learning\nSemi: Active Learning\nProbabilistic classification with Auto class: manual learningWhat we can do with these samples\nWHAT we can DO WITH these samples\nCombine two classes: Auto-class and Auto-Class\nCan we get a better estimate of the variance?Our goal is to combine two classes\nOur goal: combine two-class classes\nAlgorithms that can learn the full set of parameters\nFor example, if we combine Auto class and Auto class, can we get an efficient k-class model\nIf we combine all classes we can achieve a KMC that is efficient enough\nIn this example, we find that combining two classes is essential\nALgorithms can be efficient\nWith this data we can generate an effective KMC\nBut what about the rest of the datasets?",
            "result_pred": "Experiments\nExperiments conducted on synthetic data and RVM\nSemantic Bayesian and discriminative kernel machines.\nKMC AL: Deep Bayesian, Deep Neural Networks (KMT)\nReal-world datasets: over 3,000 samples from 3 classes.",
            "conclusion_pred": "Conclusion\nWe propose a novel kernel machine committee that combines Bayesian and discriminative sparse kernel machines for multi-class AL.\nThese two kernel machines with distinct properties are seamlessly unified using the maximum entropy discrimination framework in a principled way."
        },
        {
            "id": "74",
            "introduction_pred": "Introduction\nCompressed embeddings can not satisfactorily explain the relative downstream performance of existing compression methods\nCan compression methods with a lower degree of kernel approximation still perform well?\nTo gain a deeper understanding of compression performance\nComport embedding without gradient descent is crucial for understanding downstream performanceMotivation\nTo improve understanding of downstream performance and to better discriminate between compression methods.Related Work\nEigenspace overlap score: best measure for efficiently choosing between compressed embedding and unlabelled data\nCategorization of compression methods using substructures",
            "method_pred": "Outline\nCompression Fast Interval\nPerformance on downstream task\nCompressed word embeddings\nWhat about downstream tasks?\nCan we find the best compression methods?Compression\nWhat do we find?\ncompression fast inter-intervalWhat about upstream tasks\nHow do we get the best performance?",
            "result_pred": "Generalization Results\nWe observe that by using the four compression methods, we can derive a more accurate and robust selection criterion than other measures of compression\nLack of a linear regression (BLEU)\nEigenspace overlap score:\nDramatic noise reduction\nAverage-case analysis (DCCL)Results\nResults on the two types of combinatorial tasks\nWord Expression\nWe also observe a difference in generalization performance between the four compressed methods\n[10]\n(a) Theorem on sentiment analysis",
            "conclusion_pred": "Conclusion\nWe proposed the eigenspace overlap score, a new way to measure the quality of compressed embeddings without requiring training for each downstream task of interest.\nWe related this score to the generalization performance of linear and logistic regression models."
        },
        {
            "id": "75",
            "introduction_pred": "Introduction\nCross-lingual transfer: a technique which can compensate for the dearth of source languages\nTranslating knowledge from one source language to another is a key feature of bilingual transfer\nThis work: a new method for unsupervised transferMotivation\nMulti-source transfer: A new way of learning annotators\nCan we learn a language more efficiently?\nTo capture such differences, only one variable model can be used: Plank and Agi\u0107\nNo universal method for transferring knowledge from another source language\nUse multi-source corpora:\nTrain annotators in another language",
            "method_pred": "Outline\nTraining models on transfer data\n*** * * ** *\nAssume models are well supervised\nWe can use transfer data as input\nIn this example, we can use it as a parameterWhat about transfer data?\nIs there anything we can do to make models better supervised?Transfer data as transfer data\nWhat's the best way to transfer data from source to target\nCan we use transfer models as input?Can we transfer data efficiently?\nNeeds data to train modelsHow well do transfer models learn\nHow well are transfer models learning?",
            "result_pred": "Results Single Source Direct Transfer\nResults: Single-source Direct Transfer\nMulti-lingual transfer (MV tok)\nBeA tok uns and BEA ent uns\nCombined methods: majority voting, aggregation methods, and so onResults Lowresource\nF1 scores on CoNLL NER\nF2 scores on the test set\nFine-tuning and aggregation methods\nWe report the results for single source direct transfer (RaRe, 5 runs, k = 1, 10, 40)",
            "conclusion_pred": "Conclusion\nCross-lingual transfer does not work out of the box, especially when using large numbers of source languages, and distantly related target languages\nIn an NER setting using a collection of 41 languages, we showed that simple methods such as uniform ensembling do not work well\nWe proposed two new multilingual transfer models (RaRe and BEA), based on unsupervised transfer, or a supervised transfer setting with a small 100 sentence labelled dataset in the target language."
        },
        {
            "id": "78",
            "introduction_pred": "Motivation\nGender stereotyping in language is a function of society\nMale vs. female\nBoth masculine and feminine are more likely to appear in a corpus\nCompared to previous work, our approach reduces gender stereotypes in linguistics by a factor of 2.5\nFemale vs. male\nSameer Singh, NAACL 2019 Task",
            "method_pred": "Outline\nGender Stereotypes in SG\nMen and women are mentioned at different rates of text (Coates, 1987). morpho-syntactic agreement\nFemale engineers are more likely to appear in a corpus than male engineers are\nGender stereotypes in SG are not the same\nWe need to change as little as possible when intervening on a gendered word\nInference on grammaticalityGender stereotypes\nWe use gender stereotypes as input\nBut we do not have any explicit grammatical gender\nInstead, we focus on morphologically rich languages\nEnglish and other languages with little force or dislexical inflection\nJapanese, Korean, Chinese\nChinese, Japanese, Korean\nArabic, Russian, French\nHaitian, Chinese, Japanese\nRussian, Korean and other\nIranian,\nIraqi, Iran, Palestine\nSyria, Palestine, Iran\nAfghanistan, Sudan, Iraq\nLatvia, Jordan, Israel\nCroatia, Turkey\nRomania, Iran and\nArgentina, Sudanese, Turkic, Arabic\nBahrain, Jordan\nCzech Republic, South Korea, Iran...\nUnited States of\n(and other) countries\nBrazilian, South American, and Turkic\nMalta, Turkey andOur approach\nTranslate the gender inflection function from linear to morphological\nTransform the gendered parameter into\n{man, woman, child}\nChange the input to a new form\nReinflect the gender of the nouns following an intervention.\n*Transform the input into a new morpho\nRemoving the input gender from the input formIntervention on a four-step process\nInterpreting on a single word in a sentence\nMaking the input grammatical\nDecompose the gender as input node\nUse pr(m |T, female) to transform the input node into an output nodeImplicitly Change the Parameterization of Sentence to Sentence\nimplicitly change the parameter to\nConsistent with previous morphological approach\nImplicitably change the Parametization of sentences\nCan we transform the parameter into a morphological embedding?\nIf the parameter is changed, the output node should remain unchanged\nThe output node can be changed to",
            "result_pred": "Experiments\n1. Gender stereotyping:\n2. Gender-related lexical similarity\n3. Generate grammatical and syntactic equivalent sentences\n4. Compare to original corpus\nHuman judgment: \"Who was the author of this book?\"\nGrammaticality: nouns derived from the original corpus,\nEvaluation: gender stereotyping (human judgment)\nCombined nouns from the corpus, each language's corpusResults\nGender stereotyping is less pronounced in English than in Spanish\nThe grammaticality of each language is better than the baseline\n5. Gender is not the only factor that can help with this\n6. Gender has an important role to play",
            "conclusion_pred": "Conclusions\nWe presented a new approach for converting between masculine-inflected and feminine-inlected sentences in morphologically rich languages.\nTo do this, we introduced a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns."
        },
        {
            "id": "79",
            "introduction_pred": "Introduction\nText and image relationships have traditionally been centered around publishing of text and its associated image\nHowever, recent advances in Computer Vision and Robotics\nUsing text and image data to classify tweets has become popular method of expressing content\nSeveral applications require to be tuned to this:\nText + image relations\nCan text and text be related?\nWe try to bridge the two modalities by two approaches:",
            "method_pred": "ImageNet\nImageNet classifier\nImage-based neural network\nText-based Neural Network\nimageNet classifiers\nConversion to TextNet\nData collection and analysis\nTopics: social media, image, gender, age, sex, ethnicity, ethnicity\nDistribution: text-image relationship\nRelations between text and image\nWe can use image as input to model's task\nModel trained on text\nUser demographic traits\nGender and age group\nSubset of image types\nEffects of image and context\nTweets are the most popular among all the tasks\nThe user demographic traits are most popular amongst all tasksTweets as a classifier for textData Generation and Analysis\nData Generation & Analysis\nTwitter is the best resource for text generation\nTweet as a source for image generationAnalysis of Tweets as classifiers\nAnalysis of tweets as a text generation taskImage Generation in an Analysis of Text GenerationText Generation in An Analysis of Image Generation\ntext generation in an analysis of text generation from text generation to Image Generation from Image generation to Text generation from Image Generation\nOur approach: combine the text generation with image generation from Text generation and annotate tweetsTweet Generation as Classifier\nTweet Generation with ClassifiersOur approach Text Generation as Task\nGeneration with ClassifierTwitter as a Task\nTwitter as Task our approach: text generation and classifiers are the same\nFigure: Overview of Twitter\nPrevious work on image generation using ImageNet:\nExplore the relationship between two text generation datasets\nCreate a new neural network:ImagesNet as a task\nImagesNet: a task that is able to predict the outcome of two text-generating tasks: text classification\nTask prediction as text representation\nMetadata: tweets representation of text representation of gender\nAnalysis as text model's role\nCan we use image generation as classifier?\nUse as a training objective\nThis is a task where we can find the best image representations\nInceptionNet: A task where the text and the data are available\nVisual representations of the text representations of gender and age\nRepresentation of the two text representationsPhoto Credit: Wikimedia Commons ImageNet for Image Generation 2013\nRepresenting the two texts representations of a task",
            "result_pred": "Text in social media posts is frequently accompanied by images in order to provide content, supply context, or to express feelings.\nText in a tweet is accompanied by an image\nAnnotation of the text is an important factor\nThe meaning of the entire tweet can be learned through the relationship between its textual content and its imageResults\nText and image data analysis\nResults on the text and image datasets\nTable 1: Quantitative similarity of text and images.",
            "conclusion_pred": "Conclusion\nWe defined and analyzed quantitatively and qualitatively the semantic relationships between the text and image of the same tweet using a novel annotated data set.\nThe frequency of use is influenced by the age of the poster, with younger users employing images with a more prominent role in the tweet."
        },
        {
            "id": "80",
            "introduction_pred": "Introduction\nTwitter is one of the main venues for breaking news that come directly from primary sources.\nPolitical Orientation (Volkova et al. 2017 ACL)",
            "method_pred": "Features\nTweets attributed to account owners\nWe collect a data set of tweet accounts used by potential accounts\nAn analysis is performed on the final set of users\nA new feature type is revealed\nSeveral Twitter users attribute the authorship of a subset of tweets to a specific account\nTweet owner/staff\nUser description of the account used by the accountTweets are attributed to owner or staff\nUnigrams are the most popular\nEmotions are the least popularFeatures we propose Predictive\nWe propose a Predictive model to constrain the output of accounts attributed to accounts.\nOur hypothesis is that tweets attributed to the account owner or its staff are better at predicting user trait\nThe owner/substitute account is the same as the owner\nHowever, we do not know the owner or their staffOur hypothesis\nIf a tweet is attributed to an account ownerWe collect data sets of tweets used by account attributed to\nTwo types of tweets: owner and staff: tweets signed by the same person\nTrained on a set of tweets from the same domain\nTwitter user attributed to a tweet\nTrain a neural model to predict owner's output\nTailored by a neural network\nIn this experiment, we split users into ten folds using U.S. politicians, which leaves us with 147 accounts. The average performance is 4.8% on ROC.Tweet owner or owner manager\nThey are the same but have lower activation rates\nTheir tweets are not attributed to them\nInstead, we use the retweets made by an account, which is as a more suitable metric in this setup\nTweet owner manager is the one who is the author\nSubstitute for a Neural Network\nUse the full dataset as training data\nEach tweet attributed to its account is different than the one attributed to it\nand two accounts are similar\nthe person with the highest activation rate\ntheir tweets are more likely to be the same\na few accounts are less popular",
            "result_pred": "Results\nSocial media handles are not co-workers\nNot all tweets are signed by the same person\nTwitter handles are self-attributable to a third partyResults on Twitter\nTweets by a different person are attributed to the same author\nSocial Media handles are often co-authors of tweets\npoliticians who self-reported their tweets using a signature\nTrain the language skills needed to distinguish between the two types of tweets\nResults on social media :\nsocial-media handles are trained on the same language skills as the owner and staff\nInstances with co-attributed tweets are more likely to outperform those that do not belong to a particular group of users\nAutomatic classification of tweets based on the author's signature",
            "conclusion_pred": "Conclusions\nWe introduced a novel application of NLP: predicting if tweets from an account are attributed to their owner or to staffers.\nPast research on predicting and studying Twitter account characteristics such as type or personal traits (e.g., gender, age) assumed that the same person is authoring all posts."
        },
        {
            "id": "85",
            "introduction_pred": "Motivation\nAttention over word alignment is a key feature of modern computer vision systems\nOne of the key innovations that led to this is the introduction of the attention mechanism in Statistical Translation (NMT)\nSaliency-driven Word Alignment Interpretation",
            "method_pred": "Saliency\nSaliency cosmetically similar to Transformer\nIn this paper we propose\nSmooth gradients:\nGradients: high-precision, non-linear gradients\n*Gradient: high precision gradients, low noise\n~Smooth Gradients: High Precision gradients and low noise gradients in the input\nWe propose that\nHigh Precision Gradients (Gradients)\nModels that are good at interpretable\nOur proposalAnalysis\nWhat about the neural nets?\nSalient cosmetologically similar to the Transformer?In this work we propose that we can find a way to interpret the input with a single input.\nThe neural nets are the same as Transformer but they are not the same!\nThey are not different!Our proposal\nWe can find the best way",
            "result_pred": "Evaluation\nNeed to do force decoding with NMT model.\nSaliency-driven Word Alignment Interpretation for NMT\nCan be used with other interpretations as well.Human Free Decoding Results\nHuman Free decoding: take the predicted word alignments from the target side of the corpus and measure AER against the human alignment.",
            "conclusion_pred": "Future Work\nSaliency and SmoothGrad\nWe propose a series of saliency methods that are model-agnostic, are able to be applied either offline or online, and do not require parameter updates or architectural change.\nConclusion\nNMT models do learn interpretable alignments."
        },
        {
            "id": "93",
            "introduction_pred": "Related Work\nEntity typing is related to selectional preference (Resnik, 1996,\nHe played the violin in the Carnegie Hall)\nCaucasian Facial Expressions of Desire (Volkova et al. 2014 ACL)Motivation\nSome previous work, such as \"Bill Robbed John\", \"Bill Lafferty\", \"J. Brooks\", \"Tom Brady\", and \"Mike Brady\" were all described by very fine-grained types\nBill robbed John.\n\"Bill\" is more likely a \"victim\" because he was \"robbed.\"",
            "method_pred": "Outline\nOverview of the task\nTask: find the \"best\" type ontologies\nObjective: find them all\nTraining them all:\n1. Find them all via crowdsourcing\n2. Train them\n3. Use them all as training examples\n4. Learn them all by hand\nLearn them all using crowdsourcing types\n5. Use the examples as inputData Analysis\nData Analysis\nContextualized Supervision\nTrain them all to find the best type\n\"Who was the President of the United States?\"\nWhat is the title of the President's speech?\nWhich country was the president\nWho is the Secretary of the Association for Computational Linguistics?Humanitarian Issues\nWhat can we do to help the President\nHow can we help the Vice PresidentChallenges of Supervision\nChallenge: Find the Best Type\nIn this task, find the most suitable typeTraining the Best Person\nTraining the Top Person\nLearning them all from a single input\nUsing the examples to train\nFind them all in a single training example\nConcatenate a dataset\nfind them all!\nUse them all for training\nCan we find the right type?",
            "result_pred": "Experiment Setup\n2K crowdsourced examples (KB and Wikipedia definitions)\n20M entity linking data (20M head words)Results Datasets\nExperiment (Datasets):\n50 examples from the development set\nAverage mean reciprocal rank (MRR) for each annotated label\nFint-grained types: \"man\" , \"person\"",
            "conclusion_pred": "Conclusion\nWe introduce a new entity typing task.\nWe are able to use distant supervision to improve performance on our new dataset. . .\nNew types of distant supervision boost performance on an existing fine-grained entity typing benchmark."
        },
        {
            "id": "104",
            "introduction_pred": "Introduction\nSemantic parsing aims to map natural language text to a formal representation (e.g., logical even if this is just a random guess).\nGoal: to mitigate these issues by identifying which parts of input contribute to uncertain predictions.Motivation\nWe generally are not good at distinguishing fact-checking\nIt is important to know whether model predictions are correct or not.\nTo draw inferences about model behavior and its causes\nComplexity of Modeling and Interpretation\nMany errors and inconsistencies\nBeing small and noisy makes model output more difficult to learn\nExisting models use feature engineering - over simplistic; or recently deep neural networks - learn based on posterior probability",
            "method_pred": "Neural Tasks\nDeep Learning: Semantic Parsing\nConcatenate Neural Tasks\nInput: Neural Task\nNeural TaskWhat is uncertainty\nWhat is it?\nWe need to know\nThe answer to the question\nIs it:\nWhat do we know?Semantic Parsers\nWe know the answer\nBut what is uncertainty\nIn this paper we need to find out\n\"What is the meaning of uncertainty?\"\nCan we find out by training neural task\nWhy is it so hard\nBecause we don't know the answers\nOur model is not fool proof\nIt's not foolproof\nInstead it's fool proof!\nGiven a set of parameters\nAn example of how we can fool ourselves\n1. We can use neural model as input\n2. We need to be foolproof.\n3. We don't have the data yet\n4. If we had the data, we would have known\n5. We would have learned\n6. But we didn't have data\n7. Instead we use neural network\n8. Neural model is fool proof,\n9. If the model isn't fool proof then\nit will fool us\n10. If it is foolproof, it will fool our model\n11. If\nthe answer to our questionDeep Learning Neural Task\nInference: Neural Model\nRecurrent training: neural modelDataset Overview\nData: Overview\nModel trained on neural network, neural model trained on it\nTraining neural model on neural networks\nIntuition: Neural model trained upon neural network and neural model learn\nTask: Recurrent training\nFigure: Deep Learning neural model with\nlinear noise, low input noise, high input noise\nPrediction: False\nParsing neural network with low noise noiseData: Insight\nInput : Neural model with decoder\nsensor neural network (train neural network)\nOutput: Neural models trained on the neural network.Inputs are noisy\nTrain neural model in neural network environment\nEach neuron is represented by its parent\neach neuron has its own expression\nand each neuron has their own representation\nSample a neural model from neural network",
            "result_pred": "Experiments Datasets\nIFTTT and DJANGO datasets\nF1 score is the most important metric\nIndividual confidence metrics are significant with p < 0.05 according to bootstrap hypothesis testing (Efron and Tibshirani, 1994)\nTable 1: Accuracy of prediction accuracy on the IFTTT/DJANGO dataset.\nThe correlation between F1 and individual confidence metrics on the following metrics:\nConfidence Estimation F1 Score is the best measure of accuracy",
            "conclusion_pred": "Conclusions\nWe presented a confidence estimation model and an uncertainty interpretation method for neural semantic parsing.\nExperimental results show that our method achieves better performance than competitive baselines on two datasets."
        },
        {
            "id": "127",
            "introduction_pred": "Introduction\nHuman-computer conversation is a critical and challenging task in\nIt aims to help people complete tasks such as buying tickets, shopping, etc.\nBy attention mechanism, we can control the specificity of responses depending on a variety of factors.Motivation\nCaucasian Facial Expressions of Emotion and Joy\n[Liang et al. (2016] proposed to use a Seq2Seq framework to generate responses based on general context and context",
            "method_pred": "Outline\nConversational vs. human conversations\n1. Overview\n2. Topic-based responses\n3. Response generation\n4. Given a random sample of utterances\nWe need a single model\nUnsupervised generationSeq\nNormalization of responsesConversation vs. Human Conversations\nSeq: OverviewRecognition vs. Man Conversations\nConvolution vs. Person Conversations deep neural network\nNeural network to generate a response\nHuman conversations deep neural networks to generate responses deep neural nets to generate an additional responseWe need to train a multinomial neural network to learn a novel decoder\nOur target is human conversation\nQuestion: How do we generate a new decoder?\nHow do we get a better decoder for human conversation?Our approach\nWe can find responses that are specific to our target\nIn this way we can use\nunprecedented generation of an utterance\nRecurrent generation of a response from a human context\nNatural language understanding\nSemantic understanding of the responses deep Neural network to find the most specific decoder we need\nThe best way to train is to learn the responses in a natural language understanding deep neural netNormalization vs. Semantic understanding\nOur approach\nNormalize the activation of an IWF\nGeneration of an IIWFNatural Language Understanding Deep Neural Networks Deep neural network deep neural wiring\nSyntactic understanding of utterance we need to learn\nGeneral knowledge of a given utterance is the property of its response",
            "result_pred": "Evaluation Results\nSC-Seq2Seq\n(1) Given a response, we generate a response\n2) The human evaluates the responses\nThe human evaluate the responses according to the four criteria:\nScoresq: the response generated by the human\n*: the human evaluates each response according to four criteriaBaseline Comparison\n(2) Standardized responses are generated with the same parameters as the human evaluations\nAverage n-gram precision on a baseline set is statistically significant",
            "conclusion_pred": "Conclusion\nWe propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity\nWe introduce an explicit specificity control variable into the Seq2Seq model, which interacts with the usage representation of words to generate responses at different specificity levels.\nEmpirical results showed that our model can generate either general or specific responses, and significantly outperform state-of-theart generation methods."
        },
        {
            "id": "135",
            "introduction_pred": "Introduction\nMetaphor Processing:\nMikol,\nUnsupervised Machine Translation\nZhu, Tsvetkov et al. (2014) proposed a novel approach: to identify literal metaphors and non-literal ones, and to translate them.\nMotivation\nWe generally are not good at distinguishing metaphors\nIt is crucial to have a model that recognises their literal meaning and when it is unlabeled they are not understandable in the topic\nWe propose an alternative method: to parse and parse words from word embeddings and translate\nShe sat down and ate lunch.",
            "method_pred": "The metaphoric meaning of a sentence\nThe literal meaning of an inflectional sentenceThe context of a word\nthe context of its utterance\n\"She\" is a synonym\nsynonyms are the most common in the context\nIn this context we find the best fit word for the meaning of the wordOur first hypothesis\nWe find the metaphoric word for a sentence to be close to the target\nOur second hypothesis\nSkip-gram learning the metaphorical meaning of sentences\nWe found the best-fit word for each sentence in contextMethod of our first hypothesisWhat is the context of which we train?\nContext: the utterance of which is the literal meaning\nTraining the model to find the correct context",
            "result_pred": "Experiments\nMetaphoric expressions are widespread in natural language, posing a significant challenge for various natural language processing tasks such as Machine Translation.\nTypical English-to-Chinese (MASH) and Japanese-to -English (Mash)\nTwo popular machine translation systems for English to Chinese\nMASH: MASH and MASH+\nLanguages: Japanese, Korean,\nEnglish to Chinese,Results on Machine Translation\nResults on machine translation\nWe used machine learning to analyse the identified metaphors, paraphrasing them into their literal counterparts, so that they can be better translated by machines.",
            "conclusion_pred": "Conclusion\nWe proposed a framework that identifies and interprets metaphors at word-level with an unsupervised learning approach.\nOur model outperforms strong baselines in both sentence and phrase evaluations."
        },
        {
            "id": "151",
            "introduction_pred": "Introduction\nAMRs have generated considerable research activity (Banarescu et al. (2013)\nHowever, dependency parsing still faces the challenge of distinguishing between tree representations and tree representations.Motivation\nWe generally are not good at distinguishing dependencies\nTree representations are hard to parse and cannot be represented as sequences.\nBecause they are graphs and not strings, they can't be computed as sequences\nWe combine neural supertags for identifying the individual words and decoding\nAMR parser:\nKB-based decoder:",
            "method_pred": "What is AMR\nWhat is it?\nModel: AMR parser\nModified rootgraph\nMODEL tree:\nInput: a tree with a root\nOutput: a graph with a leaf\nPropose an AMR parsing task\nAdd a \"tree\"-graph structure to the graph\nTrain the parser with the input node\nSubgraphs: the root of the tree\nExample: we want the graph to be the same as before\nBut we don't know what the root is\nInstead, we just want it to look like thisModel: parse the graph with an input\nApproach: parse it with an output graphModification of the AMR parsers\nModify the graph using the input graphs\nUse the input graph as the rootExample: We want the graphs to look the same\nHowever, we can't find the root so we use the leafMODEL parsing task\nMODERATION OF THE AMRA parsers\nWe want graphs that look like these\nIn this paper we define the AM algebra as G\nIt can be written as anything\nand it can be done with any of the following operations\ngit clone https://github.com/MartinezAlchemy/AMR_ParsingThe AM algebra is a parser\nit can be structured as anything from a tree-graph to a graph-graph.\nThe classifier is the classifier\nis the root node of the graph.Subgraph parsing\nsubgraphs are the same, but they have different edges\nThey have different ways of representing graphssubgraph parse the AMra parsers in a new direction\nsub-graphs can be found\nthe classifier can be learned as an operation\nadd a \"add a\" node to the AMRA parse the tree with\nmodify the rootgraph with an op(s)\nparameter is the \"root\"\nclassifier is also the \"top\" node of an AMra\nParameter is not the \"bottom\" node but the \"right\" node is.",
            "result_pred": "Evaluation\nSmatch scores  of our models, compared to a selection of previously published results.\nFoland and Martin (2017) : F-Score for Smatch + Smatch+\nK&G + K&G edge scoring model\n[F-Score: Smatch,\nRotations, dilations, reflections, reflections.Results\nAccuracy and SVM accuracy\nBoth edges and flat-lined trees outperform the original JAMR-style baseline\nThe best edges seem to work well with the original model.",
            "conclusion_pred": "Conclusion\nWe presented an AMR parser which applies methods from supertagging and dependency parsing to map a string into a well-typed AMR.\nThe AM term represents the compositional semantic structure of the AMR explicitly, allowing us to use standard treebased parsing techniques."
        },
        {
            "id": "179",
            "introduction_pred": "Introduction\nEvent extraction paradigms have typically been successful:\nManual annotation: assign an event type to each argument\nEvent mention: assign a non-inferior event type\nCause-effect joke joke : laughter\n(Image source: Wikipedia) Event mention structures:",
            "method_pred": "What is ACE\nWhat is it?\nTrain a neural network to represent the event\nConvolutional neural network\nGenerate a new classifier\nTrained on a new ontologyWhat can we learn about ACE<et>What are ACEs\nTraining on an Event\nWe can find out about ACEs\nPre-trained on a set of\nstructured representations\nModel trained on an event structure\nOur approach: learn the structure of an event representation\nEach sentence is represented by an AMR representationThe ACEs are effective\nThey are the best among all possible models\nWhat can ACEs learn about?Our Approach ACEs Are Effective\nOur Approach: ACE's are the Best among All Possible Models\nAction: Train a neural model to represent each sentence\nTheir type is the type of event represented by the AMR representationsThis is the first time we have used the ACEs as training methods!\nIn this way we can compose new representations for each sentence as we want\nFor each sentence:\n\"This is a great time to be a member of the ACE family.\"\nThis is an ACE-like structureTraining on ACEs is effective\nTrain on ACE's with a pre-trained\nsequence of events\n1. Train on a classifier with\n3. Features\nClassifier is the classifier trained on\n4. Features are learned\n5. Training is the best of all possible combinations\nSummary: ACE is the Best of All Possible Styles\nIt is an Event that is most effectiveThank you for your attention\nThank you!",
            "result_pred": "Experimental Paradigms\nZeroshot (without manual annotations)\nWe take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology\nOur framework incorporates all the features derived from manual annotations.\nFor experiments, we ran on 23 unseen event types:\n3,000 annotated sentences annotated with 500 event mentions\nSemantic space: a shared semantic space\n\"if the event type is semantically closest to its type, then it belongs to the classifier.\"",
            "conclusion_pred": "Conclusions\nWe propose a transferable neural architecture, which leverages existing humanconstructed event schemas and manual annotations for a small set of seen types.\nThis framework can be applied to any unseen event type."
        },
        {
            "id": "183",
            "introduction_pred": "Motivation\nHuman affective computing is now multimodal task\nIt is hard to extract informative features from text and audio\nTo decode human affect, it is critical to consider both the emotional content and how it is uttered.\nExtract informative textual and acoustic descriptors from textual data\nSearch for: \"hell\" for anger but indicating happy on \"good\"\nFuse informative textual representations from audio and extractive visual features from\nBeauty is in the eye of the beer holder.",
            "method_pred": "Outline\nMotivation: find the best way to train a multimodal neural network\nAttention: find out the most efficient way\nReinforcement learning: find an efficient way to do this\nInput: high-level input\nOutput: outputAttention Training\n1. Initialize the multimodals\n2. Select the best method\n3. Select an optimal model\n4. Use the best available architecture\n5. Use it as training\n6. For each word\nUse it as an example\n7. Select a best modelOur approach\nTo train a multi-level neural network:\nWe will use it as a baseline\nLearn the best possible architectureThank you for your attention\nThank you!\nFor your attention\nYou did it for me\nThanks! :)\nI love you too!Fusion\nFusion\nSentiment: I love you, but I have no idea how to use it\nThank-you for listening!",
            "result_pred": "Experiments\nExperiments conducted on text and audio data\nSequence of human affect and subjective information from multi-datasets is still challenging because: (i) it is hard to extract informative features to represent human affects from heterogeneous inputs; (ii) current fusion strategies only fuse different modalities at abstract levels, ignoring time-dependent interactions between modalitiesResults\nResults on MS Word Datasets\nWe see that our model outperforms state-of-the-art approaches on published datasets.",
            "conclusion_pred": "Conclusion\nThe proposed architecture uses both the audio and text data to analyze the sentiments and emotions.\nWe introduced a hierarchical multimodal architecture with attention and word-level fusion to classify utterance-level sentiment and emotion."
        },
        {
            "id": "232",
            "introduction_pred": "Introduction\nMost existing methods to extractive summarization rely on extraction from source text\nTo produce fluent, generic document summaries\nExtractive summarization using human abstracts\nCao et al. (2018)\nNewLink and NewLink (2015) use truncated words from the source documents to generate sentences\nCloze-style comprehension questions\nWe compare two methods to convert human abstract data:",
            "method_pred": "Outline\nIntroduction of extractive summarization\nQuestion-Focused E Reward (Y |X)\nHuman abstracts: extractive summaries\nWe seek to optimize a space between extractive word embeddings and QA pairs per sentence\nIn this paper we obtain K question-answer summaries from each human abstracts and extract a QA pair per sentence.Question Focused Reward\nWe reward a summary if it can be used as a document\nThis work is a major achievement\nExercise: extract an extractive summary from a source\nReinforcement Learning: extract a summary\nThe reward function R(Y ) is used to optimize the policy P (Y )\nIt is possible to generate multiple human summaries per sentence, but human abstract is more informative than others.\nIf there are fewer than 10 entities, the system should be able to train the full modelOur Approach\nQuestion: Which entities reside in the summary?\nThey are the source of the summary document, and useful for answering key questions\nBut they cannot be easily aligned\nOur goal is to maximize the extractive QA reward\nWhich entities are present?Thank you for your valuable suggestions\nThank your for your generous suggestions\n\"Shipping the vaccine today is major achievement!\"\nTraditionally, the vaccine was created by Merck, but the National Institutes of Health\nSince the vaccine is a multivax, it is easy to constrain the training of the full system\nGiven a summary document (e-4):\nSample the summary from the source document and convert it to a single-direction LSTM\nEach sentence is a vector of words that encode the summary word\neach sentence is represented by a Q(Y , x ) vector\nand each word encode the Q(y , x) vectorThis work\nExplicitly explore the QA-focused E Reward\nFor each human sentence:\nCan we extract an abstract from a sentenceChallenge\nIf the first doses of the Ebola vaccine are on a commercial flight to West Africa,\nwill arrive on a Friday, according to a spokesperson from GlaxoSmithKline\nWhat about the rest of the vaccine?",
            "result_pred": "QA System Results\nQA system: Results\n(Chen et al., 2016)\nHuman annotators (Hermann et al. '16) are encouraged to preserve salient information for future reference documents.\nSequence-based summaries (K=1 to 5) are an efficient way to generate QA pairs.Results Sample\nResults: Sample\n10.7% and 21.9% qA pairs, respectively.",
            "conclusion_pred": "Conclusion\nWe explore a new training paradigm for extractive summarization.\nWe use reinforcement learning to exploit the space of extractive summaries and promote summaries that are concise, fluent, and adequate for answering questions."
        },
        {
            "id": "241",
            "introduction_pred": "Introduction\nKeyphrase extraction from textual content is successful:\n extract keyphrases based on textual content\nCan automated keyphrase extraction be successful?\nEvaluate the importance of each word in the document\nSelect the subset of words that most clearly expresses the meaning of the keyphrase\nKeyphrase Extraction from Translation: Liu et al. (2004) extracted n-grams of text and extracted it\nTable 1: Syntax of keyphrase extraction from translation:",
            "method_pred": "Key of Encoder\nDecoder\n1. Select a sequence of words\n2. Select the sequence\n3. Select words from the input\n4. Select phrases from the output\n5. Recruit the sequences\n6. Select sentences from the data\n7. Select sequences from the text\n8. Select from the source sequencesThe first sequence of sequences\nThe second sequence of sequenceKeyphrase extraction task\nKey-of-sequence extraction task\nRNN: extract the first key from source sequences from sentences\nEncoder: Select sequences of words from inputMethod of Recurrent Neural Networks\nMethod: Recurrent neural networks\nConcatenation of key-phrases\nRecap of sequence of phrases\n\"video\" and \"text\" are the most frequent in the dataset\nSequence of sequences from input is the same\nWe need to train a deep neural network\nThe \"video\" is the best way\nIt is the most-supervised RNN\nPreferably supervised\nCan we use it to train the deep neural networks?\nIn this task, we need two ways\nfold the training time\nCreate a new key sequence from source sequence",
            "result_pred": "Experiment Settings\nBaselines: macro-averaged precision (F1) and F-measure (F2)\nResults and baselines\nSequential Matching (Porter Stemmer)Results and Analysis\nThree keyphrase generation baselines: precision, recall and F1.\nBaseline for measuring the accuracy of the algorithm\nKeyphrase Generation Baseline\nMorphology: Syntax-based, Hierarchical, Semantic and Logistic\nF1 is computed by multiplying precision by the number of correctly-predicted keyphrases over the total number of data records.",
            "conclusion_pred": "Conclusion\nWe proposed a generative model for predicting keyphrases that appear in scientific publications and news articles.\nIt achieves a significant performance boost on extracting topically relevant keyphemases that are absent in the text."
        },
        {
            "id": "247",
            "introduction_pred": "Introduction\nNigeria's most populous nation\nThis is a novel architecture that allows to copy text from source\nOur best model is able to thwart corruption\nRapidly give attention, to curbing violence in the northeast\nWe have shown great success with our hybrid system\nNAACL\n(Image source: wikileaks.org)\nAttendance: non-monetary",
            "method_pred": "What is the ROUGE\nWhat is it?\nWe find it necessary to obtain coverage for the full text\nIn this example:\nThe first sentences of the summary\nTranslation summary is the output of the task\nRecap of the sentence\nWhat can we do to increase coverage?What are the other ways to generate coverage for a full text\nWHAT ARE THE OTHER WAY TO RAISE RECAP?",
            "result_pred": "Experiments\nAccuracy of pointer-generator model\n*> Accuracy on source text\n**> Precision to generate novel summariesTraining and test\nTest and test set:\n50k word pairs\nPre-trained, pre-trained\n100k pairs of tokens\nCoverage: coverage of the modelExperiment training and test time\nExperiment: training & test time\ncoverage + coverage",
            "conclusion_pred": "Conclusion\nWe presented a hybrid pointer-generator model that reduces errors and improves accuracy\nWe applied our model to the CNN summarization task.\nOur model outperforms the previous model."
        },
        {
            "id": "269",
            "introduction_pred": "Introduction\nIntroduce syntactic LMs into phrase-based translation\nImprove translation quality\nTranslating source and target languages using tree-based decoding\nUsing hierarchical phrase-structure and syntactic semantics:\nIntegration with Tree-based Translation:Motivation\nExtension 1: Syntactic grammar & syntactic structure",
            "method_pred": "Phrasebased Translation Hypotheses\nPhrase-based Translate Model\nPropose a new Syntactic Language\nDecoder: extract phrase-based translation\nParsing as left-corner decoding\nProvide a syntactic language representation\nThe Syntactic language is represented by a set of syntactic trees\nEach syntactic tree is represented as a lattice\neach syntactic element is an internal representation of the syntactic constituents\nThis is the Syntactic Lattice tree\nthe number of possible translations\nand one or more syntactic constituent\na is the number of translated words\nb is number of translation possibilities\nc is the translation possibility\nd is the decoder's output\ne is the output of the translation hypothesis\nth is the set of translation options\nt is the syntactical tree representationPropose an incremental translationExample Example\nExample of an incremental parser running in a linear time\ns is the length of input\nst is the maximum possible translation possible\nh is the longest possible translation available\ni is the target of the incremental parser\nn is the total number of translations availableProbabilistic translation hypothesis analysis\nprobabilistically translation hypothesis hypothesis analysis\np(e) is the estimate of the total probability mass over all possible translation possibilities,\nwhere p(e|t) is a number of words in the possible translation space\nPhrases are translated by translation",
            "result_pred": "Experiments\nExperiments on the 5-gram LM trained for the GALE Arabic-English task using the English Gigaword corpus.\nMoses phrase-based translation model:\nAverage per-word perplexity: ppl\nTest set: WSJ\nHuck et al., 2017\nWord-to-word translation: Urdu-English\nCombined corpus: 1,20,000 sentences\nEnglish GALE (Arabic-English)\n1,200 sentences per sentence\n3-gram LMs trained with Moses\nN-gram Language Model: N-gram + n-gram",
            "conclusion_pred": "Conclusion\nWe presented empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.\nWe provided formal definition of incremental syntactic language models."
        },
        {
            "id": "277",
            "introduction_pred": "Introduction\nCross-lingual plagiarism is a very significant problem nowadays, specifically in higher education institutions.\nIn monolingual context, this problem is rather well treated by several recent researches (Potthast et al., 2014).\nNevertheless, the expansion of the Internet, which facilitates access to documents throughout the world and to increasingly efficient (freely available) machine translation tools, helps to spread cross-language plagiarism.Background\nThere are several methods for detecting this kind of plagiarism, including:\nDistributed Representations of Words\nDistribution of Word Embeddings in Cross-Language Text Detection",
            "method_pred": "Cross-Language Similarity Detecting Word Embedding\nCross-language textual similarity detection\nPropose a more straightforward way to detect plagiarism\nConvolutional Similarity (CL-WES)\nWe propose to estimate the similarity of two textual units in a document embeddings\nIn this paper we propose to use\nCL based on concepts derived from Wikipedia\nComputational Similarity [Barrard et al. 2014]\nOur approach is to estimate a syntax two textual unit by a vector of vectors.\nThis paper: propose to build a lexical similarity detection tree\nRecognate if the syntactic unit is plagiarism or not\nAssign a weight to each sub-word of the root, sentence level and\nLet U express the textual unit\nThe weight of each subword is the lexical unitOur approach: propose a more direct comparison between two sentences\nAdd two lexical units to the root of the word\nCombine the two weightsCrossLanguage textual Similarity Detection\nCross -Language textual similarity Detection\nLearn a better way to find out if plagiarism is occurring\nUse the same lexical resource as the input\nImprove the similarity detection by using the following methods:\n1. Combining the best of the best\n2. Improve the textual WES\n3. Use the same methods as in CL-C3G\n4. Combination of the above methods\n5% of the input is lexical\nUsing the same method as in the CL-CTS\nClassification of Unigram dictionary as a parallel corpus\nLanguages express the meaning of the textual units\nLanguage similarity detection is the aim of cross-language similarity detection: estimate a syntactic similarity\nWord EmbeddingsThe weights with the help of Similarity\nSemantic Similarity detection: Estimate the similarity between textual units by a matrix of vectors\nFind the best possible weight\nEstimate the word embedding weights\nIf two words express the same meaning\nIdentify if two words are plagiarism/not\nPrediction: find the best-formed weightMethod of Cross-Language Recurrent Similarity Analysis\nMethod ofCross-Lec Recurrent Comparative Similarity - Analysis\nIt consists of the following:Propose an efficient way to",
            "result_pred": "Evaluation Protocol\nCl-CTS-WE obtains overall performance gain of +3.83% on chunks and +7.01% on sentences\nDecision Tree F1 Score\nThe F1 score is the harmonic mean of precision and recall.\nFor each iteration, 10 folds are carried out by changing the M selected units randomly selected from S. The same unit may be selected several timesResults and Discussion\nExperiments conducted on EN-FR sub-corpus\nTwo methods are proposed: one for chunk level and one for sentence level\nCombined methods proposed on each sub-category\nEach method is applied on a particular sub-cabinet\n10 folds is carried out for each iteration\nResult is the proportion of relevant matches (similar cross-lingual units) retrieved among all the matches retrieved\nClinical evaluation methodology for crosslingual similarity detection\nResults and discussion on cross-language textual similarity detection",
            "conclusion_pred": "Conclusion\nWe have augmented several baseline approaches using word embeddings.\nThe most promising approach is a cosine similarity on syntactically weighted distributed representation of sentence."
        },
        {
            "id": "316",
            "introduction_pred": "Introduction\nLambda Dependency-Based Semantics:\nTranslating natural language data into executable semantic parsing\nExtracting such a model from non-monetary sources is not the only way to improve performance.\nTo deal with rare or unknown words, parsers must have a much larger vocabulary size",
            "method_pred": "Transfer Learning\nTransition into Overnight data sets\n1. Utilize NLmap as input tree\n2. Extract sentences from NLmap\n3. Use them as input words\n4. Use the NLmap embedding as input word\n5. Use it as output wordTransfer Transition\nThe neural parser is based on attention\nIt contains more than two thousand non-terminal word embedding\nit is possible to transfer across data sets:\nNLmap: a tree of NLmaps\nLSTM: a neural parser\nInput: a vector of wordsTransition to Overnight Data Set\ntransition into NLmap: A tree of sentences\nThe input is a set of words in NLmap,\nthe output is a stack of sentences.\nWe want to learn the full output treeDelexicalization\nTransfer to NLmap\nConcatenate the full input tree with\nParsing the full NLmap into a stack\nOutput tree is the same size as the input tree, but it is not the same complexity\nBut it is possible\nto transfer across Data sets: a) The input tree contains all the words in SLU\nand b) are the same\n\"new\" domains\nDyer, Duong et al. 2015\nTransfer between NLmap and a\nLink between NLmaps and A\nUse it as input sentence\nOr, use it as an example\n(or, use as a model\n)\nFor example, use the NLmaps embedding to extract sentences from input words.Data\nUnits: a and b\nLLs: A and b, respectively\nEach sentence in the sentence is the output of NLmap or\nThis sentence is an example of what it is like to learn a new semantic representation\nIn this paper, we experiment\nwith\nsamples from NLmaps, a and B\nls: a, B, C, D\na and b are examples of what can be learned\ni. For example, find the highest-resource translation\nii. for example, the word embeddings\niii. for instance, find a translation of the input word in the\niv. to get the best of\nor, we will use\n",
            "result_pred": "Experiments\nSingle-task semantic parsing (SVM)\nWe study the correlation between delexicalization and performance.\nThe correlation is asymptotically as well as temporally as the similarity of the two datasets.",
            "conclusion_pred": "Conclusions\nWe framed SLU as an executable semantic parsing task, which addresses a limitation of current commercial SLU systems.\nBy applying our framework to different data sets, we demonstrate that the framework is effective for Q&A and SLU\nWe explored a typical scenario where it is necessary to learn a semantic parser for a new domain with little data."
        },
        {
            "id": "317",
            "introduction_pred": "Introduction\nA news-aggregator is a website or mobile application that constantly select a large amount of web content, e.g., much shorter than the original headlines.\nstory or statement whose content is most relevant to the given task.Related Work\nNews headlines can be generated quickly and out of vast amounts of information, benefiting the reading and the editing community.",
            "method_pred": "Outline\nModels trained on short titles\n1. Given a set of encoder leads\n2. Generate a new encoder for each title\n3. Use the encoder as input\n4. Use it as a tool\n5. Use as input a few encoder candidates\n6. If the model is good\nUse as input an encoder\n7. If it is good, use it as input for news\n8. If we are good at it\nWe can use as input two encoder candidate weights\nThe model is useful\nIt can generate short titles from encoder to lead\nHowever, it is not practical to use multiple encoder models\nInstead, use them as input.\nIn this way, we can use multiple modelsModels learned from previous studies\nOur model can perform better\nIf the model was goodOur model is not good\nIf it is bad\nit is not fool proof\nBut it is a good model\nAs long as it has the correct output\nAnd it is effective\nEven if the input is wrong\nCan we use it for multiple purposes?",
            "result_pred": "Evaluation\nWe conducted two crowdsourcing tasks to separately measure readability and usefulness.\nThe readability task asked ten workers how readable each short title was on a four-point scale (higher is better)",
            "conclusion_pred": "Conclusion\nWe addressed short-title generation from news articles for a news aggregator to support the editorial process.\nWe proposed an encoder-decoder model with multiple encoders for separately encoding multiple information sources, i.e., news headlines and leads."
        },
        {
            "id": "331",
            "introduction_pred": "Introduction\nSentiment analysis: Topicivity and coherence\nNegative Orientation: Relation to Adversarial Learning\nEVIDENCE: Relations between syntactic units\nHierarchical Structure of a corpus",
            "method_pred": "What is the relationship between sentiment and astronomy?\nDomain: climatology, politics, astronomy, food\nLanguage: astronomy, culture, music\nDiscourse: Propagation, Interpretation\nProposed by inter-annotator\nSemantic orientation of the satellite\nSubjective: relations between literature and astronomy\nRelational: relations with astronomyDiscourse analysis\nWhat are the relations between astronomy and space?What are relations between ground and sky?",
            "result_pred": "Experiments\nBasque Opinion Corpus (Basque)\n1. Study sentiment words\n2. Study coherence relations\n3. Match sentiment words by association\n4. Match word pairs\n5. Match whole sentence pairsResults on Basque News Corpus annotator\nBASQA: News Corporation (Chen et al., 2017)",
            "conclusion_pred": "Conclusion\nWe have annotated a part of the Basque Opinion Corpus using Rhetorical Structure Theory (RST)\nWe also obtained more fine-grained results regarding subjectivity and attachment.\nThe manual evaluation of the corpus shows that the inter-annotator agreement of the type of rhetorical relations is 39.81%."
        },
        {
            "id": "332",
            "introduction_pred": "Background SpeechRaterr\nContext: Need to evaluate many dimensions of spoken language in the task of automated scoring of non-native spontaneous speech\nUntil recently: Develop features related to fluency, pronunciation, prosody, vocabulary, grammar, content\nStill needed: Explore features measuring discourse coherence, progression of ideas",
            "method_pred": "Discourse Treebank\nDiscourse treebank\nSpeakers with high RST proficiency\nHuman vs. non-native language\nWe have an automated discourse scoring system\ndiscourse tree:\nAutomatic and human-annotated responses\nIn this work, we have a deep discourse tree\na single annotated set of relations\ntwo expert annotators\nan automatic RST parser\nand two spoken language models\nAnnotated discourse tree of discourse\nUn-structured discourse structure\nnon-native discourse structure and syntactic relationsAutomatic discourse tree\nWe had a deep RST tree: a neural network of relations that are naturally occurring within the context of spoken language.\nThe neural network is naturally occurring\nIt is possible to annotate the neural network from spoken languageDiscrepat discourse structure\ndiscrepat: discourse structure of discourse within a corpus of discourse.Non-native speech\nNot fluent in spoken language, non-conversation, or discourse structure exist only in the discourse structure (e.g. Spoken language)\nThere are many ways to express discourse structure in the corpus\nFor example, we can annotate discourse structure from human language, language types, and discourse structure within words\nAnd we can do this through human language",
            "result_pred": "Experimental Setup\n12,194 speakers (73,164 responses) who responded to the six test questions\nHuman scoring algorithms\n16,194 (30,000 responses)\nSpelling and grammar:\nAutomatic speech recognition output (without punctuations and capitalization)Results and Discussion\nAccuracy of 95.7% on an independent evaluation set\nEvaluation of the discourse structure of non-native spontaneous speech\nRST-based discourse features can reach an accuracy of 65.4%",
            "conclusion_pred": "Conclusion\nWe first obtained an annotated corpus of 1440 spoken responses produced by non-native speakers of English in the context of an English speaking proficiency assessment using Rhetorical Structure Theory and then trained automatic discourse parsers based on the human annotations."
        },
        {
            "id": "337",
            "introduction_pred": "Introduction\nLanguage identification is a number of applications in natural language processing\nOne application of language identification is to account for different semantic transliteration rules between languages when the target language is Chinese.\nHuang et al. (2005) use character-level n-gram language models to improve the accuracy of letter-to-phoneme conversion.",
            "method_pred": "Language identification\nKonstantopoulos (2007) language identification\nLanguage identification with\nTransliteration of personal languages into textLanguage Identification with Transliteration\nLanguage Identification using Transliterations\nJapanese, Chinese, English, Japanese\nKorean, Russian, Korean, Japanese, Korean\nRussian, Chinese\nEnglish, Japanese and KoreanProblem of language identification with translations\nProblem of linguistic identification with translation\nTrain language models using language representations\nLearn to weigh the translations\nUse translations as training examples\nTrained translations as input\nCan we train language models with translations?\nHow long should we train them?Goal\nIdentify the language of a player\nGoal: score the player's n-grams correctly\nThe player has an official name\nHe has two teammates\nChinese teammates, one of whom is a professional\nIn this paper, Konstantopoulos performs the best.\nHowever, he does not perform the best\nInstead, he uses transliteration as his advice\ntrain language models on translationslanguage identification with Transgrams\nlanguage ID with TransGeneration\nConcept language model: linear kernel\nlanguage model: min-gram\nLanguages: Japanese, English\nSVMs: Linear Kernel\nWe discarded those that fell into both categories, making the tagged data much more dissimilar\nData other hand, Li-Hindi corpus of names\nTransfermarkt corpus of languages\nkrueng scripts\nBosnia and Herzegovina scriptsLanguages identification with transliterations\nLinguiteration with Transgenerational\nN-gram frequency profile, linear kernel, and Sigmoid kernels\nLin et al. [2009] language identification using translationData Other\nChinese, English and Devanagari scripts",
            "result_pred": "Intrinsic evaluation\nWe studied the Transfermarkt corpus of first names and surnames.\nWe also studied the Chinese-English-Japanese corpus (CEJ) corpus (Jiampojamarn et al., 2009).\n1. We trained the language identification model on 1000 tagged names; this resulted in a top-1 accuracy of 46.0%, as compared to 44.0% achieved by a single transliteration model trained over 1000 labelled names.",
            "conclusion_pred": "Conclusion\nWe propose a novel approach to the task of language identification of names.\nWe have shown that applying SVMs with n-gram counts as features outperforms the predominant approach based on language models.We also tested language identification in one of its potential applications."
        },
        {
            "id": "340",
            "introduction_pred": "Introduction\nAdpositional meaning is crucial to piecing together the interpretation of a sentence\nExamples of such annotations include:\n1. English: grammatical template; 2. Semantic semantics of prepositions and postpositions\n2. Relation between prepositional and lexicographic meanings\nThis work: a broad-coverage annotation scheme for English that disentangles the two elements of meaning, or construal, over and above the scenario relation that its object participates in",
            "method_pred": "What is preposition\nWhat is the preposition of a noun?\nThe preposition is the product of a speaker's and/or a person's preposition\nWe use the adposition as a predicate\nExamine the relations between preposition and adverb\nAssume that the prepositions are related\nThey are the same\nBut what is the relationship between the adpositions?Preposition is a complex thing\nIt has many sub-positions\nMany sub-propositions:\nSome sub-prepositions (e.g., kaa, kaa)\nWhat about the semantic relation between adposition and kaa\nWhich sub-subsets reflects the semantic role of Bipasha\nPreposition and preposition relations\nThis is not the first time we have seen this\nIn the past we have observed it\nUsually in the same sentence\nHowever, we do not see it as a linear problem\nInstead, we use the full preposition as an example\nand some sub-types as well\nbut we use them as the training groundWhat about preposition representations\nMain idea of preposition representation\nPURPOSE: construal representations\nwhich sub-sets are the functions that describe construals\nFor example, the \"natural\" way of representing construans\nfor example, \"the natural way\"\nthe \"natural way\" is represented\nby the \"unnatural\" ways of representing them\nsub-sets as the functions of a prepositional representationPulveristic Hypothesis of Prepositions\nPULVERISTS OF PREPPOSITION\nmain idea of postposition representation is the same as in the natural way\nhere is a way to represent construants\nwhere they are situated in the world\nwith respect to adposition representationsExamining Semantic Structure of PrepositionSubspace of Semantics\nsubspace of semantic structure\nSub-sub-sentences\nunder the adverb/subverb functions\nover all the verbs/substituents\nor else, they are part of the same sub-sentence\nsimilar to in the example of kaa at kaa is a relation to kaa but\noccurrences in the scene\n",
            "result_pred": "Experiments\nExperiments conducted on Domaingeneral language (English:\n4,250 preposition tokens in 55,000 word pairs)\nSemantic-Evaluation of preposition syntactic function\nThe role/relation of a preposition is directly related to its lexical function.\n\"The role of the adposition is essentially the same as that of the preposition...\"Experimental Paradigms\nEXPERIMENTS ON DATASET\n(a) Individual tokens annotated at the token level",
            "conclusion_pred": "Conclusion\nWe have considered the semantics of adpositions and case markers in English and a few other languages with the goal of revising a broad-coverage annotation scheme used in previous work.\nWe pointed out situations where a single supersense did not fully characterize the interaction between the adposition and the scene elaborated by the PP."
        },
        {
            "id": "355",
            "introduction_pred": "Introduction\nSentiment analysis: Topicivity and coherence\nNegative Orientation: Relation to Adversarial Learning\nEVIDENCE: Relations between syntactic units\nHierarchical Structure of a corpus",
            "method_pred": "What is the relationship between sentiment and astronomy?\nDomain: climatology, politics, astronomy, food\nLanguage: astronomy, culture, music\nDiscourse: Propagation, Interpretation\nProposed by inter-annotator\nSemantic orientation of the satellite\nSubjective: relations between literature and astronomy\nRelational: relations with astronomyDiscourse analysis\nWhat are the relations between astronomy and space?What are relations between ground and sky?",
            "result_pred": "Experiments\nBasque Opinion Corpus (Basque)\n1. Study sentiment words\n2. Study coherence relations\n3. Match sentiment words by association\n4. Match word pairs\n5. Match whole sentence pairsResults on Basque News Corpus annotator\nBASQA: News Corporation (Chen et al., 2017)",
            "conclusion_pred": "Conclusion\nWe have annotated a part of the Basque Opinion Corpus using Rhetorical Structure Theory (RST)\nWe also obtained more fine-grained results regarding subjectivity and attachment.\nThe manual evaluation of the corpus shows that the inter-annotator agreement of the type of rhetorical relations is 39.81%."
        },
        {
            "id": "370",
            "introduction_pred": "Introduction\nCross-language plagiarism is a very significant problem nowadays, specifically in higher education institutions.\nIn monolingual context, this problem is rather well treated by several recent researches (Potthast et al., 2014, 2015).\nHowever, the expansion of the Internet, which facilitates access to documents throughout the world and to increasingly efficient (freely available) machine translation tools, helps to spread cross-language similarities.",
            "method_pred": "CrossLanguage plagiarism detection\nCross-Language: Plagiarism detection\nLanguage: Gramularity and similarity\nCrossLanguage: Word similarity and syntactic similarityWhat is plagiarism?\nWhat is it about?\nLinguistic similarity is a complex phenomenon\nMultiple languages (English, French, Spanish)\nSpeech-based lexical similarity detectionCross-language plagiarism\nWe can find a way to measure plagiarism across languages\nWe will use\nCL-Language as a method of detecting plagiarism\nIt is a multolingual (translation-based) method\nEach language has its own set of lexical resources\nDifferent languages have different vocabularies\nBut we will use CL-Language\nThe differences between languages are the same\nHowever, we will not use them\nWhat about the translations?The best approaches so far\nThe best methods so far\nIncorporating the best approaches\nNot the best, but the best!\nFor instance, we can find two methods that are better, but not the best\nBecause they are more efficient\nThey are able to find the best way\nto measure the granularity of sentences\nand they will do it\nfor instance, using the same methods as the one used by the previous methods\nbut they will not be good enough\nAnd they will be worse\neven if they are not the same as before\nthe methods will be better\nwill be worse than the previous ones\nAlso, the results will be even worseThis is the first time we can truly evaluate the plagiarism performance\nSo what is the best approach?",
            "result_pred": "Cross-lingual similarity detection\nWe apply the same evaluation protocol as in Ferrero et al. ( 2016)'s paper.\n10 folds are carried out by changing the source language units randomly selected from S\nM-1 original source language unit randomly selected by M-1 other units randomly chosen from S. The same unit may be selected several times\nF1 score is the harmonic mean of precision and recall\nThe proportion of relevant matches retrieved among all the relevant matches to retrieve",
            "conclusion_pred": "Conclusion\nWe conducted a deep investigation of cross-language plagiarism detection methods on a challenging dataset.\nWe revealed strong correlations across languages but also across text units."
        },
        {
            "id": "391",
            "introduction_pred": "Introduction\nChinese is an analytic language without inflectional morphemes. Chinese morphology mainly focuses on analyzing morphological word formation.\nIn this paper, we conceive the Chinese word forming process from a syntactic point of view (Packard, 2000).\nMotivation\nMorphological type schemes for Chinese bi-character content words\nCan morphological classification for Chinese derived words and compound words be effective?\nSome previous work focused on longer unknown words and has limited scalability.",
            "method_pred": "Outline\nMorphological Type Classification\n1. Derived Word: Rule-Based Scheme\n2. Modified Word: Feature-based SchemeMorphology Type Classification\nChinese morphological type classification\nChinese bicharacter task: \"n-head\"Chinese Morphological Type Corpus\nThe \"n\"-head\" task: Chinese morphological types.\nIn this paper, there is still a lack of complete theories, compound words are independent from word contexts\nDerived words are Chinese morphology formed in certain formations (e.g. morphological, lexical, syntactic, etc.)\n3. Modified Words are Chinese words formed of, characters following a certain corpus syntactic relations\n4. ModifiedWords are Chinese bi-character content words, which cannot be categorized into our com-pound words scheme.Compound Word Corpus 1\nCompound Words - Corpus of Chinese Bi-character Content\nThe corpus is incrementally developed in two stages:\nInitial set\nConj/mother/mother\nMaddox/mother is developed to classify\nBi-character word C1 C2 is the \"mother\" type\nC1 C1 is a morphological derived word, where a certain set of relations is collected based on\nA morphological analyzer is developed\na morphological lexical relations analyzer\nb-character words are composed of constituent constituent words\nc2 is composed of a few characters following the same corpus syntactical relations (Ying et al., 1992;\nB-character constituent words are formed through a certain syntactic relation analyzer analyzer [Hanchuan Sim, 1992;Lu,\nYing, Liu, and Hainan Liu, 2010]Our approach\nWe develop a rule-based classifier\nWe convert the \"sfx\" type from the \"conj\" type to the \"c1\" type.",
            "result_pred": "Experiments\nRandom Forest and SVM outperformed all other models and baselines.\nSequence of morphological types is the most important factor.Results\nComparable performance on macro F-measure for compound words.",
            "conclusion_pred": "Conclusion\nWe developed a set of tools and resources for leveraging morphology of Chinese bicharacter words.\nWe propose a category scheme, develop a corpus, and build an effective morphological analyzer."
        },
        {
            "id": "405",
            "introduction_pred": "Introduction\nIn recent years, there has been growing interest in diachronic lexical resources, which comprise terms from different language periods. (Borin and Forsberg, 2011;Riedl et al., 2012)\nThese resources are mainly used for studying language change and supporting searches in historical domains, bridging the lexical gap between modern and ancient language.\nIn particular, we are interested in this paper because it proposes to use a new method for classification: Query Expansion (QPP).\nIt contains entries for modern terms, denoted as target terms.",
            "method_pred": "Terminology Extraction diachronic corpus\nTerminology: Extraction (i.e. generality)\nWe focus on two properties: Terminology:\nPseudo Relevance Feedback (QPP) and Query Performance Prediction (pseudo-relevance).\nQuery Performance Prediction: QPP\nSparse non-stop modern terms from the top retrieved documents.\nQPP is applied to identify lexical resource that aims to map between modern terms and their semantically related lexical constituents.Targets of modern lexical resources in diachron corpus\nExample: in a recent paper, phrase chunker is used to filter out stop modern and restrict candidate terms to nouns or noun phrases thesaurus\nMethod: query performance prediction\nquery performance prediction (query performance)Terminological properties of ancient documents\nMethod of query performance Prediction\nMeasures that express ancient related terms indicate the collocation strength of the term in the corpusAn Example of Modern Lexical Resources\nAn Example: of Modern lexical Resources\nSelected from a key-list of terms whose coherence in the language is already known (e.g. unithood or termhood) indicate the corpus strength relevancy of a term in a lexical corpus",
            "result_pred": "Evaluation Setting\nThe most important document in our diachronic corpus is the Responsa corpus, which contains over 8,000,000 documents from the 11 th century until today.\nThe vast majority of these documents are annotated by two domain experts:\nHuck et al., 2017;\nArabic, Turkish, Latvian, Russian, Turkish\nEnglish, French, Italian, Spanish,\nRomanian, Polish,Results\nOur method further expands modern candidate terms with polynomial kernel extensions\nWe found that their expanded forms contain ancient terms that help the system making the right decision.",
            "conclusion_pred": "Conclusions\nWe introduced a method that combines features from two closely related tasks, terminology extraction and query performance prediction, to solve the task of target terms selection for diachronic thesaurus.\nWe showed that enriching TE measures with QPP measures improves performance."
        },
        {
            "id": "408",
            "introduction_pred": "Introduction\nLearning a new language from news articles is time consuming.\nIt is crucial to have a dedicated learner on a daily basis to improve the learning experience.Motivation\nMost existing language learning software are instruction-driven or user-driven.",
            "method_pred": "What is WSD\nWhat is WordNews?\nWe propose to build a dictionary\nWordNews vocabulary\nChinese word disambiguation\nJapanese word\nEnglish sentence\nThe dictionary is available in context\nIn this way we can use it as a learning platform\nOur word aligns with monolingual\nTranslation is easy to obtain\nBut we need to make use of\nthe dictionary as it is available\nand our word aligning with the rest of the WSD lexicon\nWhat are the meanings of the words in the dictionary?WordNet\nWordNet word alignments with the corresponding word of the corresponding language\nWhich is the correct translation\nwhich is correct according to the dictionaryOur Example\nLanguage of the World\nLanguage that is well adapted to the task\nHow to get the best of both worlds\nLearn a dictionary for the given taskLanguage of a World\nlanguage of a world\nWords of the same kind as the target\nlanguage that is not well adapted\nLanguages that are not yet fully adaptedChinese language of the world\nOur Example\ntranslation is easy\nCan we generate multiple translations for the target task?",
            "result_pred": "Evaluation 1\n1. WordGap baseline (95.4%)\n2. WordNet: a web based translation API\n3. Bing: a search-based language learning platform\nWord Search: a free web search based on WordNet\n4. Expressions: word pairs\nSuggested word pairs for comparison\n5-word pairs per sentence\nEach pair of pairs of pairs",
            "conclusion_pred": "Conclusion\nWe proposed WordNews, a client extension and server backend that transforms the web browser into a second language learning platform.\nLeveraging web-based machine translation APIs and a static dictionary, it offers a viable user-driven language learning experience by pairing an improved, context-sensitive tooltip definition service."
        },
        {
            "id": "419",
            "introduction_pred": "Motivation\nBLEU scores and human judgements scores in MT shared tasks have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006).\nHowever, the relatively consistent correlation of higher BLEU score (Papineni et al. 2002) and better human jud acknowledgements in major machine translation shared tasks has led to the conventional wisdom that translations with significantly higher BleU scores generally suggests a better translation than its lower scoring counterparts (Bojar et al, 2014;Nakazawa et al 2014;Cettolo et. 2014).Related Findings Outside MT Shared Task Submission\nB. LeU score: low for adequacy and high for fluency\nRIBES score: very high for both score and sentiment",
            "method_pred": "Speechbased MT\nSpeech based MT\nHuman evaluations\nRIBES * REAES * M\u00e0rquez * Lavie *\nMao Tome and the Sacred Tree\nBLEU is the baseline translation\nThe \"prominent\" translation inferior to the BLEU\nPreferably better than the \"boutique\" translationBaseline Translation\nThe hypothesis translation baseline score\nPairwise comparison between the organizers' phrase-based MT baseline system and the crowdsourced system\nBaseline: hypothesis n-gram precision matching\nProbabilized syntacticized parse\u4e0a  semantic entities\nParsewise comparison with the RIBES\nProposed a hypothesis translation with human judgements\nQuestion: what is the minimum minimum minimum?\nIs the baseline the better translation?Our approach Baseline Translation WAT 2015\nHuman Evaluation: Workshop on Asian Translation\nRobustly compare the baseline and precisions\nOur approach: skip-grams replace the surface ngrams replaced the baseline ngramms replaced by a lexicalized one\nIn this paper we present the following: Baseline: WAT2015\nhuman evaluation:\nmetrics trained on skip grams and neural network\npre-trained regression models on skipgrams\nsourz and M\u00e0vquez\nthe crowd-sourced MT baseline evaluators were asked to judge whether the hypothesis translation was better, or unlabeled, or partially translated\nWe also show that the baseline is better, but the precisions are not the same\nreorder the baseline\nimplemented the baseline with skip grammings replaced by an inflate\ne.g. skip gramms replace the baseline w.Human evaluations\nPre-trained regressors on skip- gramms replaced the original\nGANIZED GROUPS\nRESOURCES WITH FURTHER METHODS\nREAISTS * MOUTPUT MACHINE SYSTEM\nINTERACTIVE MULTI-GROUPS replaced by FURELY MIXED GROWTH\nOBJECTIVE: Improve upon the baseline by allowing the human evaluator to match the GRUPS with the hypothesis translators\nEvaluate the baseline using skip gram meters replaced by neural network-based BLEUs\nExcluding the \"",
            "result_pred": "Experimental Setup\nIBES + RIBES\n*outperformed by KenLM (WAT, 2014)Human Evaluation\nHuman Evaluation - BLEU\n3 sets of output:\n1. Word alignment: word alignment + human judgement\n2. Standardization: word alignments + RibESEvaluation\nBLEU: B. Language combination\nRIBES: phrase-based MT system\nWhitaker, Newman-Griffis, Haldar, et al.",
            "conclusion_pred": "Conclusion\nWe have demonstrated a real-world case where high BLEU and RIBES scores do not correlate with better human judgement.\nUsing our system's submission for the WAT 2015 patent shared task, we presented several factors that might contribute to the poor correlation."
        },
        {
            "id": "424",
            "introduction_pred": "Introduction\nWAT is a new open evaluation campaign focusing on Asian languages.\nParticipants are invited to participate in WAT by word-of-mouth\nTo learn more about Asian languages and the problems to be solved",
            "method_pred": "Outline\nOverview of the proposed translation\nThe Syntax-based SMT system\nHuman evaluation:\nTraining: Human evaluation\nProceedings of the Workshop\nRecognition of the high quality human evaluationBaseline of the SMT System\nBaseline: the Syntax--Based SMT\nTranslation subtaset\nTranslate: string-to-string\nJapanese Training: Syntax+String\nInternational Training: Human Evaluation\nWorld's largest language evaluation systemChallenge for human evaluation system\nChallenge: human evaluation systems\nTrain the translation model\nBuild the translation system using crowdsourcing\nAssume that the human evaluation is better than the human\nWhat about the machine translation system?\nHow can we measure the quality of human evaluation?Training\nHuman Evaluation System\nIn this paper\nWe will estimate the syntactic interval between human evaluation and human translationHuman evaluation System<et>Training the system with the highest confidence interval\nAll teams submitted one or more sentences to parallel human evaluation.\nFor the first time in human evaluation, we can estimate the confidence interval between two sentences.PEC2015 SMT baseline\nPEC2014 SMT: baseline\nRBMT: the number of wins\nTatjana Training: Training automatic translation server\nMining up to the best quality human translation system",
            "result_pred": "Automatic Evaluation\nHuman Evaluation: Calculating automatic evaluation scores for WAT2014\nBLEU translation subtasks\nJoint SVM Model in MSR\nPhrase-based SMT\nTwo translation evaluation tools:\nSMT, SMT and RBMTHuman Evaluation Results\nAutomatic evaluation results of WAT 2014 are reported from several perspectives\nParticipants compute the translation results using the same web interface as the input sentence\n50% of the translation submissions are annotated with human judgements\n100% of translation submissions have been manually evaluated\nHuman evaluation results were calculated with RIBES.py version 1.02.4",
            "conclusion_pred": "Conclusion\nWe had 12 participants worldwide, and collected a large number of submissions which are useful to improve the current machine translation systems by analyzing the submissions and finding the issues.\nWe are planning to conduct context-aware MT evaluations."
        },
        {
            "id": "425",
            "introduction_pred": "Introduction\nEn-ja, ja-en,\nJapanese, Japanese, Chinese...\nThese are the four most promising translation formalisms for Asian languages.\nTo tackle the reordering problem, we first need a forest-to-string translation formalism\nThen we translate Japanese (ja), a language with SOV word order, to/from English (en) or Chinese (zh), languages with SVO word order.Related Work\nForest-to -string translation allows for more robust use of source-side syntax by not considering a 1-best parse tree but a plethora of parse candidates stored efficiently in a packed-forest data structure",
            "method_pred": "Translation task\nTranslation task:\nJapanese-English 6-gram dictionary\nEnglish, Chinese, Japanese\nChinese, Japanese, KoreanLanguage Model Training\nJapanese -English\nWe trained a tree substitution grammar (STSG-to-string)\nKoehn loglinear model\nParsing RNNLM with\nMikolov et al., 2006\nLanguage model trainingTraining Example\nWe used all of the data for training the language model, null-aligned ja-en translation possible point in the parse low confidence set\nTraining Example ja-zh translation availableJapanese\nLanguage Model training\nJi-zh kai-zh\nTrav neural network language model trained on the Japanese/Chinese Wikipedia corpus\n~1.5 million sentences of training data available\nTranslation models trained on all available dataTranslation tasks\nEnglishChinese Penn Treebanks\nChinese Penn treebanks\n* * * * Japanese, Chinese and\nSourcetranslation of the sentence into a tree-bank system.\nThere are a number of ways to incorporate these dictionaries, but in the end we decided against submitting this system\nIn translation, because we had unknown words that existed in dictionary data, we trained syntactic parser on them\nFor example, \"\u6a19\u984c\"super-sentential context in the lexically different but semantically different context.Syntactic parsing\nSemantic parsing\n\"Shang-ja\" is the most common word in dictionary but\nIt appears in the output of the training set but not in the test setParametric Parsing\nParametric parsing\nThe first word in the dictionary is \"shang\"\nThis word appeared prolifically in the dev set but also in the target language\nSubsentential \"\u8868\" is considered by many times in the n-gram training, but not by the dev systemParameterization Optimization\nThis work was supported by JSPS KAKEN Egret parser\n(Petrovara Institute of Science, 2006).\nParameterization was performed by\nUsing the Stanford Parser\nOptimize for BLEU+RIBES\nUse a recurrent neural network to convert\nsubsentential contexts into semantically similar context\nSemantically similar to \"Shang",
            "result_pred": "Experimental Results\nBLEU+RIBES system (using RNNLM as the evaluation measure)\nHuman evaluation (using KyTea)",
            "conclusion_pred": "Conclusion\nThe system achieved the highest translation accuracy on all four language pairs targeted by the WAT 2014 Workshop on Asian Translation.\nWe added rescoring using a recurrent neural network language model (RNNLM)."
        },
        {
            "id": "432",
            "introduction_pred": "Introduction\nOpen source machine translation: novel techniques and techniques which we apply can typically be performed on commodity computing resources which are widely available\nIn short, there should be no reason why small research groups and even lone researchers should not be able to join and make substantive contributions furthering our field\nPhrase-based Translation: Relation to Extraction and Synthesis",
            "method_pred": "Overview\nOverview of the shared task\nJoshua supports parsing-based machine translation using SAMT (Zollmann and Venugopal, 2006).Joshua Translation System\nJoshua: Translation System\nLopez (2008) of minimum error rate training\nHiero (Lopez, 2008) was used to translate the lowercase-to-truecase test set into truecase.\nJoshua decoder produces n-best lists of translations\nTuned grammars are used to extract the lowercased results from the test set\nProposed translation grammar tuning is performed on the tokenized (but not normalized) target language corpus\nMonotone word alignments are deterministically created, mapping normalized lowercase training text to the original truecase text\nSubsampling is performed for the training set, followed by alignment and grammar extraction using Berkeley aligner 4 (Liang et al., 2006)\nParsing-based Machine Translation with SAMT\nTranslation Grammars targeted for use with shared task setBaseline Target Target\nBaseline: Target language pair\nEnglish news-commentary10.de-en europarl-v5.fr-en giga-fren.release2 undoc.2000.en-fr\nJapanese news-news-commentaries10.d.en.es-en-en.",
            "result_pred": "Managing Experiment Setup\nManaging experiment setup\nOpen source virtual machine translation (LDC)\nSRI Language Model Toolkit (Kneser and Ney, 1995)Experimental Results\nExperimental results -\nSix language pairs in German-English\nEnglish-French, French-German, Spanish-English,\nGerman-English:\nFrench-German: French-English;\nSpanish-English :\nEuroparl: Europarl;Evaluation of Statistical Machine Translation Shared Task\nEvaluated on the BLEU (Papineni et al., 2002) and TER datasets (Snover et al. 2006)",
            "conclusion_pred": "Conclusion\nWe present the Johns Hopkins University submission to the 2010 WMT shared translation task.\nWe describe processing steps using open data and open source software used in our submission.We provide the scripts and configurations required to train, tune, test our machine translation system."
        },
        {
            "id": "443",
            "introduction_pred": "Introduction\nWIPO is a UN agency in charge of Intellectual Property\nPatent Cooperation Treaty => facilitate patent protection in multiple countries\nW.IPO receives ~ 150,000 patent applications/year\nTitles and abstracts must be available in English and French\nNeeds in CAT tools + huge parallel corpus => SMT",
            "method_pred": "What is Intellectual Property Organization?\nIPO translators and translation teams\nATM Machine Translation Copet\nAutomatic translation of patent documents\nTranslated text from French to English\nParaphrasing (translation machine)\nTranslation machine translation machine translation\nConversion to human translationTranslation Machine Translation\nTranslation Machine translation machine translated\nPAST: SMT-domain-domain:\nTASK TRANSLATION Machine Translation Machine\n\"Automatic\" translation of patents\nQuestion: How can we translate patent documents from French into English?What are the challenges of translating patent documents into French\n1. Translation Machine Translation\n2. Translation machines trained on translation machine\n3. Training\n4. Translation Machines trained on translations machine translation machines\nWhat is the challenge of translating patents into French?\nThe challenge is translation machine learning\nCan we translate patents into English without human input\nHow to translate patents in FrenchTranslated patent documents\nTranslate patents into sentences\nLanguage models trained on translated patentsPAST Translation Machine translation Machine translation\nWhat do the challenges entail?",
            "result_pred": "Automatic evaluation\n2 translator-revisers per document\n1 translator: per document (out of 1390 documents)\n2 translators: each has a BLEU score\n3 non translators : each had a score of 0.15\n516 translations produced during 13 days\nThe best settings (after a few runs with various parameters) turned out to be, in a word, of a quality at least equivalent to that of the translations produced by the outsourcing agencies\nImprovement is limited due to the fact that the translation is human-driven",
            "conclusion_pred": "Conclusion\nTapta proved to be an accelerated training aid, which in many instances allowed an inexperienced translator to produce rough or pretranslations of a sufficiently acceptable quality, to then be finally checked by a reviser for quality assurance or enhancement purposes\nThe tests also demonstrated that the time needed by beginners to prepare, draft and input the translations was reduced thanks to the many equivalents proposed by Tapta.\nIt was also observed that the translations produced from simple source texts were often of a quality comparable to that of translations produced by translators."
        },
        {
            "id": "449",
            "introduction_pred": "Introduction\nWhat if we use word alignments in a CAT system to improve translation?\nHow to measure translation accuracy?Assumption: Accuracy is lower\nTranslators can focus on choosing a translation where words in a SL segment are needed\nEffective translation using fuzzy-match scores\nWe propose a different method of translation:\nWord alignments for source and target segments:",
            "method_pred": "Outline\nMethodology\n1. Given a set of SL segments in TM i\nfind them aligned with TM\n2. Use them as training examples\n3. If the SL segment in TM ii\nUse it as training example\n4. Use it as a translation exampleMethodology x\nIf the word alignment is able to re-use alignment from source to be translated\nTMs that can be aligned with it are kept unedited\nTarget segment: if the word alignments are not aligned with the translation directions\nIf they are, use them to convert the translation direction\nTo determine which TUs should be changed\nThe optimal alignment is determined by the TMs that have been extracted from the source\nTrained on the task of converting the source into a new TU\nIn this paper we explored a new method to improve the alignment:\nWe used the previously computed words in TMi\nFor the first time, we used the SL segments to be matched\nOur approach is symmetric\nIt is possible to obtain the best alignment for the new TUs\nBut what about the old TUs?\nCan we use them for the translation task?Our approach\nTarget word alignment: w ij\nGiven a SL segment of TM i ij, find it aligned wij with TM i j\nUsing the SL-based model\nFinding them aligned wij is the best way\nReconstruction of the alignment problem\nMaintain the optimal alignment\nto determine which segments should be aligned\nTM of is easy to obtain\nNo need to change the alignment function\nOnly if the words align wij are aligned wi-jTMs aligned wij with SL segments\nTO FIND OUT THE RIGHT LANGUAGE\nWhere can we find the best aligned SL segments\nWith respect to the SL words aligned w-j, we have used(v ik ) = v ik (w-th, source-of-SL segments)\nwhere can we get the best-aligned SL segments?",
            "result_pred": "Evaluation\nAccuracy and coverage obtained with each set of matching TUs in TM\nE.g. the percentage of words aligned with one word in the translation\nCoverage and F1 scores\nFy-match score > 100%Efficiency and coverage\nEfficiency is an optimistic estimate.\nClinical significance is small\nMajority and unanimity criteria are significant\nThe majority criterion is more important than the majority criterion.",
            "conclusion_pred": "Concluding remarks\nWe propose a new approach to guide TM-based CAT users by recommending the words in a translation proposal that should be changed or kept unedited.\nThe method we propose requires the TM to be pre-processed in advance in order to get the alignment between the TUs."
        },
        {
            "id": "460",
            "introduction_pred": "Introduction and related work\nHuman error detection is an intrinsically difficult task.\nIt is crucial to have reliable automatic methods for detecting inflectional and reordering errors.Motivation and Conclusions\nHuman errors analysis and classification are expensive and time consuming.",
            "method_pred": "Error classification automatic\nError classification: automatic\nHuman error classification: manual\nReference automaticRecovery of human error classification errors\nHuman Error classification error classification error\nGeneral Assembly: reordering human errors\nRecovery from human errors errorsHuman evaluation as reordering automatic<et>Human Evaluation as Reordering AutomaticGeneral Assembly manual error classification\nGeneral assembly: manual error analysis\nAutomatically detect human errors and correct translations\nPrefer human evaluation as well as human evaluators\nThe automatic tool identifies human errors in the source sentence\nIt is possible to use human evaluation for reordering manualReordering Automatic\nReordering manual errors: human evaluation is needed\nExamine human evaluation using human language\nReorder manual errors by humans\nIdentify human errors as errors by human evaluation\nReduce human evaluation from human evaluation to human evaluation only\nAssess human evaluation with human language analyzer\nTruncate human evaluation and error classificationError analysis manual\nAutomatic: manual errors are most frequent\nIn this work automatic errors are more common\nThey are most common among human evaluation systems\nand they are not detected by humans but by automatic tools\nreordering manual errorAutomatic errors classification<et>General assembly automatic error classification auto\nRobust human evaluation against human language model\nautomatic errors classification auto by human eval evaluator\nin this work manual errors classification\nby human evalui\nusing human language models\ne.g.\nhuman language model autoAuto errors classification automatic",
            "result_pred": "Experimental set-up\nHuman error analysis for all texts and all error classes\nSix English translation outputs obtained by state-ofthe-art statistical phrase-based translation systems in the framework of the GALE1 project and the fourth Workshop on Statistical Machine Translation 2 (WMT).\nTwo GALE outputs are translations from Arabic into English, and the third is a result of Chinese-to-English translation.Results and correlations\nResults and correlation across different error classes are very high\nAutomatically compare WMT outputs with human error classification\nManual error classification measures are more flexible\nHuman Error Classification measures are slightly lower\nThe correlation coefficients across the human and automatic measures are ~consistent\nFor WMT and WMT texts, only one reference translation was available.\nThis is a good indication that the automatic measures can successfully detect weak and strong points of particular translation systems.",
            "conclusion_pred": "Conclusions\nWe propose a systematic method for automatic error classification of machine translation output\nThe method detects five error classes commonly used in human error analysis: inflectional errors, reordering errors, missing words, extra words and incorrect lexical choice.\nWe have shown that the error classification results obtained by this approach correlate very well with the results of human and automatic error analysis."
        },
        {
            "id": "466",
            "introduction_pred": "Introduction\nOverview of geometric idealizations underlying spatial prepositional phrases.\nLinguistic patterns of motion verbs across languages.Another set of motivations...\n-Bill MacCartney, Stanford CS224U Slides",
            "method_pred": "What is the environment like\nWhat does the environment look like?\nMore capacity is upping the ante.\nLess power equals more bandwidth.What can we do to adapt to the changing environment?<et>What we need to do\nWHAT WE NEED TO DO\nGilder's law: network bandwidth triples every eighteen monthsMT in Bentley\nHow much has MT in Bentley?The environment is changing\nTHE EXAMPLES OF COURSEWARE\nWhat is it like to read a courseware translation?\n\"I read the book and it was the best translation of courseware I ever read.\"\n\"What is this book about?\"\nHow can we adapt to a changing environment and make it better?",
            "result_pred": "Exploding content is creating new needs\nExploding Content is Creating New needs\nA week's worth of The New York Times contains more information than an 18 th century person encountered in a lifetime More unique information will be generated this year than in the previous 5000 years\nGartner predicts content processed by companies between 2007 and 2012 will grow 15 fold more quickly than the population of the United States\nWordPress.org/documents\nGoogle Docs/WordPress\nTable 1: Size of the word-press corpus",
            "conclusion_pred": "Future Work\nFuture Work\nConclusions\nWe will be able to start with a simple but effective way to start.\nWe can start with simple and effective methods of measuring and measuring."
        },
        {
            "id": "469",
            "introduction_pred": "Introduction\nOver the past fifty years of development (Hutchins, 2001), statistical MT for low e-resourced languages has obtained good results when applied to several pairs of languages such as English, French, Italia, Japanese, etc.\nObjective: extract parallel sentence pairs from a comparable corpus\nStatistical method to build a statistical translation model for source/target languages\nUnsupervised extraction from a large parallel corpus",
            "method_pred": "Outline\nVietnamese translation system\nMining parallel translation models\nTrained on a bilingual machine translation\nTranslation models trained on a multilingual machine\nThe translation models were tested on a parallel corpus\nthe \"simulated\" translation model\n\"unsupervised\" Sys1\ntranslation models were applied on parallel data\nWe want to improve the translation quality of the target language\nFor the first time, we can find the optimal translation for the target sentence\nIn this paper, we found the following results:\nConversion of the translation model to a parallel machine translation:Vietnam translation system\nThe system was tested on an unsupervised\nconversion was performed on a translation model trained on\nTune the translation models on the parallel data and find the correct translationOur research: mining parallel data from a translation system source\nMinimize the number of translations required\nOur experiments: mining a bilingual translation model on the target\nPrecision of translation model is performed on translation model using\nprecision of translations is performed by\nsub-supervised Sys\nPairs of sentences were extracted using Moses\nMethod: extract parallel data of sentences\nSparse translation model with translation module\nOptimize translation quality\nTrain translation models with translation modules\nAssume that the translation is good\nCan we get the best quality of parallel data?\nExamine if the translations are correct\nIf they are not, we want to make the best translation possible\nThis is our research project\nCombining two translation systems\nFinding the optimal parallel translation model for the task\nRe-training translation models using translation modules and\nUsing a translation module trained on the source corpusThe first translation system to mine parallel data is Sys2\nIt is possible to mine the full translation data from Sys 2\nWith the help of Moses (Unsupervised), we can create a new parallel translation system.\nAfter mining the full dataPairs extracted from the sentence of the correct translated pair\nAn iterative method was used to extract the full sentence from the translation data.SemiSupervised: extract the parallel information of Sys two sentences from a sentence of\nSemi: extract sentence information of the corresponding sentence from a different sentence of a similar sentenceThis is the first",
            "result_pred": "Experiments\nFrench-English parallel sentence pairs extracted from Europarl corpus\nSemi-supervised method extracted from a comparable corpus using a combination similar to our W2 type.\nBLEU, TER, METEOR, NMT, SVM, MT,\nTwo translation systems were built: one based on semi-suprema (Sys1) and another based on unsupervised (Sys2).Translation system evaluation\nThe quality of the translation systems was also evaluated.",
            "conclusion_pred": "Conclusion\nWe presented an unsupervised method for extracting parallel sentence pairs from a comparable corpus.\nThe method was tested in a hard condition: the corpus does not exist."
        }
    ]
}